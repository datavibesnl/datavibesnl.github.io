"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[9077],{8757:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>h,frontMatter:()=>i,metadata:()=>o,toc:()=>l});var s=t(4848),a=t(8453);const i={},r="4.2 Structured Streaming (Hands On)",o={id:"Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.2 Structured Streaming Hands On",title:"4.2 Structured Streaming (Hands On)",description:"Introduction",source:"@site/docs/Databricks Certified Data Engineer Associate - Preparation/4. Incremental Data Processing/4.2 Structured Streaming Hands On.md",sourceDirName:"Databricks Certified Data Engineer Associate - Preparation/4. Incremental Data Processing",slug:"/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.2 Structured Streaming Hands On",permalink:"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.2 Structured Streaming Hands On",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Databricks Certified Data Engineer Associate - Preparation/4. Incremental Data Processing/4.2 Structured Streaming Hands On.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"4.1 Structured Streaming",permalink:"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming"},next:{title:"4.3 Incremental Data Ingestion",permalink:"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.3 Incremental Data Ingestion"}},c={},l=[{value:"Introduction",id:"introduction",level:2},{value:"Dataset setup",id:"dataset-setup",level:2},{value:"How to work with data streaming in Spark SQL",id:"how-to-work-with-data-streaming-in-spark-sql",level:2},{value:"How to query a streaming temporary view",id:"how-to-query-a-streaming-temporary-view",level:2},{value:"How to persist incremental results",id:"how-to-persist-incremental-results",level:2},{value:"What happens when new data arrives?",id:"what-happens-when-new-data-arrives",level:2},{value:"How to create a triggered incremental batch with the <strong><code>availableNow</code></strong> trigger",id:"how-to-create-a-triggered-incremental-batch-with-the-availablenow-trigger",level:2}];function d(e){const n={admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"42-structured-streaming-hands-on",children:"4.2 Structured Streaming (Hands On)"}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","We will explore the basics of working with Spark Structured Streaming to allow incremental processing of data",(0,s.jsx)("br",{})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","We will continue using our bookstore dataset, with its 3 tables:"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Customers;"}),"\n",(0,s.jsx)(n.li,{children:"Orders;"}),"\n",(0,s.jsx)(n.li,{children:"Books"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"dataset-setup",children:"Dataset setup"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Let us first copy our dataset"]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sh",children:"%run ../Includes/Copy-Datasets\n"})})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"how-to-work-with-data-streaming-in-spark-sql",children:"How to work with data streaming in Spark SQL"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","To work with data streaming in SQL, you must first use ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"spark.readStream"})})," method PySpark API",(0,s.jsx)("br",{})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"spark.readStream"})})," method allows to query a Delta table as a stream source"]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'spark.readStream.table(\n    "books"\n)\n'})})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","We can register a temporary view against this stream source"]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:"{3-5}",children:'spark.readStream.table(\n    "books"\n).createOrReplaceTempView(\n    "books_streaming_tmp_vw"\n)\n\n'})})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ",'The temporary view created here is a "',(0,s.jsx)(n.em,{children:"streaming"}),'" temporary view that allows to apply most transformations in SQL the same way as we would with the static data',(0,s.jsx)("br",{})]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"how-to-query-a-streaming-temporary-view",children:"How to query a streaming temporary view"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Let us first query this streaming temporary view"]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"SELECT * FROM books_streaming_tmp_vw\n"})})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","As you can see, the query is still running, waiting for any new data to be displayed here",(0,s.jsx)("br",{})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","We don't display a streaming result unless a human is actively monitoring the output of a query during development or live dashboarding",(0,s.jsx)("br",{})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Let us now apply some aggregations on this streaming temporary view"]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",metastring:"{3,5}",children:"SELECT \n    author, \n    COUNT(book_id) AS total_books\nFROM books_streaming_tmp_vw\nGROUP BY author\n"})})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","We are querying a streaming temporary view, this becomes a streaming query that executes infinitely, rather than completing after retrieving a single set of results. And here we are just displaying an aggregation of input as seen by the stream",(0,s.jsx)("br",{})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","For streaming queries like this, we can always explore an interactive dashboard to monitor the streaming performance"]}),"\n",(0,s.jsxs)(n.admonition,{type:"warning",children:[(0,s.jsxs)(n.p,{children:["When working with streaming data, some operations are not supported like ",(0,s.jsx)(n.strong,{children:"sorting"})]}),(0,s.jsxs)(n.admonition,{type:"info",children:[(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"SELECT * FROM books_streaming_tmp_vw\nORDER BY author\n"})}),(0,s.jsxs)(n.p,{children:["You can use advanced methods like ",(0,s.jsx)(n.strong,{children:"windowing"})," and ",(0,s.jsx)(n.strong,{children:"watermarking"})," to achieve such operations\n:::"]})]})]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"how-to-persist-incremental-results",children:"How to persist incremental results"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","In order to persist incremental results, we need first to pass our logic back to PySpark DataFrame API",(0,s.jsx)("br",{})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","We are creating another temporary view",(0,s.jsx)("br",{})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Since we are creating this temporary view from the result of a query against a streaming temporary view"]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"CREATE OR REPLACE TEMP VIEW author_counts_tmp_vw AS (\n    SELECT author, COUNT(book_id) AS total_books\n    FROM books_streaming_tmp_vw\n    GROUP BY author\n)\n"})})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","In PySpark DataFrame API, we can use the ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"spark.table()"})})," to load data from a streaming temporary view back to a DataFrame"]}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsx)(n.p,{children:"Spark always load streaming views as a streaming DataFrames, and static views as a static DataFrames, meaning that incremental processing must be defined from the very beginning with Read logic to support later an incremental writing"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","We are using DataFrame ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"writeStream"})})," method to persist the result of a streaming query to a durable storage",(0,s.jsx)("br",{})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","This allows us to configure the output with the three settings",(0,s.jsx)("br",{})]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["The trigger ",(0,s.jsx)(n.strong,{children:"intervals"}),", here ",(0,s.jsx)(n.strong,{children:"every 4 seconds"}),"."]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:"{4}",children:"spark.table(\n    \"author_counts_tmp_vw\"\n).writeStream.trigger(\n    processingTime='4 seconds'\n).outputMode(\n    'complete'\n).option(\n    'checkpointLocation',\n    'dbfs:/mnt/demo/author_counts_checkpoint'\n).table(\n    'author_counts'\n)\n"})})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.strong,{children:"output mode"}),", either ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"append"})})," or ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"complete"})}),"."]}),"\n",(0,s.jsx)(n.admonition,{type:"warning",children:(0,s.jsxs)(n.p,{children:["For aggregation streaming queries, we must always use ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"complete"})})," mode to overwrite the table with the new calculation"]})}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:"{6}",children:"spark.table(\n    \"author_counts_tmp_vw\"\n).writeStream.trigger(\n    processingTime='4 seconds'\n).outputMode(\n    'complete'\n).option(\n    'checkpointLocation',\n    'dbfs:/mnt/demo/author_counts_checkpoint'\n).table(\n    'author_counts'\n)\n"})})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"checkpointLocation"})})," to help tracking the progress of the streaming processing"]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:"{8-9}",children:"spark.table(\n    \"author_counts_tmp_vw\"\n).writeStream.trigger(\n    processingTime='4 seconds'\n).outputMode(\n    'complete'\n).option(\n    'checkpointLocation',\n    'dbfs:/mnt/demo/author_counts_checkpoint'\n).table(\n    'author_counts'\n)\n"})})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","You can think about such a streaming query as an ",(0,s.jsx)(n.strong,{children:"always-on incremental query"}),", and we can always explore its interactive dashboard",(0,s.jsx)("br",{})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","From this dashboard, we can see that the data has been processed and we can now query our target table",(0,s.jsx)("br",{})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Our table has been written to the target table, the ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"author_counts"})})," table, and we can see that each author has currently only 1 book"]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"SELECT * FROM author_counts\n"})})}),"\n",(0,s.jsx)(n.admonition,{type:"warning",children:(0,s.jsx)(n.p,{children:"What you see here is not a streaming query simply because we are querying the table directly. I mean, not as a streaming source through a streaming DataFrame"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"what-happens-when-new-data-arrives",children:"What happens when new data arrives?"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","When we execute a streaming query, the streaming query will continue to update as new data arrives in the source",(0,s.jsx)("br",{})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","To confirm this, let us add new data to our source table",(0,s.jsx)("br",{})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Let us run this query and see what will happen in our streaming"]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:'INSERT INTO books\nVALUES (\n    "B19", \n    "Introduction to Modeling and Simulation",\n    "Mark W. Spong",\n    "Computer Science",\n    25\n),\n(\n    "B20",\n    "Robot Modeling and Control",\n    "Mark W. Spong",\n    "Computer Science",\n    30\n),\n(\n    "B21",\n    "Turing\'s Vision: The Birth of Computer Science",\n    "Chris Bernhardt",\n    "Computer Science", \n    35\n)\n'})})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","We can see that there is new data arriving. Let us query our target tabel again to see the updated books counts for each author"]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"SELECT * FROM author_counts\n"})})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Let us come back to our streaming query, and cancel it to see another scenario"]}),"\n",(0,s.jsx)(n.admonition,{type:"warning",children:(0,s.jsx)(n.p,{children:"Always remember to cancel any active stream in your notebook, otherwise the stream will be always on and prevents the cluster from auto termination"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.h2,{id:"how-to-create-a-triggered-incremental-batch-with-the-availablenow-trigger",children:["How to create a triggered incremental batch with the ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"availableNow"})})," trigger"]}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","For our last scenario, we will add some books for new authors to our source table"]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:'INSERT INTO books\nVALUES (\n    "B16", \n    "Hands-On Deep Learning Algorithms with Python",\n    "Sudharsan Ravichandiran",\n    "Computer Science",\n    25\n),\n(\n    "B17",\n    "Neural Network Methods in Natural Language Processing",\n    "Yoav Goldberg",\n    "Computer Science",\n    30\n),\n(\n    "B18",\n    "Understanding digital signal processing",\n    "Richard Lyons",\n    "Computer Science", \n    35\n)\n'})})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Use the ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"availableNow"})})," option to modify the ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"trigger"})})," method to change our query from an always-on query triggered every ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"4 seconds"})})," to a triggered incremental batch",(0,s.jsx)("br",{})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","With the ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"availableNow"})})," ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"trigger"})})," option, the query will process all new available data and stop on its own after execution"]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:"{4}",children:'spark.table(\n    "author_counts_tmp_vw"\n).writeStream.trigger(\n    availableNow=True\n).outputMode(\n    "complete"\n).option(\n    "checkpointLocation",\n    "dbfs:/mnt/demo/author_counts_checkpoint"\n).table(\n    "author_counts"\n).awaitTermination()\n'})})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Use the ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"awaitTermination"})})," method to block the execution of any cell in this notebook until the incremental batch's write has succeeded"]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:"{12}",children:'spark.table(\n    "author_counts_tmp_vw"\n).writeStream.trigger(\n    availableNow=True\n).outputMode(\n    "complete"\n).option(\n    "checkpointLocation",\n    "dbfs:/mnt/demo/author_counts_checkpoint"\n).table(\n    "author_counts"\n).awaitTermination()\n'})})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","With the ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"availableNow"})})," trigger option, the query run in a batch mode",(0,s.jsx)("br",{})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","It is executed to process all the available data and then stop on its own",(0,s.jsx)("br",{})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Let us finally query the target table again to see the updated data"]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"SELECT * FROM author_counts\n"})})}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var s=t(6540);const a={},i=s.createContext(a);function r(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);