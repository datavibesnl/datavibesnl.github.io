"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[3270],{6160:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>h,frontMatter:()=>i,metadata:()=>o,toc:()=>c});var s=t(4848),r=t(8453);const i={},a="4.4 Auto Loader (Hands On)",o={id:"Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.4 Auto Loader Hands On",title:"4.4 Auto Loader (Hands On)",description:"Introduction",source:"@site/docs/Databricks Certified Data Engineer Associate - Preparation/4. Incremental Data Processing/4.4 Auto Loader Hands On.md",sourceDirName:"Databricks Certified Data Engineer Associate - Preparation/4. Incremental Data Processing",slug:"/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.4 Auto Loader Hands On",permalink:"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.4 Auto Loader Hands On",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"4.3 Incremental Data Ingestion",permalink:"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.3 Incremental Data Ingestion"},next:{title:"4.5 Multi-hop Architecture",permalink:"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.5 Multihop Architecture"}},d={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Dataset Setup",id:"dataset-setup",level:2},{value:"Explore the data source directory",id:"explore-the-data-source-directory",level:2},{value:"How to work with the Auto Loader to read the current file",id:"how-to-work-with-the-auto-loader-to-read-the-current-file",level:2}];function l(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",input:"input",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"44-auto-loader-hands-on",children:"4.4 Auto Loader (Hands On)"}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","In this notebook, we will explore incremental data ingestion from files using ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"Auto Loader"})}),(0,s.jsx)("br",{})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","We will continue using our bookstore dataset with its 3 tables:",(0,s.jsx)("br",{})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Customers;",(0,s.jsx)("br",{})]}),"\n",(0,s.jsxs)(n.li,{children:["Orders;",(0,s.jsx)("br",{})]}),"\n",(0,s.jsx)(n.li,{children:"Books"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"dataset-setup",children:"Dataset Setup"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Let us start by running the copy dataset script"]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sh",children:"%run ../Includes/Copy-Datasets\n"})})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","We will ingest the new data from orders received in ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:".parquet"})})," Files"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"explore-the-data-source-directory",children:"Explore the data source directory"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Let us explore our data source directory"]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:"{1-3}",children:'files = dbutils.fs.ls(\r\n    f"{dataset_bookstore}/orders-raw"\r\n)\r\ndisplay(files)\n'})})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Currently we have only one parquet file in this directory"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"how-to-work-with-the-auto-loader-to-read-the-current-file",children:"How to work with the Auto Loader to read the current file"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Use ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"Auto Loader"})})," to read the current file in this directory and detect new files as they arrive to ingest them into a target table",(0,s.jsx)("br",{})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Use the ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"readStream"})})," and ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"writeStream"})})," methods from Spark Structured Streaming API to work with ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"Auto Loader"})})]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:"{1,11}",children:'spark.readStream.format(\r\n    "cloudFiles"\r\n).option(\r\n    "cloudFiles.format",\r\n    "parquet"\r\n).option(\r\n    "cloudFiles.schemaLocation",\r\n    "dbfs:/mnt/demo/orders_checkpoint"\r\n).load(\r\n    f"{dataset_bookstore}/orders_raw"\r\n).writeStream.option(\r\n    "checkpointLocation",\r\n    "dbfs:/mnt/demo/orders_checkpoint"\r\n).table(\r\n    "order_updates"\r\n)\n'})})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","The format here is ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"cloudFiles"})})," indicating that this is an ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"Auto Loader"})})," stream"]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:"{2}",children:'spark.readStream.format(\r\n    "cloudFiles"\r\n).option(\r\n    "cloudFiles.format",\r\n    "parquet"\r\n).option(\r\n    "cloudFiles.schemaLocation",\r\n    "dbfs:/mnt/demo/orders_checkpoint"\r\n).load(\r\n    f"{dataset_bookstore}/orders_raw"\r\n).writeStream.option(\r\n    "checkpointLocation",\r\n    "dbfs:/mnt/demo/orders_checkpoint"\r\n).table(\r\n    "order_updates"\r\n)\n'})})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","We provide two options ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"cloudFile.format"})})," and we describe that we are reading data files of type ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"parquet"})})]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:"{4-5}",children:'spark.readStream.format(\r\n    "cloudFiles"\r\n).option(\r\n    "cloudFiles.format",\r\n    "parquet"\r\n).option(\r\n    "cloudFiles.schemaLocation",\r\n    "dbfs:/mnt/demo/orders_checkpoint"\r\n).load(\r\n    f"{dataset_bookstore}/orders_raw"\r\n).writeStream.option(\r\n    "checkpointLocation",\r\n    "dbfs:/mnt/demo/orders_checkpoint"\r\n).table(\r\n    "order_updates"\r\n)\n'})})}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","The ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"schemaLocation"})}),", a directory in which ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"Auto Loader"})})," can store the information of the inferred schema."]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:"{7-8}",children:'spark.readStream.format(\r\n    "cloudFiles"\r\n).option(\r\n    "cloudFiles.format",\r\n    "parquet"\r\n).option(\r\n    "cloudFiles.schemaLocation",\r\n    "dbfs:/mnt/demo/orders_checkpoint"\r\n).load(\r\n    f"{dataset_bookstore}/orders_raw"\r\n).writeStream.option(\r\n    "checkpointLocation",\r\n    "dbfs:/mnt/demo/orders_checkpoint"\r\n).table(\r\n    "order_updates"\r\n)\n'})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","We provide the location of our data source files with the ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"load"})})," method"]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:"{9-10}",children:'spark.readStream.format(\r\n    "cloudFiles"\r\n).option(\r\n    "cloudFiles.format",\r\n    "parquet"\r\n).option(\r\n    "cloudFiles.schemaLocation",\r\n    "dbfs:/mnt/demo/orders_checkpoint"\r\n).load(\r\n    f"{dataset_bookstore}/orders_raw"\r\n).writeStream.option(\r\n    "checkpointLocation",\r\n    "dbfs:/mnt/demo/orders_checkpoint"\r\n).table(\r\n    "order_updates"\r\n)\n'})})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","We are chaining immediately the ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"writeStream"})})," to write the data into the target table ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"order_updates"})}),"`"]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:"{11-13}",children:'spark.readStream.format(\r\n    "cloudFiles"\r\n).option(\r\n    "cloudFiles.format",\r\n    "parquet"\r\n).option(\r\n    "cloudFiles.schemaLocation",\r\n    "dbfs:/mnt/demo/orders_checkpoint"\r\n).load(\r\n    f"{dataset_bookstore}/orders_raw"\r\n).writeStream.option(\r\n    "checkpointLocation",\r\n    "dbfs:/mnt/demo/orders_checkpoint"\r\n).table(\r\n    "order_updates"\r\n)\n'})})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","We provide the location for storing the checkpoint information which allows ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"Auto Loader"})})," to track the ingestion process"]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:"{14-16}",children:'spark.readStream.format(\r\n    "cloudFiles"\r\n).option(\r\n    "cloudFiles.format",\r\n    "parquet"\r\n).option(\r\n    "cloudFiles.schemaLocation",\r\n    "dbfs:/mnt/demo/orders_checkpoint"\r\n).load(\r\n    f"{dataset_bookstore}/orders_raw"\r\n).writeStream.option(\r\n    "checkpointLocation",\r\n    "dbfs:/mnt/demo/orders_checkpoint"\r\n).table(\r\n    "order_updates"\r\n)\n'})})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Let us run this command to begin our Auto Loader stream"]}),"\n",(0,s.jsx)(n.admonition,{type:"tip",children:(0,s.jsx)(n.p,{children:"Before running the command, notice that we are using the same directory for storing both the schema and the checkpoints"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","The ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"Auto Loader"})})," is a streaming query since it uses Spark Structured Streaming to load data incrementally"]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.p,{children:"This query will be continuously active. New data will be processed and loaded into the target table as soon as the new data arrives in the data source."})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Once the data has been ingested to Delta Lake by Auto Loader, we can interact with it the same way we would with any table",(0,s.jsx)("br",{})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","The data has been loaded well from our source directory. Let us check how many records we have in our table"]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"SELECT COUNT(*) FROM orders_updates\n"})})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","We are going to land the new data files in our source directory.",(0,s.jsx)("br",{})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","We use this helper function coming with our bookstore dataset to copy new files in our source directory"]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.p,{children:"This allows us to simulate somehow an external system writing data in this directory, and each time we execute the celll, a new file will be landed in our source directory"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Run the cell twice to add new data files. Be aware that each file has 1000 records",(0,s.jsx)("br",{})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Let us list the contents of our source directory again",(0,s.jsx)("br",{})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","We have two additional files added to the directory. Our Auto Loader stream is still active and can process these new data files",(0,s.jsx)("br",{})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","The Auto Loader has detected automatically that there are new data files in our source directory and it started processing them",(0,s.jsx)("br",{})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Let us run this query again to confirm that the data has been ingested"]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"SELECT COUNT(*) FROM orders_updates\n"})})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Let us finally explore our table history"]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"DESCRIBE HISTORY orders_updates\n"})})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","A new table version is indicated for each streaming update. These update events is related to the new batches of data arriving at the source",(0,s.jsx)("br",{})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Let us end up by dropping our table and removing the checkpoint location"]}),"\n",(0,s.jsxs)(n.admonition,{type:"info",children:[(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"DROP TABLE orders_updates\n"})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'dbutils.fs.rm(\r\n    "dbfs:/mnt/demo/orders_checkpoint",\r\n    True\r\n)\n'})})]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var s=t(6540);const r={},i=s.createContext(r);function a(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);