"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[4821],{4152:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>l});var t=s(4848),i=s(8453);const a={},o="4.3 Incremental Data Ingestion",r={id:"Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.3 Incremental Data Ingestion",title:"4.3 Incremental Data Ingestion",description:"Introduction",source:"@site/docs/Databricks Certified Data Engineer Associate - Preparation/4. Incremental Data Processing/4.3 Incremental Data Ingestion.md",sourceDirName:"Databricks Certified Data Engineer Associate - Preparation/4. Incremental Data Processing",slug:"/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.3 Incremental Data Ingestion",permalink:"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.3 Incremental Data Ingestion",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Databricks Certified Data Engineer Associate - Preparation/4. Incremental Data Processing/4.3 Incremental Data Ingestion.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"4.2 Structured Streaming (Hands On)",permalink:"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.2 Structured Streaming Hands On"},next:{title:"4.4 Auto Loader (Hands On)",permalink:"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.4 Auto Loader Hands On"}},c={},l=[{value:"Introduction",id:"introduction",level:2},{value:"What is incremental data ingestion?",id:"what-is-incremental-data-ingestion",level:2},{value:"COPY INTO",id:"copy-into",level:2},{value:"<strong><code>Auto Loader</code></strong>",id:"auto-loader",level:2},{value:"<strong><code>Auto Loader</code></strong> Checkpointing",id:"auto-loader-checkpointing",level:3},{value:"When to use <strong><code>Auto Loader</code></strong> vs COPY INTO command?",id:"when-to-use-auto-loader-vs-copy-into-command",level:2}];function d(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",input:"input",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"43-incremental-data-ingestion",children:"4.3 Incremental Data Ingestion"}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","We will talk about incremental data ingestion from files in Databricks",(0,t.jsx)("br",{})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","We will talk about two methods:",(0,t.jsx)("br",{})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"COPY INTO"})})," command;",(0,t.jsx)("br",{})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"Auto Loader"})})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"what-is-incremental-data-ingestion",children:"What is incremental data ingestion?"}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Incremental data ingestion is the ability to load data from new files that have been encountered since the last ingestion",(0,t.jsx)("br",{})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Each time we run our data pipeline, we don't need to reprocess the files we have processed before. We need to process only the new arriving data files",(0,t.jsx)("br",{})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Databricks provides 2 mechanisms for incrementally and efficiently processing new data files as they arrive in a storage location:",(0,t.jsx)("br",{})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"COPY INTO"})})," SQL command",(0,t.jsx)("br",{})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"Auto Loader"})})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"copy-into",children:"COPY INTO"}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"COPY INTO"})})," is a SQL command that allows a user to load data from a file location into a Delta table",(0,t.jsx)("br",{})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","The ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"COPY INTO"})})," command loads data idempotently and incrementally"]}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsx)(n.p,{children:"Idempotently means each time you run the command, it will load only the new files from the source location while the files that have been loaded before are simply skipped"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","The command is pretty simple. ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"COPY INTO"})})," a target table from a specific source location",(0,t.jsx)("br",{})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","We specify the format of the source file to load, for example, ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"CSV"})})," or ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"Parquet"})})," and any related format options",(0,t.jsx)("br",{})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","In adittion to any option to control the operation of the ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"COPY INTO"})})," command",(0,t.jsx)("br",{})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Here we are loading from a CSV file, having a ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"header"})})," and a specific ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"delimiter"})})]}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",metastring:"{5-6}",children:"COPY INTO my_table\nFROM '/path/to/files'\nFILEFORMAT=CSV\nFORMAT_OPTIONS (\n    'delimiter'='|',\n    'header'='true'\n)\nCOPY_OPTIONS (\n    'mergeSchema'='true'\n)\n"})})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","In the ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"COPY_OPTIONS"})}),", we are specifying that the schema can be evolved according to the incoming data"]}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",metastring:"{8-10}",children:"COPY INTO my_table\nFROM '/path/to/files'\nFILEFORMAT=CSV\nFORMAT_OPTIONS (\n    'delimiter'='|',\n    'header'='true'\n)\nCOPY_OPTIONS (\n    'mergeSchema'='true'\n)\n"})})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"auto-loader",children:(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"Auto Loader"})})}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","The second method to load data incrementally from files is ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"Auto Loader"})}),", which uses structured streaming in Spark to efficiently process new data files as they arrive in a storage location",(0,t.jsx)("br",{})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","You can use ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"Auto Loader"})})," to load billions of files into a table",(0,t.jsx)("br",{})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"Auto Loader"})})," can scale to support real time ingestion of millions of files per hour"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h3,{id:"auto-loader-checkpointing",children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"Auto Loader"})})," Checkpointing"]}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"Auto Loader"})})," uses checkpointing to track the ingestion process and to store metadata of the discovered files",(0,t.jsx)("br",{})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"Auto Loader"})})," ensures that data files are processed exactly once",(0,t.jsx)("br",{})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"Auto Loader"})})," can resume from where it left off if a failure occurs",(0,t.jsx)("br",{})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","With ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"Auto Loader"})}),", we use the ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"readStream"})})," and ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"writeStream"})})," methods",(0,t.jsx)("br",{})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"Auto Loader"})})," has a specific format of ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"StreamReader"})})," called ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"cloudFiles"})}),". And in order to specify the format of the source files, we use simply ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"cloudFiles.format"})})," option"]}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:"{2,4}",children:'spark.readStream.format(\n    "cloudFiles"\n).option(\n    "cloudFiles.format",\n    <source_format>\n).load(\n    "/path/to/files"\n).writeStream.option(\n    "checkpointLocation",\n    <checkpoint_directory>\n).table(\n    <table_name>\n)\n'})})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","The location of the source files is specified with the ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"load"})})," function"]}),"\n",(0,t.jsxs)(n.admonition,{type:"info",children:[(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"Auto Loader"})})," will detect new files as they arrive in this location and queue them for ingestion"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:"{7}",children:'spark.readStream.format(\n    "cloudFiles"\n).option(\n    "cloudFiles.format",\n    <source_format>\n).load(\n    "/path/to/files"\n).writeStream.option(\n    "checkpointLocation",\n    <checkpoint_directory>\n).table(\n    <table_name>\n)\n'})})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","We write the data into a target table using the ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"StreamWriter"})}),", where you provide the location to store the checkpointing information"]}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:"{9-10}",children:'spark.readStream.format(\n    "cloudFiles"\n).option(\n    "cloudFiles.format",\n    <source_format>\n).load(\n    "/path/to/files"\n).writeStream.option(\n    "checkpointLocation",\n    <checkpoint_directory>\n).table(\n    <table_name>\n)\n'})})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"Auto Loader"})})," can automatically configure the schema of your data. It can detect any update to the fields of the source dataset"]}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsx)(n.p,{children:"The inferred schema can be stored to a location to be used later"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Use the option ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"cloudFiles.schemaLocation"})})," to provide the location where ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"Auto Loader"})})," can store the schema"]}),"\n",(0,t.jsxs)(n.admonition,{type:"info",children:[(0,t.jsx)(n.p,{children:"This location could be simply the same as the checkpoint location"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:"{7-8}",children:'spark.readStream.format(\n    "cloudFiles"\n).option(\n    "cloudFiles.format",\n    <source_format>\n).option(\n    "cloudFiles.schemaLocation",\n    <schema_directory>\n).load(\n    "/path/to/files"\n).writeStream.option(\n    "checkpointLocation",\n    <checkpoint_directory>\n).option(\n    "mergeSchema",\n    "true"\n).table(\n    <table_name>\n)\n'})})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h2,{id:"when-to-use-auto-loader-vs-copy-into-command",children:["When to use ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"Auto Loader"})})," vs COPY INTO command?"]}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Use the ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"COPY INTO"})})," command to ingest thousands of files",(0,t.jsx)("br",{})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Use ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"Auto Loader"})})," to ingest millions of files or more over time",(0,t.jsx)("br",{})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"Auto Loader"})})," can split the processing into multiple batches so it is more efficient at scale"]}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsxs)(n.p,{children:["Databricks recommends to use ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"Auto Loader"})})," as general best practice when ingesting data from a cloud-object storage"]})}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>r});var t=s(6540);const i={},a=t.createContext(i);function o(e){const n=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);