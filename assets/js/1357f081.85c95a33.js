"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[9072],{15:(e,s,t)=>{t.r(s),t.d(s,{assets:()=>c,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>l,toc:()=>d});var n=t(4848),i=t(8453);const a={},r="4.1 Structured Streaming",l={id:"Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming",title:"4.1 Structured Streaming",description:"Introduction",source:"@site/docs/Databricks Certified Data Engineer Associate - Preparation/4. Incremental Data Processing/4.1 Structured Streaming.md",sourceDirName:"Databricks Certified Data Engineer Associate - Preparation/4. Incremental Data Processing",slug:"/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming",permalink:"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Databricks Certified Data Engineer Associate - Preparation/4. Incremental Data Processing/4.1 Structured Streaming.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"3.5 Higher Order Functions and SQL UDFs",permalink:"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.5 Higher Order Functions and SQL UDFs"},next:{title:"4.2 Structured Streaming (Hands On)",permalink:"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.2 Structured Streaming (Hands On)"}},c={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Data Stream",id:"data-stream",level:2},{value:"Approaches to data stream processing",id:"approaches-to-data-stream-processing",level:3},{value:"Spark Structured Streaming",id:"spark-structured-streaming",level:2},{value:"How to interact and query an infinite data source?",id:"how-to-interact-and-query-an-infinite-data-source",level:3},{value:"Input Streaming Table",id:"input-streaming-table",level:2},{value:"How to persist the result of a streaming query",id:"how-to-persist-the-result-of-a-streaming-query",level:3},{value:"Trigger methods",id:"trigger-methods",level:2},{value:"Trigger based on fixed intervals with <strong><code>processingTime</code></strong> option",id:"trigger-based-on-fixed-intervals-with-processingtime-option",level:3},{value:"Trigger based on available data with <strong><code>Once</code></strong> or <strong><code>availableNow</code></strong> option",id:"trigger-based-on-available-data-with-once-or-availablenow-option",level:3},{value:"Output Modes",id:"output-modes",level:2},{value:"Checkpointing",id:"checkpointing",level:2},{value:"Guarantees",id:"guarantees",level:2},{value:"Fault Tolerance",id:"fault-tolerance",level:3},{value:"Exactly-once guarantee",id:"exactly-once-guarantee",level:3},{value:"Unsupported Operations",id:"unsupported-operations",level:2}];function o(e){const s={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(s.h1,{id:"41-structured-streaming",children:"4.1 Structured Streaming"}),"\n",(0,n.jsx)(s.h2,{id:"introduction",children:"Introduction"}),"\n",(0,n.jsxs)(s.ul,{className:"contains-task-list",children:["\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:["\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ","This lecture focuses on Spark Structured Streaming in Databricks",(0,n.jsx)("br",{})]}),"\n"]}),"\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:["\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ","You will learn what is a ",(0,n.jsx)(s.strong,{children:"Data Stream"})," and how to process streaming data using ",(0,n.jsx)(s.strong,{children:"Spark Structured Streaming"}),(0,n.jsx)("br",{})]}),"\n"]}),"\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:["\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ","You will also understand how to use ",(0,n.jsx)(s.strong,{children:"DataStreamReader"})," to perform a stream read from a source and how to use and configure ",(0,n.jsx)(s.strong,{children:"DataStreamWriter"})," to perform a streaming write to sink"]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(s.h2,{id:"data-stream",children:"Data Stream"}),"\n",(0,n.jsxs)(s.ul,{className:"contains-task-list",children:["\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:["\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ","A ",(0,n.jsx)(s.strong,{children:"Data Stream"})," is any data source that grows over time"]}),"\n",(0,n.jsxs)(s.admonition,{type:"info",children:[(0,n.jsxs)(s.p,{children:["New data in a ",(0,n.jsx)(s.strong,{children:"Data Stream"})," might correspond to:",(0,n.jsx)("br",{})]}),(0,n.jsxs)(s.ul,{children:["\n",(0,n.jsxs)(s.li,{children:["A new JSON log file landing into a cloud storage",(0,n.jsx)("br",{})]}),"\n",(0,n.jsxs)(s.li,{children:["Updates to a database captured in a CDC or Change Data Capture feed",(0,n.jsx)("br",{})]}),"\n",(0,n.jsx)(s.li,{children:"Events queued in a pub/sub messaging feed like Kafka"}),"\n"]})]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(s.h3,{id:"approaches-to-data-stream-processing",children:"Approaches to data stream processing"}),"\n",(0,n.jsxs)(s.ul,{className:"contains-task-list",children:["\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ","There are 2 approaches to processing a data stream:",(0,n.jsx)("br",{})]}),"\n"]}),"\n",(0,n.jsxs)(s.ol,{children:["\n",(0,n.jsxs)(s.li,{children:["\n",(0,n.jsxs)(s.p,{children:["Reprocess the entire dataset each time you receive a new update to your data",(0,n.jsx)("br",{})]}),"\n"]}),"\n",(0,n.jsxs)(s.li,{children:["\n",(0,n.jsx)(s.p,{children:"Only capture files or records that have been added since the last time an update was run"}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(s.h2,{id:"spark-structured-streaming",children:"Spark Structured Streaming"}),"\n",(0,n.jsxs)(s.ul,{className:"contains-task-list",children:["\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:["\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ","Spark Structured Streaming is a scalable streaming processing engine",(0,n.jsx)("br",{})]}),"\n"]}),"\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:["\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ","Spark Structured Streaming allows you to query an infinite data source and automatically detect new data and process the result incrementally into a data sink",(0,n.jsx)("br",{})]}),"\n"]}),"\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:["\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ","A ",(0,n.jsx)(s.strong,{children:"sink"})," is just a durable file system, such as ",(0,n.jsx)(s.strong,{children:(0,n.jsx)(s.code,{children:"files"})})," or ",(0,n.jsx)(s.strong,{children:(0,n.jsx)(s.code,{children:"tables"})})]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(s.h3,{id:"how-to-interact-and-query-an-infinite-data-source",children:"How to interact and query an infinite data source?"}),"\n",(0,n.jsxs)(s.ul,{className:"contains-task-list",children:["\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:["\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ","Spark Structured Streaming allows the user to interact with a streaming source by treating the source as if it were a static table of records"]}),"\n",(0,n.jsx)(s.admonition,{type:"info",children:(0,n.jsx)(s.p,{children:"New data in the input data stream is simply treated as a new rows appended to a table"})}),"\n"]}),"\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:["\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ",'Such a table representing a infinite data source is seen as an "',(0,n.jsx)(s.strong,{children:"unbounded"}),'" table']}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(s.h2,{id:"input-streaming-table",children:"Input Streaming Table"}),"\n",(0,n.jsxs)(s.ul,{className:"contains-task-list",children:["\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:["\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ","An input data stream could be:"]}),"\n",(0,n.jsxs)(s.ul,{children:["\n",(0,n.jsxs)(s.li,{children:["A directory of files;",(0,n.jsx)("br",{})]}),"\n",(0,n.jsxs)(s.li,{children:["A messaging system like Kafka;",(0,n.jsx)("br",{})]}),"\n",(0,n.jsxs)(s.li,{children:["A Delta table. ",(0,n.jsx)("br",{})]}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:["\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ","Delta Lake is well-integrated with Spark Structured Streaming",(0,n.jsx)("br",{})]}),"\n"]}),"\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:["\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ","We can simply use ",(0,n.jsx)(s.strong,{children:(0,n.jsx)(s.code,{children:"spark.readStream"})})," to query the delta table as a streaming source, which allows to process:"]}),"\n",(0,n.jsxs)(s.ul,{children:["\n",(0,n.jsxs)(s.li,{children:["All of the data present in the table;",(0,n.jsx)("br",{})]}),"\n",(0,n.jsx)(s.li,{children:"Any new data that arrive later"}),"\n"]}),"\n",(0,n.jsxs)(s.admonition,{type:"info",children:[(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-python",children:'streamDF = spark.readStream.table("Input_Table")\n'})}),(0,n.jsx)(s.p,{children:"This creates a streaming data frame on which we can apply any transformation as if it were just a static data frame"})]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(s.h3,{id:"how-to-persist-the-result-of-a-streaming-query",children:"How to persist the result of a streaming query"}),"\n",(0,n.jsxs)(s.ul,{className:"contains-task-list",children:["\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:["\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ","To persist the result of a streaming query, we need to write them out to durable storage using dataframe.",(0,n.jsx)(s.strong,{children:(0,n.jsx)(s.code,{children:"writeStream"})})," method",(0,n.jsx)("br",{})]}),"\n"]}),"\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:["\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ","With the ",(0,n.jsx)(s.strong,{children:(0,n.jsx)(s.code,{children:"writeStream"})})," method, we can configure our output"]}),"\n",(0,n.jsxs)(s.admonition,{type:"info",children:[(0,n.jsxs)(s.p,{children:["We can trigger the streaming processing every ",(0,n.jsx)(s.strong,{children:(0,n.jsx)(s.code,{children:"2 minutes"})})," to check if there are new arriving records, and we choose to ",(0,n.jsx)(s.strong,{children:(0,n.jsx)(s.code,{children:"append"})})," them to the target table"]}),(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-python",metastring:"{4,6}",children:'streamDF = spark.readStream.table("Input_Table")\n\nstreamDF.writeStream.trigger(\n    processingTime="2 minutes"\n).outputMode(\n    "append"\n).option(\n    "checkpointLocation",\n    "/path"\n).table("Output_Table")\n'})}),(0,n.jsx)(s.p,{children:"All this happened thanks to checkpoints created by Spark to track the progress of your streaming processing"})]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(s.h2,{id:"trigger-methods",children:"Trigger methods"}),"\n",(0,n.jsxs)(s.ul,{className:"contains-task-list",children:["\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:["\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ","When defining a streaming write, the ",(0,n.jsx)(s.strong,{children:(0,n.jsx)(s.code,{children:"trigger"})})," method specify when the system should store the next set of data"]}),"\n",(0,n.jsxs)(s.admonition,{type:"info",children:[(0,n.jsxs)(s.p,{children:["This is called the ",(0,n.jsx)(s.strong,{children:(0,n.jsx)(s.code,{children:"trigger interval"})}),(0,n.jsx)("br",{})]}),(0,n.jsxs)(s.p,{children:["By default, if you don't provide any ",(0,n.jsx)(s.strong,{children:(0,n.jsx)(s.code,{children:"trigger interval"})}),", the data will be processed every half second"]})]}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(s.h3,{id:"trigger-based-on-fixed-intervals-with-processingtime-option",children:["Trigger based on fixed intervals with ",(0,n.jsx)(s.strong,{children:(0,n.jsx)(s.code,{children:"processingTime"})})," option"]}),"\n",(0,n.jsxs)(s.ul,{className:"contains-task-list",children:["\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ","You can specify a fixed interval. The data will be processed in micro batches at your specified interval, for example ",(0,n.jsx)(s.strong,{children:"every 5 minutes"})]}),"\n"]}),"\n",(0,n.jsxs)(s.h3,{id:"trigger-based-on-available-data-with-once-or-availablenow-option",children:["Trigger based on available data with ",(0,n.jsx)(s.strong,{children:(0,n.jsx)(s.code,{children:"Once"})})," or ",(0,n.jsx)(s.strong,{children:(0,n.jsx)(s.code,{children:"availableNow"})})," option"]}),"\n",(0,n.jsxs)(s.ul,{className:"contains-task-list",children:["\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:["\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ","You can run your stream in a ",(0,n.jsx)(s.strong,{children:"batch mode"})," to process all available data at ",(0,n.jsx)(s.strong,{children:"once"})," using either trigger ",(0,n.jsx)(s.strong,{children:(0,n.jsx)(s.code,{children:"Once"})})," option or ",(0,n.jsx)(s.strong,{children:(0,n.jsx)(s.code,{children:"availableNow"})})," option",(0,n.jsx)("br",{})]}),"\n"]}),"\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:["\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ","With the ",(0,n.jsx)(s.strong,{children:(0,n.jsx)(s.code,{children:"Once"})})," and ",(0,n.jsx)(s.strong,{children:(0,n.jsx)(s.code,{children:"availableNow"})})," option, the trigger will stop on its own once all available data is processed",(0,n.jsx)("br",{})]}),"\n"]}),"\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:["\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ","The difference between ",(0,n.jsx)(s.strong,{children:(0,n.jsx)(s.code,{children:"Once"})})," and ",(0,n.jsx)(s.strong,{children:(0,n.jsx)(s.code,{children:"availableNow"})})," is:",(0,n.jsx)("br",{})]}),"\n",(0,n.jsxs)(s.ul,{children:["\n",(0,n.jsxs)(s.li,{children:["With ",(0,n.jsx)(s.strong,{children:(0,n.jsx)(s.code,{children:"Once"})}),", all data will be processed in a ",(0,n.jsx)(s.strong,{children:"single batch"}),";",(0,n.jsx)("br",{})]}),"\n",(0,n.jsxs)(s.li,{children:["With ",(0,n.jsx)(s.strong,{children:(0,n.jsx)(s.code,{children:"availableNow"})}),", all data is processed in ",(0,n.jsx)(s.strong,{children:"multiple micro batches"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(s.h2,{id:"output-modes",children:"Output Modes"}),"\n",(0,n.jsxs)(s.ul,{className:"contains-task-list",children:["\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:["\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ","In ",(0,n.jsx)(s.strong,{children:(0,n.jsx)(s.code,{children:"append"})})," mode, only ",(0,n.jsx)(s.strong,{children:"new rows are incrementally appended"})," to the target table with each batch",(0,n.jsx)("br",{})]}),"\n"]}),"\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:["\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ","In ",(0,n.jsx)(s.strong,{children:(0,n.jsx)(s.code,{children:"complete"})})," mode, the result table is recalculated each time a write is triggered, so the target table is ",(0,n.jsx)(s.strong,{children:"overwritten with each batch"})]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(s.h2,{id:"checkpointing",children:"Checkpointing"}),"\n",(0,n.jsxs)(s.ul,{className:"contains-task-list",children:["\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:["\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ","Databricks creates checkpoints by storing the current state of your streaming job to cloud storage"]}),"\n",(0,n.jsx)(s.admonition,{type:"info",children:(0,n.jsx)(s.p,{children:"An important note here is that checkpoints cannot be shared between several streams"})}),"\n"]}),"\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:["\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ","A separate ",(0,n.jsx)(s.strong,{children:"checkpoint location"})," is required for every streaming write to ensure processing guarantees"]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(s.h2,{id:"guarantees",children:"Guarantees"}),"\n",(0,n.jsxs)(s.ul,{className:"contains-task-list",children:["\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ","Structured streaming provide two guarantees:"]}),"\n"]}),"\n",(0,n.jsx)(s.h3,{id:"fault-tolerance",children:"Fault Tolerance"}),"\n",(0,n.jsxs)(s.ul,{className:"contains-task-list",children:["\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:["\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ","The streaming agent can resume from where it left off if there is a failure",(0,n.jsx)("br",{})]}),"\n"]}),"\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:["\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ",(0,n.jsx)(s.strong,{children:(0,n.jsx)(s.code,{children:"checkpointing"})})," and the mechanism called ",(0,n.jsx)(s.strong,{children:(0,n.jsx)(s.code,{children:"write-ahead logs"})})," allows the user to track the streaming progress by recording the offset range of data being processed during each trigger interval"]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(s.h3,{id:"exactly-once-guarantee",children:"Exactly-once guarantee"}),"\n",(0,n.jsxs)(s.ul,{className:"contains-task-list",children:["\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:["\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ","Structured streaming also ensures ",(0,n.jsx)(s.strong,{children:"exactly once"})," data processing because the streaming sink are designed to be idempotent"]}),"\n",(0,n.jsx)(s.admonition,{type:"info",children:(0,n.jsxs)(s.p,{children:["Multiple writes of the same data identified by the offset, do ",(0,n.jsx)(s.strong,{children:"not"})," result in duplicates being written to the sink"]})}),"\n"]}),"\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:["\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ","The two guarantees here only work if the streaming source is repeatable, like cloud-based object storage or pub/sub messaging service",(0,n.jsx)("br",{})]}),"\n"]}),"\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:["\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ","Repeatable data sources and idempotent sinks allows Spark Structured Streaming to ensure end-to-end exactly once semantics under any failure condition"]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(s.h2,{id:"unsupported-operations",children:"Unsupported Operations"}),"\n",(0,n.jsxs)(s.ul,{className:"contains-task-list",children:["\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:["\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ","We need to understand that some operations are not supported by streaming data frames",(0,n.jsx)("br",{})]}),"\n"]}),"\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:["\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ","Operations such as ",(0,n.jsx)(s.strong,{children:"sorting"})," and ",(0,n.jsx)(s.strong,{children:"deduplication"}),", are either too complex or logically not possible to do when working with streaming data",(0,n.jsx)("br",{})]}),"\n"]}),"\n",(0,n.jsxs)(s.li,{className:"task-list-item",children:["\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.input,{type:"checkbox",disabled:!0})," ","There are advanced streaming methods like ",(0,n.jsx)(s.strong,{children:"windowing"})," and ",(0,n.jsx)(s.strong,{children:"watermarking"})," that can help to do such operations"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:s}={...(0,i.R)(),...e.components};return s?(0,n.jsx)(s,{...e,children:(0,n.jsx)(o,{...e})}):o(e)}},8453:(e,s,t)=>{t.d(s,{R:()=>r,x:()=>l});var n=t(6540);const i={},a=n.createContext(i);function r(e){const s=n.useContext(a);return n.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function l(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),n.createElement(a.Provider,{value:s},e.children)}}}]);