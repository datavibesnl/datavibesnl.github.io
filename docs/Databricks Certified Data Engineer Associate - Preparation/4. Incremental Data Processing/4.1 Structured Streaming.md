# 4.1 Structured Streaming

## Introduction

- [ ] This lecture focuses on Spark Structured Streaming in Databricks<br/>

- [ ] You will learn what is a **Data Stream** and how to process streaming data using **Spark Structured Streaming**<br/>

- [ ] You will also understand how to use **DataStreamReader** to perform a stream read from a source and how to use and configure **DataStreamWriter** to perform a streaming write to sink

## Data Stream

- [ ] A **Data Stream** is any data source that grows over time

    :::info
        New data in a **Data Stream** might correspond to:<br/>
        
        - A new JSON log file landing into a cloud storage<br/>
        - Updates to a database captured in a CDC or Change Data Capture feed<br/>
        - Events queued in a pub/sub messaging feed like Kafka
    :::

### Approaches to data stream processing

- [ ] There are 2 approaches to processing a data stream:<br/>

1. Reprocess the entire dataset each time you receive a new update to your data<br/>

2. Only capture files or records that have been added since the last time an update was run


## Spark Structured Streaming

- [ ] Spark Structured Streaming is a scalable streaming processing engine<br/>

- [ ] Spark Structured Streaming allows you to query an infinite data source and automatically detect new data and process the result incrementally into a data sink<br/>

- [ ] A **sink** is just a durable file system, such as **`files`** or **`tables`**

### How to interact and query an infinite data source?

- [ ] Spark Structured Streaming allows the user to interact with a streaming source by treating the source as if it were a static table of records

    :::info
        New data in the input data stream is simply treated as a new rows appended to a table
    :::

- [ ] Such a table representing a infinite data source is seen as an "**unbounded**" table

## Input Streaming Table

- [ ] An input data stream could be:

    - A directory of files;<br/>
    - A messaging system like Kafka;<br/>
    - A Delta table. <br/>

- [ ] Delta Lake is well-integrated with Spark Structured Streaming<br/>

- [ ] We can simply use **`spark.readStream`** to query the delta table as a streaming source, which allows to process:

    - All of the data present in the table;<br/>
    - Any new data that arrive later

    :::info
        ```python
        streamDF = spark.readStream.table("Input_Table")
        ```

        This creates a streaming data frame on which we can apply any transformation as if it were just a static data frame
    :::

### How to persist the result of a streaming query

- [ ] To persist the result of a streaming query, we need to write them out to durable storage using dataframe.**`writeStream`** method<br/>

- [ ] With the **`writeStream`** method, we can configure our output

    :::info
        We can trigger the streaming processing every **`2 minutes`** to check if there are new arriving records, and we choose to **`append`** them to the target table

        ```python {4,6}
        streamDF = spark.readStream.table("Input_Table")

        streamDF.writeStream.trigger(
            processingTime="2 minutes"
        ).outputMode(
            "append"
        ).option(
            "checkpointLocation",
            "/path"
        ).table("Output_Table")
        ```

        All this happened thanks to checkpoints created by Spark to track the progress of your streaming processing
    :::

## Trigger methods

- [ ] When defining a streaming write, the **`trigger`** method specify when the system should store the next set of data

    :::info
        This is called the **`trigger interval`**<br/>

        By default, if you don't provide any **`trigger interval`**, the data will be processed every half second
    :::

### Trigger based on fixed intervals with **`processingTime`** option

- [ ] You can specify a fixed interval. The data will be processed in micro batches at your specified interval, for example **every 5 minutes**

### Trigger based on available data with **`Once`** or **`availableNow`** option

- [ ] You can run your stream in a **batch mode** to process all available data at **once** using either trigger **`Once`** option or **`availableNow`** option<br/>

- [ ] With the **`Once`** and **`availableNow`** option, the trigger will stop on its own once all available data is processed<br/>

- [ ] The difference between **`Once`** and **`availableNow`** is:<br/>

    - With **`Once`**, all data will be processed in a **single batch**;<br/>
    - With **`availableNow`**, all data is processed in **multiple micro batches**

## Output Modes

- [ ] In **`append`** mode, only **new rows are incrementally appended** to the target table with each batch<br/>

- [ ] In **`complete`** mode, the result table is recalculated each time a write is triggered, so the target table is **overwritten with each batch**

## Checkpointing

- [ ] Databricks creates checkpoints by storing the current state of your streaming job to cloud storage

    :::info
        An important note here is that checkpoints cannot be shared between several streams
    :::

- [ ] A separate **checkpoint location** is required for every streaming write to ensure processing guarantees

## Guarantees

- [ ] Structured streaming provide two guarantees:

### Fault Tolerance

- [ ] The streaming agent can resume from where it left off if there is a failure<br/>

- [ ] **`checkpointing`** and the mechanism called **`write-ahead logs`** allows the user to track the streaming progress by recording the offset range of data being processed during each trigger interval


### Exactly-once guarantee

- [ ] Structured streaming also ensures **exactly once** data processing because the streaming sink are designed to be idempotent

    :::info
        Multiple writes of the same data identified by the offset, do **not** result in duplicates being written to the sink
    :::

- [ ] The two guarantees here only work if the streaming source is repeatable, like cloud-based object storage or pub/sub messaging service<br/>

- [ ] Repeatable data sources and idempotent sinks allows Spark Structured Streaming to ensure end-to-end exactly once semantics under any failure condition

## Unsupported Operations

- [ ] We need to understand that some operations are not supported by streaming data frames<br/>

- [ ] Operations such as **sorting** and **deduplication**, are either too complex or logically not possible to do when working with streaming data<br/>

- [ ] There are advanced streaming methods like **windowing** and **watermarking** that can help to do such operations