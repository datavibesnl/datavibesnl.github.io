# 4.3 Incremental Data Ingestion

## Introduction

- [ ] We will talk about incremental data ingestion from files in Databricks<br/>

- [ ] We will talk about two methods:<br/>

    - **`COPY INTO`** command;<br/>

    - **`Auto Loader`**

## What is incremental data ingestion?

- [ ] Incremental data ingestion is the ability to load data from new files that have been encountered since the last ingestion<br/>

- [ ] Each time we run our data pipeline, we don't need to reprocess the files we have processed before. We need to process only the new arriving data files<br/>

- [ ] Databricks provides 2 mechanisms for incrementally and efficiently processing new data files as they arrive in a storage location:<br/>

    - **`COPY INTO`** SQL command<br/>

    - **`Auto Loader`**

## COPY INTO

- [ ] **`COPY INTO`** is a SQL command that allows a user to load data from a file location into a Delta table<br/>

- [ ] The **`COPY INTO`** command loads data idempotently and incrementally

    :::info
        Idempotently means each time you run the command, it will load only the new files from the source location while the files that have been loaded before are simply skipped
    :::

- [ ] The command is pretty simple. **`COPY INTO`** a target table from a specific source location<br/>

- [ ] We specify the format of the source file to load, for example, **`CSV`** or **`Parquet`** and any related format options<br/>

- [ ] In adittion to any option to control the operation of the **`COPY INTO`** command<br/>

- [ ] Here we are loading from a CSV file, having a **`header`** and a specific **`delimiter`**

    :::info
        ```sql {5-6}
        COPY INTO my_table
        FROM '/path/to/files'
        FILEFORMAT=CSV
        FORMAT_OPTIONS (
            'delimiter'='|',
            'header'='true'
        )
        COPY_OPTIONS (
            'mergeSchema'='true'
        )
        ```
    :::

- [ ] In the **`COPY_OPTIONS`**, we are specifying that the schema can be evolved according to the incoming data

    :::info
        ```sql {8-10}
        COPY INTO my_table
        FROM '/path/to/files'
        FILEFORMAT=CSV
        FORMAT_OPTIONS (
            'delimiter'='|',
            'header'='true'
        )
        COPY_OPTIONS (
            'mergeSchema'='true'
        )
        ```
    :::

## **`Auto Loader`**

- [ ] The second method to load data incrementally from files is **`Auto Loader`**, which uses structured streaming in Spark to efficiently process new data files as they arrive in a storage location<br/>

- [ ] You can use **`Auto Loader`** to load billions of files into a table<br/>

- [ ] **`Auto Loader`** can scale to support real time ingestion of millions of files per hour

### **`Auto Loader`** Checkpointing

- [ ] **`Auto Loader`** uses checkpointing to track the ingestion process and to store metadata of the discovered files<br/>

- [ ] **`Auto Loader`** ensures that data files are processed exactly once<br/>

- [ ] **`Auto Loader`** can resume from where it left off if a failure occurs<br/>

- [ ] With **`Auto Loader`**, we use the **`readStream`** and **`writeStream`** methods<br/>

- [ ] **`Auto Loader`** has a specific format of **`StreamReader`** called **`cloudFiles`**. And in order to specify the format of the source files, we use simply **`cloudFiles.format`** option

    :::info
        ```python {2,4}
        spark.readStream.format(
            "cloudFiles"
        ).option(
            "cloudFiles.format",
            <source_format>
        ).load(
            "/path/to/files"
        ).writeStream.option(
            "checkpointLocation",
            <checkpoint_directory>
        ).table(
            <table_name>
        )
        ```
    :::

- [ ] The location of the source files is specified with the **`load`** function 

    :::info
        **`Auto Loader`** will detect new files as they arrive in this location and queue them for ingestion

        ```python {7}
        spark.readStream.format(
            "cloudFiles"
        ).option(
            "cloudFiles.format",
            <source_format>
        ).load(
            "/path/to/files"
        ).writeStream.option(
            "checkpointLocation",
            <checkpoint_directory>
        ).table(
            <table_name>
        )
        ```
    :::

- [ ] We write the data into a target table using the **`StreamWriter`**, where you provide the location to store the checkpointing information

    :::info
        ```python {9-10}
        spark.readStream.format(
            "cloudFiles"
        ).option(
            "cloudFiles.format",
            <source_format>
        ).load(
            "/path/to/files"
        ).writeStream.option(
            "checkpointLocation",
            <checkpoint_directory>
        ).table(
            <table_name>
        )
        ```
    :::

- [ ] **`Auto Loader`** can automatically configure the schema of your data. It can detect any update to the fields of the source dataset

    :::info
        The inferred schema can be stored to a location to be used later
    :::

- [ ] Use the option **`cloudFiles.schemaLocation`** to provide the location where **`Auto Loader`** can store the schema

    :::info
        This location could be simply the same as the checkpoint location

        ```python {7-8}
        spark.readStream.format(
            "cloudFiles"
        ).option(
            "cloudFiles.format",
            <source_format>
        ).option(
            "cloudFiles.schemaLocation",
            <schema_directory>
        ).load(
            "/path/to/files"
        ).writeStream.option(
            "checkpointLocation",
            <checkpoint_directory>
        ).option(
            "mergeSchema",
            "true"
        ).table(
            <table_name>
        )
        ```
    :::

## When to use **`Auto Loader`** vs COPY INTO command?

- [ ] Use the **`COPY INTO`** command to ingest thousands of files<br/>

- [ ] Use **`Auto Loader`** to ingest millions of files or more over time<br/>

- [ ] **`Auto Loader`** can split the processing into multiple batches so it is more efficient at scale

    :::info
        Databricks recommends to use **`Auto Loader`** as general best practice when ingesting data from a cloud-object storage
    :::