# 4.6 Multihop Architecture (Hands On)


## Introduction
- [ ] In this notebook, we will create a Delta Lake multi-hop pipeline<br/>

- [ ] We will continue using our bookstore dataset, with its 3 tables: <br/>

    - Customers;<br/>
    - Orders;<br/>
    - Books

## Dataset setup

- [ ] Let us start by running the **`Copy-Datasets`** script

    :::info
        ```sh
        %run ../Includes/Copy-Datasts
        ```
    :::

## Explore source directory

- [ ] Before starting, let us first check our source directory

    :::info
        ```python
        files = dbutils.fs.ls(
            f"{dataset_bookstore}/orders-raw"
        )
        display(files)
        ```
    :::

- [ ] Currently we have 3 **`.parquet`** files in our source directory<br/>

## How to implement the bronze layer

- [ ] We start usually by creating an Auto Loader against our source directory<br/>

- [ ] Here, we are configuring a **`readStream`** on our **`parquet`** source using Auto Loader with schema inference

    :::info
        ```python {7-8}
        spark.readStream.format(
            "cloudFiles"
        ).option(
            "cloudFiles.format",
            "parquet"
        ).option(
            "cloudFiles.schemaLocation",
            "dbfs:/mnt/demo/checkpoints/orders_raw"
        ).load(
            f"{dataset_bookstore}/orders-raw"
        ).createOrReplaceTempView(
            "orders_raw_temp"
        )
        ```
    :::

- [ ] We register a streaming temporary view to do data transformation in Spark SQL<br/>

- [ ] Our temporary view is named **`orders_raw_temp`**

    :::info
        ```python {11-13}
        spark.readStream.format(
            "cloudFiles"
        ).option(
            "cloudFiles.format",
            "parquet"
        ).option(
            "cloudFiles.schemaLocation",
            "dbfs:/mnt/demo/checkpoints/orders_raw"
        ).load(
            f"{dataset_bookstore}/orders-raw"
        ).createOrReplaceTempView(
            "orders_raw_temp"
        )
        ```
    :::

- [ ] Our stream has been created, but notice that it is not active yet until we do a **`display`** or **`writeStream`** operation<br/>

- [ ] We will enrich our raw data with additional metadata describing the source file and the time it was ingested

    :::info
        Such information is useful for troubleshooting errors if corrupted data is encountered.

        ```sql {4-5}
        CREATE OR REPLACE TEMPORARY VIEW orders_tmp AS (
            SELECT 
                *,
                current_timestamp(),
                input_file_name() AS source_file
            FROM orders_raw_temp
        
        )
        ```
    :::

- [ ] Let us take a look on our enriched raw data

    :::info
        ```sql
        SELECT * FROM orders_tmp
        ```
    :::

- [ ] Our stream is active now, and we can see that we have successfully inserted the data with the metadata of the arrival time and the source file<br/>

- [ ] Let us cancel this stream for now<br/>

- [ ] We are going to pass this enriched data back to PySpark API to process an incremental write to a Delta Lake table called **`orders_bronze`**

    :::info
        ```python
        spark.table(
            "orders_tmp"
        ).writeStream.format(
            "delta"
        ).option(
            "checkpointLocation",
            "dbfs:/mnt/demo/checkpoints/orders_bronze"
        ).outputMode(
            "append"
        ).table("orders_bronze")
        ```
    :::

- [ ] Our **`writeStream`** is active now, and we can see that it has started processing our data<br/>

- [ ] Let us check how many records were written into the bronze table

    :::info
        ```sql
        SELECT COUNT(*) FROM orders_bronze
        ```

        3000 records have been written corresponding to our three files, where each file contain 1000 records
    :::

- [ ] We can trigger another file arrival using the following function

    :::info
        ```python
        load_new_data()
        ```
    :::

- [ ] Let us come back to our active stream and see what is happening. We see that the new data is immediately detected by the streaming query<br/>

- [ ] We can confirm this by querying the number of records again in our table

## How to implement the silver layer

- [ ] We can move on to work on our second hop, the silver layer

### Create a new temporary view from JSON files

- [ ] Here we are creating a customers static temporary view from JSON files<br/>

- [ ] We are doing it with PySpark API, but we can also do it with Spark SQL

    :::info
        ```python
        spark.read.format(
            "json"
        ).load(
            f"{dataset_bookstore}/customers-json"
        ).createOrReplaceTempView(
            "customers_lookup"
        )
        ```
    :::

- [ ] Let us take a look at our **`customers_lookup`** temporary view

    :::info
        ```sql
        SELECT * FROM customers_lookup
        ```
    :::

- [ ] Here we have 3 columns:<br/>
    
    - The customer **`id`** that we will use it for joining the data;<br/>
    - The customer **`email`**;<br/>
    - The **`profile`** information as a complex JSON object<br/>

### Create a streaming temporary view from an existing table

- [ ] To work on our bronze data in the silver layer, we will start by creating a streaming temporary view against our bronze table

    :::info
        ```python
        spark.readStream.table(
            "orders_bronze"
        ).createOrReplaceTempView(
            "orders_bronze_tmp"
        )
        ```
    :::

### Combine orders with customers data

- [ ] In this silver level, we are doing several enrichments and checks<br/>

- [ ] We join the order data with the customers information to add customers names

    :::info
        ```sql {6-7,16-17}
        CREATE OR REPLACE TEMPORARY VIEW orders_enriched_tmp AS (
            SELECT 
                order_id,
                quantity,
                o.customer_id,
                c.profile:first_name AS f_name,
                c.profile:last_name AS l_name,
                CAST(
                    from_unixtime(
                        order_timestamp, 
                        'yyyy-MM-dd HH:mm:ss'
                    ) AS timestamp
                ) AS order_timestamp,
                books
            FROM orders_bronze_tmp AS o
            INNER JOIN customers_lookup c
            ON o.customer_id = c.customer_id
            WHERE quantity > 0
        )
        ```
    :::

### Parse Unix timestamp into human-readable format

- [ ] We parse the **`order_timestamp`** from Unix timestamp into human readable format

    :::info
        ```sql {8-13}
        CREATE OR REPLACE TEMPORARY VIEW orders_enriched_tmp AS (
            SELECT 
                order_id,
                quantity,
                o.customer_id,
                c.profile:first_name AS f_name,
                c.profile:last_name AS l_name,
                CAST(
                    from_unixtime(
                        order_timestamp, 
                        'yyyy-MM-dd HH:mm:ss'
                    ) AS timestamp
                ) AS order_timestamp,
                books
            FROM orders_bronze_tmp AS o
            INNER JOIN customers_lookup c
            ON o.customer_id = c.customer_id
            WHERE quantity > 0
        )
        ```
    :::

### Exclude orders without items

- [ ] We exclude any order with no items

    :::info
        ```sql {18}
        CREATE OR REPLACE TEMPORARY VIEW orders_enriched_tmp AS (
            SELECT 
                order_id,
                quantity,
                o.customer_id,
                c.profile:first_name AS f_name,
                c.profile:last_name AS l_name,
                CAST(
                    from_unixtime(
                        order_timestamp, 
                        'yyyy-MM-dd HH:mm:ss'
                    ) AS timestamp
                ) AS order_timestamp,
                books
            FROM orders_bronze_tmp AS o
            INNER JOIN customers_lookup c
            ON o.customer_id = c.customer_id
            WHERE quantity > 0
        )
        ```
    :::

### Create a writeStream to persist data into a silver table

- [ ] Let us do a **`writeStream`** for this **`orders_enriched_tmp`** data into a silver table

    :::info
        ```python
        spark.table(
            "orders_enriched_tmp"
        ).writeStream.format(
            "delta"
        ).option(
            "checkpointLocation",
            "dbfs:/mnt/demo/checkpoints/orders_silver"
        ).outputMode(
            "append"
        ).table(
            "orders_silver"
        )
        ```
    :::

- [ ] The data has been processed with the stream. Let's query our silver table

    :::info
        ```sql
        SELECT * FROM orders_silver
        ```
    :::

- [ ] Let us now check how many records do we have?

    :::info
        ```sql
        SELECT COUNT(*) FROM orders_silver
        ```
    :::

- [ ] Let us trigger another new file and wait for it to propagate through the previous two streams, from bronze to silver layer

    :::info
        ```python
        load_new_data()
        ```
    :::

- [ ] Let us see our stream dashboard. The new data has been received. Let us check the counts in the silver table

    :::info
        ```sql
        SELECT COUNT(*) FROM orders_silver
        ```
    :::

## How to implement the gold layer

- [ ] Let us now work on the gold layer<br/>

- [ ] We need a stream of data from the silver table into a streaming temporary view

    :::info
        ```python
        spark.readStream.table(
            "orders_silver"
        ).createOrReplaceTempView(
            "orders_silver_tmp"
        )
        ```
    :::

### How to aggregate data from a streaming data source

- [ ] We write another stream to create an aggregate gold table for the daily number of books for each customer

    :::info
        ```sql
        CREATE OR REPLACE TEMP VIEW daily_customer_books_tmp AS (
            SELECT
                customer_id,
                f_name,
                l_name,
                date_trunc(
                    "DD",
                    order_timestamp
                ) AS order_date,
                SUM(quantity) AS books_counts
            FROM orders_silver_tmp
            GROUP BY 
                customer_id, 
                f_name,
                l_name,
                date_trunc(
                    "DD", 
                    order_timestamp
                )
        )
        ```
    :::

### How to persist aggregated data into the gold table

- [ ] Let us write this aggregated data into a gold table called **`daily_customer_books`**

    :::info
        ```python
        spark.table(
            "daily_customer_books_tmp"
        ).writeStream.format(
            "delta"
        ).outputMode(
            "complete"
        ).option(
            "checkpointLocation",
            "dbfs:/mnt/demo/checkpoints/daily_customer_books"
        ).trigger(
            availableNow=True
        ).table(
            "daily_customer_books"
        )
        ```
    :::

    :::note
        The stream stopped on its own after processing all the available data in micro batches. This is because we are using trigger **`availableNow`** option
    :::

- [ ] With this way, we can combine streaming and batch workloads in the same pipeline<br/>

- [ ] We are also using the **`complete`** output mode to rewrite the updated aggregation each time our logic runs

    :::info
        Keep in mind that Structured Streaming assumes data is only being appended in the upstream tables. Once a table is updated or overwritten, it is no longer valid for streaming
    :::

    :::note
        In our case here, we cannot read a stream from this gold table
    :::

- [ ] To change this behavior, you can set options like **`ignoreChanges`**, but they have other limitations<br/>

- [ ] Let us now query the gold table

    :::info
        ```sql
        SELECT * FROM daily_customer_books
        ```
    :::

- [ ] Here we can see our aggregated data. Customers currently have books counts between 5 and 10<br/>

- [ ] Let us finally land all the remaining data files in our source directory

    :::info
        ```python
        load_new_data(all=True)
        ```
    :::

- [ ] The data will be propagated from our source directory into the bronze, silver layer, until the gold layer<br/>

- [ ] For the gold layer, we need to rerun our final query to update the gold table. Since the query is configured as a batch job using the trigger **`availableNow`** syntax<br/>

- [ ] Let us rerun this stream query

    :::info
        ```python
        spark.table(
            "daily_customer_books_tmp"
        ).writeStream.format(
            "delta"
        ).outputMode(
            "complete"
        ).option(
            "checkpointLocation",
            "dbfs:/mnt/demo/checkpoints/daily_customer_books"
        ).trigger(
            availableNow=True
        ).table(
            "daily_customer_books"
        )
        ```
    :::

- [ ] All the available data has been processed. Let us confirm this by re-query our gold table

    :::info
        ```sql
        SELECT * FROM daily_customer_books
        ```
    :::

- [ ] Now we can see customers having more books counts after processing the new changes

## Cleanup

- [ ] Let us end up by stopping all the active streams by running this **`for`** loop

    :::info
        ```python
        for stream in spark.streams.active:
            print("Stopping stream: " + stream.id)
            stream.stop()
            stream.awaitTermination()
        ```
    :::