# 4.2 Structured Streaming (Hands On)

## Introduction
- [ ] We will explore the basics of working with Spark Structured Streaming to allow incremental processing of data<br/>

- [ ] We will continue using our bookstore dataset, with its 3 tables: 

1. Customers;
2. Orders;
3. Books

## Dataset setup

- [ ] Let us first copy our dataset

    :::info
        ```sh
        %run ../Includes/Copy-Datasets
        ```
    :::

## How to work with data streaming in Spark SQL

- [ ] To work with data streaming in SQL, you must first use **`spark.readStream`** method PySpark API<br/>

- [ ] **`spark.readStream`** method allows to query a Delta table as a stream source

    :::info
        ```python
        spark.readStream.table(
            "books"
        )
        ```
    :::

- [ ] We can register a temporary view against this stream source

    :::info
        ```python {3-5}
        spark.readStream.table(
            "books"
        ).createOrReplaceTempView(
            "books_streaming_tmp_vw"
        )
        
        ```
    :::

- [ ] The temporary view created here is a "*streaming*" temporary view that allows to apply most transformations in SQL the same way as we would with the static data<br/>

## How to query a streaming temporary view

- [ ] Let us first query this streaming temporary view

    :::info
        ```sql
        SELECT * FROM books_streaming_tmp_vw
        ```
    :::

- [ ] As you can see, the query is still running, waiting for any new data to be displayed here<br/>

- [ ] We don't display a streaming result unless a human is actively monitoring the output of a query during development or live dashboarding<br/>

- [ ] Let us now apply some aggregations on this streaming temporary view

    :::info
        ```sql {3,5}
        SELECT 
            author, 
            COUNT(book_id) AS total_books
        FROM books_streaming_tmp_vw
        GROUP BY author
        ```
    :::

- [ ] We are querying a streaming temporary view, this becomes a streaming query that executes infinitely, rather than completing after retrieving a single set of results. And here we are just displaying an aggregation of input as seen by the stream<br/>

- [ ] For streaming queries like this, we can always explore an interactive dashboard to monitor the streaming performance

    ::::warning
        When working with streaming data, some operations are not supported like **sorting**

        :::info
            ```sql 
            SELECT * FROM books_streaming_tmp_vw
            ORDER BY author
            ```
        
            You can use advanced methods like **windowing** and **watermarking** to achieve such operations
        :::
    ::::

## How to persist incremental results

- [ ] In order to persist incremental results, we need first to pass our logic back to PySpark DataFrame API<br/>

- [ ] We are creating another temporary view<br/>

- [ ] Since we are creating this temporary view from the result of a query against a streaming temporary view

    :::info
        ```sql
        CREATE OR REPLACE TEMP VIEW author_counts_tmp_vw AS (
            SELECT author, COUNT(book_id) AS total_books
            FROM books_streaming_tmp_vw
            GROUP BY author
        )
        ```
    :::

- [ ] In PySpark DataFrame API, we can use the **`spark.table()`** to load data from a streaming temporary view back to a DataFrame

    :::note
        Spark always load streaming views as a streaming DataFrames, and static views as a static DataFrames, meaning that incremental processing must be defined from the very beginning with Read logic to support later an incremental writing
    :::

- [ ] We are using DataFrame **`writeStream`** method to persist the result of a streaming query to a durable storage<br/>

- [ ] This allows us to configure the output with the three settings<br/>

1. The trigger **intervals**, here **every 4 seconds**.

    :::info
        ```python {4}
        spark.table(
            "author_counts_tmp_vw"
        ).writeStream.trigger(
            processingTime='4 seconds'
        ).outputMode(
            'complete'
        ).option(
            'checkpointLocation',
            'dbfs:/mnt/demo/author_counts_checkpoint'
        ).table(
            'author_counts'
        )
        ```
    :::

2. The **output mode**, either **`append`** or **`complete`**.
   
    :::warning
        For aggregation streaming queries, we must always use **`complete`** mode to overwrite the table with the new calculation 
    :::

    :::info
        ```python {6}
        spark.table(
            "author_counts_tmp_vw"
        ).writeStream.trigger(
            processingTime='4 seconds'
        ).outputMode(
            'complete'
        ).option(
            'checkpointLocation',
            'dbfs:/mnt/demo/author_counts_checkpoint'
        ).table(
            'author_counts'
        )
        ```
    :::

3. The **`checkpointLocation`** to help tracking the progress of the streaming processing

    :::info
        ```python {8-9}
        spark.table(
            "author_counts_tmp_vw"
        ).writeStream.trigger(
            processingTime='4 seconds'
        ).outputMode(
            'complete'
        ).option(
            'checkpointLocation',
            'dbfs:/mnt/demo/author_counts_checkpoint'
        ).table(
            'author_counts'
        )
        ```
    :::

- [ ] You can think about such a streaming query as an **always-on incremental query**, and we can always explore its interactive dashboard<br/>

- [ ] From this dashboard, we can see that the data has been processed and we can now query our target table<br/>

- [ ] Our table has been written to the target table, the **`author_counts`** table, and we can see that each author has currently only 1 book

    :::info
        ```sql
        SELECT * FROM author_counts
        ```
    :::

    :::warning
        What you see here is not a streaming query simply because we are querying the table directly. I mean, not as a streaming source through a streaming DataFrame
    :::

## What happens when new data arrives?

- [ ] When we execute a streaming query, the streaming query will continue to update as new data arrives in the source<br/>

- [ ] To confirm this, let us add new data to our source table<br/>

- [ ] Let us run this query and see what will happen in our streaming

    :::info
        ```sql
        INSERT INTO books
        VALUES (
            "B19", 
            "Introduction to Modeling and Simulation",
            "Mark W. Spong",
            "Computer Science",
            25
        ),
        (
            "B20",
            "Robot Modeling and Control",
            "Mark W. Spong",
            "Computer Science",
            30
        ),
        (
            "B21",
            "Turing's Vision: The Birth of Computer Science",
            "Chris Bernhardt",
            "Computer Science", 
            35
        )
        ```
    :::

- [ ] We can see that there is new data arriving. Let us query our target tabel again to see the updated books counts for each author

    :::info
        ```sql
        SELECT * FROM author_counts
        ```
    :::

- [ ] Let us come back to our streaming query, and cancel it to see another scenario

    :::warning
        Always remember to cancel any active stream in your notebook, otherwise the stream will be always on and prevents the cluster from auto termination
    :::

## How to create a triggered incremental batch with the **`availableNow`** trigger

- [ ] For our last scenario, we will add some books for new authors to our source table

    :::info
        ```sql
        INSERT INTO books
        VALUES (
            "B16", 
            "Hands-On Deep Learning Algorithms with Python",
            "Sudharsan Ravichandiran",
            "Computer Science",
            25
        ),
        (
            "B17",
            "Neural Network Methods in Natural Language Processing",
            "Yoav Goldberg",
            "Computer Science",
            30
        ),
        (
            "B18",
            "Understanding digital signal processing",
            "Richard Lyons",
            "Computer Science", 
            35
        )
        ```
    :::

- [ ] Use the **`availableNow`** option to modify the **`trigger`** method to change our query from an always-on query triggered every **`4 seconds`** to a triggered incremental batch<br/>


- [ ] With the **`availableNow`** **`trigger`** option, the query will process all new available data and stop on its own after execution

    :::info
        ```python {4}
        spark.table(
            "author_counts_tmp_vw"
        ).writeStream.trigger(
            availableNow=True
        ).outputMode(
            "complete"
        ).option(
            "checkpointLocation",
            "dbfs:/mnt/demo/author_counts_checkpoint"
        ).table(
            "author_counts"
        ).awaitTermination()
        ```
    :::

- [ ] Use the **`awaitTermination`** method to block the execution of any cell in this notebook until the incremental batch's write has succeeded

    :::info
        ```python {12}
        spark.table(
            "author_counts_tmp_vw"
        ).writeStream.trigger(
            availableNow=True
        ).outputMode(
            "complete"
        ).option(
            "checkpointLocation",
            "dbfs:/mnt/demo/author_counts_checkpoint"
        ).table(
            "author_counts"
        ).awaitTermination()
        ```
    :::

- [ ] With the **`availableNow`** trigger option, the query run in a batch mode<br/>

- [ ] It is executed to process all the available data and then stop on its own<br/>

- [ ] Let us finally query the target table again to see the updated data

    :::info
        ```sql
        SELECT * FROM author_counts
        ```
    :::