# 4.4 Auto Loader (Hands On)

## Introduction

- [ ] In this notebook, we will explore incremental data ingestion from files using **`Auto Loader`**<br/>

- [ ] We will continue using our bookstore dataset with its 3 tables:<br/>

    - Customers;<br/>
    - Orders;<br/>
    - Books

## Dataset Setup

- [ ] Let us start by running the copy dataset script

    :::info
        ```sh
        %run ../Includes/Copy-Datasets
        ```
    :::

- [ ] We will ingest the new data from orders received in **`.parquet`** Files

## Explore the data source directory

- [ ] Let us explore our data source directory

    :::info
        ```python {1-3}
        files = dbutils.fs.ls(
            f"{dataset_bookstore}/orders-raw"
        )
        display(files)
        ```
    :::

- [ ] Currently we have only one parquet file in this directory

## How to work with the Auto Loader to read the current file

- [ ] Use **`Auto Loader`** to read the current file in this directory and detect new files as they arrive to ingest them into a target table<br/>

- [ ] Use the **`readStream`** and **`writeStream`** methods from Spark Structured Streaming API to work with **`Auto Loader`**

    :::info
        ```python {1,11}
        spark.readStream.format(
            "cloudFiles"
        ).option(
            "cloudFiles.format",
            "parquet"
        ).option(
            "cloudFiles.schemaLocation",
            "dbfs:/mnt/demo/orders_checkpoint"
        ).load(
            f"{dataset_bookstore}/orders_raw"
        ).writeStream.option(
            "checkpointLocation",
            "dbfs:/mnt/demo/orders_checkpoint"
        ).table(
            "order_updates"
        )
        ```
    :::

- [ ] The format here is **`cloudFiles`** indicating that this is an **`Auto Loader`** stream

    :::info
        ```python {2}
        spark.readStream.format(
            "cloudFiles"
        ).option(
            "cloudFiles.format",
            "parquet"
        ).option(
            "cloudFiles.schemaLocation",
            "dbfs:/mnt/demo/orders_checkpoint"
        ).load(
            f"{dataset_bookstore}/orders_raw"
        ).writeStream.option(
            "checkpointLocation",
            "dbfs:/mnt/demo/orders_checkpoint"
        ).table(
            "order_updates"
        )
        ```
    :::

- [ ] We provide two options **`cloudFile.format`** and we describe that we are reading data files of type **`parquet`**

    :::info
        ```python {4-5}
        spark.readStream.format(
            "cloudFiles"
        ).option(
            "cloudFiles.format",
            "parquet"
        ).option(
            "cloudFiles.schemaLocation",
            "dbfs:/mnt/demo/orders_checkpoint"
        ).load(
            f"{dataset_bookstore}/orders_raw"
        ).writeStream.option(
            "checkpointLocation",
            "dbfs:/mnt/demo/orders_checkpoint"
        ).table(
            "order_updates"
        )
        ```
    :::

  - [ ] The **`schemaLocation`**, a directory in which **`Auto Loader`** can store the information of the inferred schema.

    :::info
        ```python {7-8}
        spark.readStream.format(
            "cloudFiles"
        ).option(
            "cloudFiles.format",
            "parquet"
        ).option(
            "cloudFiles.schemaLocation",
            "dbfs:/mnt/demo/orders_checkpoint"
        ).load(
            f"{dataset_bookstore}/orders_raw"
        ).writeStream.option(
            "checkpointLocation",
            "dbfs:/mnt/demo/orders_checkpoint"
        ).table(
            "order_updates"
        )
        ```
    :::

- [ ] We provide the location of our data source files with the **`load`** method 

    :::info
        ```python {9-10}
        spark.readStream.format(
            "cloudFiles"
        ).option(
            "cloudFiles.format",
            "parquet"
        ).option(
            "cloudFiles.schemaLocation",
            "dbfs:/mnt/demo/orders_checkpoint"
        ).load(
            f"{dataset_bookstore}/orders_raw"
        ).writeStream.option(
            "checkpointLocation",
            "dbfs:/mnt/demo/orders_checkpoint"
        ).table(
            "order_updates"
        )
        ```
    :::

- [ ] We are chaining immediately the **`writeStream`** to write the data into the target table **`order_updates`**`

    :::info
        ```python {11-13}
        spark.readStream.format(
            "cloudFiles"
        ).option(
            "cloudFiles.format",
            "parquet"
        ).option(
            "cloudFiles.schemaLocation",
            "dbfs:/mnt/demo/orders_checkpoint"
        ).load(
            f"{dataset_bookstore}/orders_raw"
        ).writeStream.option(
            "checkpointLocation",
            "dbfs:/mnt/demo/orders_checkpoint"
        ).table(
            "order_updates"
        )
        ```
    :::

- [ ] We provide the location for storing the checkpoint information which allows **`Auto Loader`** to track the ingestion process

    :::info
        ```python {14-16}
        spark.readStream.format(
            "cloudFiles"
        ).option(
            "cloudFiles.format",
            "parquet"
        ).option(
            "cloudFiles.schemaLocation",
            "dbfs:/mnt/demo/orders_checkpoint"
        ).load(
            f"{dataset_bookstore}/orders_raw"
        ).writeStream.option(
            "checkpointLocation",
            "dbfs:/mnt/demo/orders_checkpoint"
        ).table(
            "order_updates"
        )
        ```
    :::


- [ ] Let us run this command to begin our Auto Loader stream

    :::tip
        Before running the command, notice that we are using the same directory for storing both the schema and the checkpoints
    :::

- [ ] The **`Auto Loader`** is a streaming query since it uses Spark Structured Streaming to load data incrementally

    :::info
        This query will be continuously active. New data will be processed and loaded into the target table as soon as the new data arrives in the data source.
    :::


- [ ] Once the data has been ingested to Delta Lake by Auto Loader, we can interact with it the same way we would with any table<br/>

- [ ] The data has been loaded well from our source directory. Let us check how many records we have in our table

    :::info
        ```sql
        SELECT COUNT(*) FROM orders_updates
        ```
    :::

- [ ] We are going to land the new data files in our source directory.<br/>

- [ ] We use this helper function coming with our bookstore dataset to copy new files in our source directory

    :::info
        This allows us to simulate somehow an external system writing data in this directory, and each time we execute the celll, a new file will be landed in our source directory
    :::

- [ ] Run the cell twice to add new data files. Be aware that each file has 1000 records<br/>

- [ ] Let us list the contents of our source directory again<br/>

- [ ] We have two additional files added to the directory. Our Auto Loader stream is still active and can process these new data files<br/>

- [ ] The Auto Loader has detected automatically that there are new data files in our source directory and it started processing them<br/>

- [ ] Let us run this query again to confirm that the data has been ingested

    :::info
        ```sql
        SELECT COUNT(*) FROM orders_updates
        ```
    :::

- [ ] Let us finally explore our table history

    :::info
        ```sql
        DESCRIBE HISTORY orders_updates
        ```
    :::

- [ ] A new table version is indicated for each streaming update. These update events is related to the new batches of data arriving at the source<br/>

- [ ] Let us end up by dropping our table and removing the checkpoint location

    :::info
        ```sql
        DROP TABLE orders_updates
        ```

        ```python
        dbutils.fs.rm(
            "dbfs:/mnt/demo/orders_checkpoint",
            True
        )
        ```
    :::