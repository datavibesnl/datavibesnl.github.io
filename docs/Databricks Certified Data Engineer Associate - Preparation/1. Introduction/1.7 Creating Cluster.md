# 1.7 Creating a Cluster

## Walkthrough - How to setup a single-node cluster

- [ ] The next step in setting up your environment is to create a cluster.<br/>

### Navigate to the Compute menu

- [ ] Navigate to **`Compute`** in the left side bar<br/>

- [ ] From here, we can create and manage our clusters

    :::info
        Remember, a cluster is a set of nodes or computers working together like a single entity.<br/>

        It consists of a master node called the **`Driver`** and some other worker nodes.<br/>
        
        The **Driver node** is responsible for coordinating the *workers* and their parallel execution of tasks
    :::

- [ ] Under all-purpose compute, click **`Create compute`**<br/>

- [ ] Click on the default name in order to change it. Name your cluster as **`Demo Cluster`**, for example<br/>

- [ ] Leave the policy **`Unrestricted`** to create fully configurable cluster<br/>

- [ ] Your cluster could be **multi-node**, that is having multiple workers, or simply a **single node**

    :::info
        A single node has no workers and run Spark jobs on the driver node
    :::

- [ ] For this course, a single node is enough. However, let us continue to see how to configure a **multi-node** cluster

## Overview of multi-node cluster configuration options

- [ ] For the **access mode**, you can allow your cluster to be shared by multiple users

    :::info
        However, only SQL and Python workloads will be supported<br/>
        Choose **`Single user`** if you are the only one to use this cluster
    :::

### Select the Databricks Runtime version (VM image)

- [ ] You need to select the Databricks runtime version

    :::info
        Databricks runtime is the virtual machine's image that comes with pre-installed libraries, which has a specific version of Spark, Scala and other libraries
    :::

- [ ] Choose 11.3 LTS, which is the latest version at the time of recording this course

    :::note
        You can choose to activate **`Photon`**, which is a vectorized query engine developed in **C++** to enhance Spark performance
    :::

### Specify the configuration of the worker nodes

#### Specify the requirements for the worker nodes

- [ ] You can go ahead and select the configuration of your worker nodes

    :::info
        These are different virtual machines sizes provided by your cloud provider<br/>

        Depending on your requirements of memory cores and hard disk, you can select the configuration
    :::

- [ ] Let's keep the default one <br/>

#### Specify the number of worker nodes

- [ ] You can select the **number of workers** you need for your cluster <br/>

### Enable auto scaling

- [ ] If you choose to enable auto scaling, then provide a range for the number of workers

    :::info
        These allow Databricks to resize your cluster automatically within this range<br/>

        Otherwise disable auto scaling and use a fixed number of workers
    :::

- [ ] Let me select **`3`** here <br/>

### Specify the configuration for the driver node

- [ ] You can now select the configuration for the **Driver node** or simply keep it the same as the worker <br/>

### Enable auto-termination

- [ ] You can enable **auto-termination** of the cluster by providing the **number of minutes** <br/>

- [ ] Let's say 30 minutes. That is, if there is no activity for 30 minutes, the cluster will auto-terminate <br/>

### Check the cluster configuration summary

- [ ] On the right, you can see a summary of your cluster configuration. Here you can see the number of DBUs 

    :::info
        **`DBU`** stand for Databricks Unit and it is a unit of a processing capability per hour
    :::

    :::note
        Each configuration tells you how much **`DBUs`** would be consumed if a virtual machine runs for an hour and then you pay for each DBU consumed
    :::

    :::info
        For example, if we have less number of workers or even a single node cluster, we will have less DBUs
    :::

## Walkthrough - How to setup a single-node cluster \[Continued\]

- [ ] Select **`Single Node`** cluster <br/>

- [ ] Click **`Confirm`**

    :::info
        With a single node cluster, we are going to consume less DBUs
    :::

- [ ] Hit the **`Create`** button to finish creating our cluster

    :::info
        Azure will now go ahead and provision the required virtual machine with a specific configuration and libraries as specified by Databricks runtime
    :::

- [ ] Our cluster is now up and running <br/>

- [ ] To access your cluster at any time, you can simply navigate to **`Compute`** in the left side bar <br/>

- [ ] You can see your clusters are listed here with its current status **`Running`** or **`Terminated`**. <br/>

- [ ] You can even edit the cluster by clicking on its name. Then click **`Edit`**

    :::info
        Remember, changing the cluster configuration may require a restart of the cluster
    :::

- [ ] Let's cancel this <br/>

- [ ] In the cluster page, you can notice two things

1. The **event log** shows all the events that have happened with the cluster 

    :::info
        For example, when the cluster was created or terminated. This helps to track the activity on a cluster
    :::

2. In the **driver log**, you will get the log generated within the cluster notebooks and libraries <br/>

- [ ] Let's now terminate our cluster