# 5.1 Delta Live Tables (Hands On)

## What are Delta Live Tables?

- [ ] Delta Live Tables or DLT is a framework for building reliable and maintainable data processing pipelines<br/>

- [ ] DLT simplify the hard work of building large scale ETL while maintaining table dependencies and data quality<br/>

- [ ] Our DLT multi-hop pipeline is well-visualized and we can see our two bronze tables, **`customers`** and **`orders_raw`**. They are joined together into the silver table **`orders_cleaned`**. From which we calculate our gold table **`daily_customer_books`**<br/>

- [ ] DLT pipelines are implemented using Databricks notebooks<br/>

- [ ] On the **pipeline details** on the right, we can see the path to the notebook containing the DLT table definitions<br/>

- [ ] We can simply click here to navigate to the source code<br/>

## Explore the source code

- [ ] Let us explore the content of this notebook to better understand the syntax used by Delta Live Tables<br/>

- [ ] In this SQL notebook, we declare our Delta Live Tables that together implement a simple multi-hop architecture<br/>

- [ ] DLT tables will always be preceded by the **`LIVE`** keyword<br/>

- [ ] We start by declaring two tables implementing the bronze layer<br/>

## Second bronze table - orders_raw

- [ ] The table **`orders_raw`** ingest Parquet data incrementally by Auto Loader from our dataset directory<br/>

- [ ] Incremental processing via Auto Loader required the addition of the **`STREAMING`** keyword in that declaration<br/>

- [ ] The **`cloud_files`** method enable auto loader to be used natively with SQL<br/>

- [ ] This method takes three parameters

1. The data file source location

    :::info
        ```sql {4}
        CREATE OR REFRESH STREAMING LIVE TABLE orders_raw
        COMMENT "The raw books orders, ingested from orders-raw"
        AS SELECT *  FROM cloud_files(
            "${datasets_path}/orders-raw",
            "parquet",
            map(
                "schema",
                "order_id STRING,
                order_timestamp LONG,
                customer_id STRING,
                quantity LONG"
            )
        )
        ```
    :::

2. The source data format, which is **`parquet`** in this case

    :::info
        ```sql {5}
        CREATE OR REFRESH STREAMING LIVE TABLE orders_raw
        COMMENT "The raw books orders, ingested from orders-raw"
        AS SELECT *  FROM cloud_files(
            "${datasets_path}/orders-raw",
            "parquet",
            map(
                "schema",
                "order_id STRING,
                order_timestamp LONG,
                customer_id STRING,
                quantity LONG"
            )
        )
        ```
    :::

3. An array of reader options

    :::info
        We declare the schema of our data

        ```sql {6-12}
        CREATE OR REFRESH STREAMING LIVE TABLE orders_raw
        COMMENT "The raw books orders, ingested from orders-raw"
        AS SELECT *  FROM cloud_files(
            "${datasets_path}/orders-raw",
            "parquet",
            map(
                "schema",
                "order_id STRING,
                order_timestamp LONG,
                customer_id STRING,
                quantity LONG"
            )
        )
        ```
    :::

    :::note
        Notice that we add a comment here that would be visible to anyone exploring the data catalog
    :::

- [ ] Let us run this query and see what will happen

    :::info
        ```sql
        CREATE OR REFRESH STREAMING LIVE TABLE orders_raw
        COMMENT "The raw books orders, ingested from orders-raw"
        AS SELECT *  FROM cloud_files(
            "${datasets_path}/orders-raw",
            "parquet",
            map(
                "schema",
                "order_id STRING,
                order_timestamp LONG,
                customer_id STRING,
                quantity LONG"
            )
        )
        ```
    :::

- [ ] Running at a DLT query from here only validate that it is syntactically valid<br/>

- [ ] To define and populate this table, you must create a DLT pipeline<br/>

- [ ] We will see later how to configure and run a new pipeline from this notebook

## Second bronze table - customer

- [ ] The second bronze table is **`customer`** that presents JSON customer data

    :::info
        This table is used below in a join operation to look up customer information

        ```sql
        CREATE OR REFRESH LIVE TABLE customers
        COMMENT "The customers lookup table, ingested from customers-json"
        AS SELECT * FROM json.`${datasets_path}/customers-json`
        ```
    :::

- [ ] We declare tables implementing the silver layer

    :::info
        This layer represents a refined copy of data from the bronze layer
    :::

    :::note
        At this level, we apply operations like data cleansing and enrichment
    :::

- [ ] Here we declare our silver table **`orders_cleaned`**, which enriches the order's data with customer information<br/>

## How to implement quality control with Delta Live Tables

- [ ] In addition, we implement quality control using **`CONSTRAINT`** keywords. Here we reject records with no **`order_id`**<br/>

- [ ] The **`CONSTRAINT`** keyword enables DLT to collect metrics on constraint violations<br/>

- [ ] It provides an optional **`ON VIOLATION`** clause specifying an action to take on records that violate the constraints<br/>

- [ ] The three modes currently supported by Delta are included in this table<br/>

1. **`DROP ROW`** where we discard records that violate constraints<br/>

2. **`FAIL UPDATE`** where the pipeline fails when the constraint is violated<br/>

3. When omitted records violating **`CONSTRAINT`** will be included, but violation will be reported in the metrics

    :::note
        Notice also that we need to use the **`LIVE`** prefix in order to refer to other DLT tables
    :::

- [ ] For streaming DLT tables we need to use the **`STREAM`** method

    :::info
        ```sql {20}
        CREATE OR REFRESH STREAMING LIVE TABLE orders_cleaned (
            CONSTRAINT valid_order_number EXPECT (
                order_id IS NOT NULL
            ) ON VIOLATION DROP ROW
        )
        COMMENT "The cleaned books orders with valid order_id"
        AS SELECT 
            order_id,
            quantity,
            o.customer_id,
            c.profile:first_name AS f_name,
            c.profile:last_name as l_name,
            CAST(
                from_unixtime(
                    order_timestamp,
                    'yyyy-MM-dd HH:mm:ss'
                ) AS timestamp
            ) AS order_timestamp,
            c.profile:address:country AS country
        FROM STREAM(
            LIVE.orders_raw
        ) AS o
        LEFT JOIN LIVE.customers c
        ON o.customer_id = c.customer_id
        ```
    :::

- [ ] We declare the gold table, in this case the daily number of books per customer in a specific region

    :::info
        Here it is China

        ```sql
        CREATE OR REFRESH LIVE TABLE cn_daily_customer_books
        COMMENT "Daily number of books per customer in China"
        AS SELECT 
            customer_id,
            f_name,
            l_name,
            DATE_TRUNC(
                "DD",
                order_timestamp
            ) AS order_date,
            SUM(quantity) AS book_counts
        FROM LIVE.orders_cleaned
        WHERE country = 'China'
        GROUP BY 
            customer_id, 
            f_name,
            l_name,
            DATE_TRUNC(
                "DD",
                order_timestamp
            )
        ```
    :::

- [ ] Let us see now how to use this notebook to create a new DLT pipeline<br/>

- [ ] Start by navigating to the **`Workflows`** tab on the sidebar<br/>

- [ ] Select the **`Delta Live Table`** tab<br/>

- [ ] Click **`Create Pipeline`**<br/>

- [ ] Fill in a pipeline name

    :::info
        *For example, demo bookstore*
    :::

- [ ] For notebook libraries use the navigator to locate and select the notebook with the delta tables definition, the one we have just explored<br/>

- [ ] Under configuration, add a new configuration parameter. Set the key to **`dataset_path`**, and the value to the location of the bookstore dataset

    :::info
        This parameter is used in the notebook in order to specify the path to our source data files

        ```sh
        datasets_path=dbfs:/mnt/demo-datasets/bookstore
        ```
    :::

- [ ] In the storage location field enter a path where the pipeline logs and data files will be stored

    :::info
        dbfs:/mnt/demo/dlt/demo_bookstore
    :::

- [ ] In the target field, enter a target database name

    :::info
        demo_bookstore_dlt_db
    :::

- [ ] The pipeline mode specifies how the pipeline will be run

    :::info
        Triggered pipelines run once and then shut down until the next manual or scheduled updates<br/>

        Continuous pipeline will continuously ingesting new data as it arrives
    :::

- [ ] For this demo, let us keep it triggered<br/>

- [ ] A new cluster would be created for our DLT pipeline<br/>

- [ ] Choose the cluster mode

    :::info
        For example, fixed size
    :::

- [ ] We set the number of workers to zero to create a single node cluster

    :::note
        Notice below the DBUs estimate provided similar to that provided when configuring interactive clusters
    :::

- [ ] Click Create<br/>

- [ ] Select Development to run the pipeline in development mode

    :::info
        This mode allows for interactive development by reusing the cluster, compared to creating a new cluster for each run in the prediction mode<br/>

        The development mode also disable retries so that we can quickly identify and fix errors
    :::

- [ ] Let us now click **`Start`**

    :::info
        The initial run will take several minutes while the cluster is provisioned
    :::

- [ ] Below we see all the events of our running pipeline, either information, warning, or errors<br/>

- [ ] On the right hand side, we see all the pipeline details and also the information related to the cluster<br/>

- [ ] In the middle, we see the execution flow visualized as a Directed Acyclic Graph or DAG

    :::info
        This representing the entities involved in the pipeline and the relationships between them
    :::

- [ ] Click on each entity to view a summary which includes the run status and other metadata summary, including the comment we set during the table definition in the notebook<br/>

- [ ] We can also see the schema of the table<br/>

- [ ] If you select the **`orders_cleaned`** table, you can notice the results reported in the data quality section

    :::info
        Because this flow has data expectation declared, those metrics are extracted here
    :::

- [ ] As you can see, we have no records violating our constraint.<br/>

- [ ] Let us now come back to our notebook for adding another table and see how this change is reflected here<br/>

- [ ] Let us open the notebook of this pipeline by clicking on the link here<br/>

- [ ] Let us scroll to the end of this notebook<br/>

- [ ] Add a new cell<br/>

- [ ] We will add a new table similar to the previous gold table declaration. But this time, instead of China, we will filter for France. But let us do something different to see what happens if we remove, for example, the **`LIVE`** prefix

    :::info
        ```sql
        CREATE OR REFRESH LIVE TABLE cn_daily_customer_books
        COMMENT "Daily number of books per customer in China"
        AS SELECT 
            customer_id,
            f_name,
            l_name,
            DATE_TRUNC(
                "DD",
                order_timestamp
            ) AS order_date,
            SUM(quantity) AS book_counts
        FROM orders_cleaned
        WHERE country = 'France'
        GROUP BY 
            customer_id, 
            f_name,
            l_name,
            DATE_TRUNC(
                "DD",
                order_timestamp
            )
        ```
    :::

- [ ] Let us see what will happen in our pipeline<br/>

- [ ] Let us now click start again to rerun our pipeline and examine the updated results<br/>

- [ ] As you can see, this generates an error. Table or view not found, because we missed the LIVE namespace<br/>

- [ ] Add again the **`LIVE`** keyword

    :::info
        ```sql {12}
        CREATE OR REFRESH LIVE TABLE cn_daily_customer_books
        COMMENT "Daily number of books per customer in China"
        AS SELECT 
            customer_id,
            f_name,
            l_name,
            DATE_TRUNC(
                "DD",
                order_timestamp
            ) AS order_date,
            SUM(quantity) AS book_counts
        FROM LIVE.orders_cleaned
        WHERE country = 'France'
        GROUP BY 
            customer_id, 
            f_name,
            l_name,
            DATE_TRUNC(
                "DD",
                order_timestamp
            )
        ```
    :::

- [ ] Let us rerun our pipeline by clicking **`Start`**<br/>

- [ ] Our pipeline is successfully completed and we can see now our two gold tables<br/>

- [ ] The events and information we see in this UI are stored in the storage configuration we provided during configuring our pipeline<br/>

- [ ] Let us explore this directory. For this, let us create a Python notebook and copy the storage location<br/>

- [ ] Let us see the content of our pipeline storage location

    :::info
        ```python
        files = dbutils.fs.ls(
            "dbfs:/mnt/demo/dlt/demo_bookstore"
        )
        
        display(files)
        ```
    :::

- [ ] There are four directories: auto loader, checkpoints, system and tables<br/>

- [ ] The system directory captures all the events associated with the pipeline<br/>

- [ ] Let us explore the events file in the system directory

    :::info
        ```python
        files = dbutils.fs.ls(
            "dbfs:/mnt/demo/dlt/demo_bookstore/system/events"
        )
        
        display(files)
        ```

        These event logs are stored as a delta table
    :::

- [ ] Let us query this table

    :::info
        ```sql
        SELECT 
            * 
        FROM delta.`dbfs:/mnt/demo/dlt/demo_bookstore/system/events`
        ```
    :::

- [ ] All events we see in the UI are stored in this data table<br/>

- [ ] Let us also see what we have in the tables directory in the storage location

    :::info
        ```python
        files = dbutils.fs.ls(
            "dbfs:/mnt/demo/dlt/demo_bookstore/tables"
        )
        
        display(files)
        ```
    :::

- [ ] Here we see our five DLT tables<br/>

- [ ] Let us come back to our pipeline to grab the database name to query these tables<br/>

- [ ] If you click on any table, you can see the metastore information<br/>

- [ ] Let us write a select query on our table using the metadata store information

    :::info
        ```sql
        SELECT 
            *
        FROM demo_bookstore_dlt_db.cn_daily_customer_books
        
        ```
    :::

- [ ] Let us end up by turning off our job cluster. To do so navigate to the compute tab in the left side bar<br/>

- [ ] Click on the job clusters tab<br/>

- [ ] Terminate this pipeline cluster