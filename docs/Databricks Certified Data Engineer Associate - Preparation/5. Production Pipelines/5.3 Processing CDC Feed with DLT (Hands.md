# 5.3 Processing CDC Feed with DLT (Hands On)

- [ ] In this demo, we are going to process change data capture feed with Delta Live Tables<br/>

- [ ] Let us first pull the latest version of our course materials from GitHub. Go to the Repos tab and click on the **`main`** branch<br/>

- [ ] On the right, click **`Pull`**. **`Confirm`**<br/>

- [ ] Before we see our DLT pipeline, let us take a look on the CDC data we will process during this demo. Our CDC data is delivered in JSON files<br/>

- [ ] We have now a new JSON file **`02.json`**<br/>

- [ ] Let us take a look on the content of this file.

    :::info
        ```sql
        SELECT 
            *
        FROM json.`${dataset.bookstore}/books-cdc/02.json`
        ```
    :::

- [ ] Here is the CDC data of our **`books`** table. As you can see, in addition to the book's data, we have two operational columns

1. **`row_status`**

    :::info
        **`row_status`** contains **`INSERT`**, **`UPDATE`**, **`DELETE`** operation indicating whether the specified record was inserted, updated or deleted
    :::

    :::note
        **`DELETE`** operations contain **`NULL`** values for all the fields other than the book ID
    :::

2. **`row_time`**

    :::info
        **`row_time`** indicates when these changes happened
    :::

    :::note
        The **`row_time`** will be used as sequence key in our CDC data processing
    :::

- [ ] Let us now switch to our pipeline <br/>

- [ ] In addition to the previous notebook we saw in the previous demo, we are adding a new notebook to this pipeline

    :::info
        We will see later how we can edit an existing pipeline to add additional notebooks
    :::

- [ ] Let us take a look on our new notebook

## Creating the bronze table layer

- [ ] In this notebook, we start by creating a bronze table to ingest books CDC feed <br/>

- [ ] We are using auto loader to load the JSON files incrementally

    :::info
        ```sql
        CREATE OR REFRESH STREAMING LIVE TABLE books_bronze
        COMMENT "The raw books data, ingested from CDC feed"
        AS SELECT * FROM cloud_files(
            "${datasets.path}/books-cdc",
            "json"
        )
        ```
    :::

## Creating the silver table layer

- [ ] We are creating the silver table

    :::info
        This is our target table into which the changes from the CDC feed will be applied
    :::

- [ ] We start by declaring the table. Since **`APPLY CHANGES INTO`** requires the target table to be declared in a separate statement <br/>

- [ ] With a target table created, we can write the **`APPLY CHANGES INTO`** command <br/>

- [ ] In this command, we specify the table **`books_silver`** as the target table <br/>

- [ ] The table **`books_bronze`** as the streaming source of our CDC feed

    :::info
        ```sql {4}
        CREATE OR REFRESH STREAMING LIVE TABLE books_silver;
        
        APPLY CHANGES INTO LIVE.books_silver
        FROM STREAM(LIVE.books_bronze)
        KEYS (book_id)
        APPLY AS DELETE WHEN row_status = "DELETE"
        SEQUENCE BY row_time
        COLUMNS * EXCEPT (
            row_status,
            row_time
        )
        ```
    :::

- [ ] Then we identify the **`book_id`** as the primary key<br/>

- [ ] If the key exists in the target table, the record will be updated. If not it will be inserted

    :::info
        ```sql {5}
        CREATE OR REFRESH STREAMING LIVE TABLE books_silver;
        
        APPLY CHANGES INTO LIVE.books_silver
        FROM STREAM(LIVE.books_bronze)
        KEYS (book_id)
        APPLY AS DELETE WHEN row_status = "DELETE"
        SEQUENCE BY row_time
        COLUMNS * EXCEPT (
            row_status,
            row_time
        )
        ```
    :::

- [ ] We specify that records where the row status is **`DELETE`** should be deleted from the target table

    :::info
        ```sql {6}
        CREATE OR REFRESH STREAMING LIVE TABLE books_silver;
        
        APPLY CHANGES INTO LIVE.books_silver
        FROM STREAM(LIVE.books_bronze)
        KEYS (book_id)
        APPLY AS DELETE WHEN row_status = "DELETE"
        SEQUENCE BY row_time
        COLUMNS * EXCEPT (
            row_status,
            row_time
        )
        ```
    :::

- [ ] We specify the **`row_time`** field for ordering the operations

    :::info
        ```sql {7}
        CREATE OR REFRESH STREAMING LIVE TABLE books_silver;
        
        APPLY CHANGES INTO LIVE.books_silver
        FROM STREAM(LIVE.books_bronze)
        KEYS (book_id)
        APPLY AS DELETE WHEN row_status = "DELETE"
        SEQUENCE BY row_time
        COLUMNS * EXCEPT (
            row_status,
            row_time
        )
        ```
    :::

- [ ] We indicate that all books fields should be added to the target table except the operational columns: **`row_status`** and **`row_time`**

    :::info
        ```sql {8-11}
        CREATE OR REFRESH STREAMING LIVE TABLE books_silver;
        
        APPLY CHANGES INTO LIVE.books_silver
        FROM STREAM(LIVE.books_bronze)
        KEYS (book_id)
        APPLY AS DELETE WHEN row_status = "DELETE"
        SEQUENCE BY row_time
        COLUMNS * EXCEPT (
            row_status,
            row_time
        )
        ```
    :::

## Creating the gold table layer

- [ ] In the gold layer, we define a simple aggregate query to create a live table from the data in our **`books_silver`** table

    :::note
        Notice here that this is not a streaming table
    :::

- [ ] Since data is being updated and deleted from our **`books_silver`** table, it is no more valid to be a streaming source for this new table

    :::info
        Remember, streaming sources must be append-only tables

        ```sql
        CREATE LIVE TABLE author_counts_state
        COMMENT "Number of books per author"
        AS SELECT 
            author,
            COUNT(*) AS books_count,
            CURRENT_TIMESTAMP() AS updated_time
        FROM LIVE.books_silver
        GROUP BY author
        ```
    :::

- [ ] In the CDC pipeline, we can also define views <br/>

- [ ] To define a view simply replace table with the **`VIEW`** keyword

    :::info
        These views are temporary views scoped to the pipeline they are a part of, so they are not persisted to the metastore
    :::

- [ ] Views can still be used to enforce data quality and metrics for views will be collected and reported as they would be for tables

## How to join and reference tables across notebooks

- [ ] Here we see how we can join and reference tables across notebooks <br/>

- [ ] We are joining our **`books_silver`** table to the **`orders_cleaned`** table, which we created in another notebook in the last lecture <br/>

- [ ] Since the API supports scheduling multiple notebooks as part of a single pipeline, configuration code in any notebook can reference tables and views created in any other notebook

    :::info
        You can think of the scope of the schema referenced by the **`LIVE`** keyword to be at the DLT pipeline level, rather than the individual notebook

        ```sql
        CREATE LIVE VIEW books_sales
        AS SELECT
            b.title,
            o.quantity
        FROM (
            SELECT
                *,
                EXPLODE(books) AS books
            FROM LIVE.orders_cleaned
        ) AS o
        INNER JOIN LIVE.books_silver AS b
        ON o.book.book_id = b.book_id;
        ```
    :::

- [ ] Let us now go to the pipeline we created in the last lecture to add this new notebook <br/>

- [ ] Here is our demo bookstore pipeline we created in the last lecture <br/>

- [ ] Click on the **`Settings`** button <br/>

- [ ] From here, click **`Add Notebook library`** <br/>

- [ ] Click **`Browse`** <br/>

- [ ] Go to **`Repo`** and choose the Books Pipeline notebook <br/>

- [ ] Click **`Save`** <br/>

- [ ] As you can see in the pipeline details, this pipeline is now referencing 2 notebooks instead of one <br/>

- [ ] Let us now click **`Start`** to run our updated pipeline

    :::info
        If you have any issue running this pipeline, try to do a full refresh in order to run your pipeline from scratch
    :::

- [ ] It means the attempts to clear all data from each table and then load all data from the streaming sources <br/>

- [ ] For now, let us click **`Start`**<br/>

- [ ] In addition to tables you created in the last lecture, we see our new **`books`** tables. And the view **`books_sales`** that join the two pipelines tables together