# 5.4 Jobs (Hands On)

## How to orchestrate jobs with Databricks 

- [ ] Databricks allows you to schedule one or multiple tasks as part of a job <br/>

- [ ] We are going to create a multi-task job consisting of 3 tasks<br/>

1. Executing a notebook that lands a new batch of data in our source directory<br/>

2. Running our Delta Live Tables pipeline created last session to process this data through a series of tables <br/>

3. Executing the notebook we created in the last session to show the pipeline results<br/>

- [ ] To create such a multi-task job, navigate to the workflow tabs on the sidebar <br/>

- [ ] In the **`Jobs`** tab, click the **`Create Job`** button <br/>

- [ ] Set a name for our job

    :::info
        For example, Bookstore Demo Job
    :::

- [ ] Then, we can start configuring our first task in this job <br/>

- [ ] Fill in a task name say **`land_new_data`** <br/>

- [ ] For type, select notebook

    :::info
        The notebook could be located in your Databricks workspace or in a repository
    :::

    :::info
        In our case, the notebook is in the workspace
    :::

- [ ] For path, we are going to select the **`land_new_data`** notebook in our workspace <br/>

- [ ] Click here to open the notebook

    :::info
        The notebook has nothing but just a call to the **`load_new_data`** function
    :::

- [ ] From the **`Cluster`** dropdown, under existing **`All-Purpose Clusters`**, let us select our Demo cluster

    :::info
        For production jobs, we must use job clusters instead of cost saving
    :::

- [ ] Click on **`Create`** button <br/>

- [ ] Now we have a job of a single task <br/>

- [ ] Let us add another task for our DLT pipeline to be executed after the success of this first task <br/>

- [ ] To do so, click this blue circle with the **`+`** sign to add a new task <br/>

- [ ] Enter DLT for the task name <br/>

- [ ] For type, select **`Delta Live Tables Pipeline`** <br/>

- [ ] For pipeline, select our demo pipeline we created during our last session <br/>

- [ ] The **`Depends on`** field defaults to your previously defined task, **`land_new_data`** in our case, so leave this value as it is <br/>

- [ ] Click now on the **`Create Task`** button <br/>

- [ ] Now, we see the two tasks and the dependencies between them <br/>

- [ ] Let us add a third task for executing a notebook to show the pipeline results<br/>

- [ ] Enter Pipeline Results for the task name<br/>

- [ ] For type, select **`Notebook`**<br/>

- [ ] For path, select the notebook pipeline result created in the last session

    :::info
        This notebook just shows the content of the pipeline storage location and query our gold table
    :::

- [ ] From the cluster dropdown, select the Demo cluster<br/>

- [ ] The **`Depends On`** field defaults again to your previously defined task, which is DLT task in our case<br/>

- [ ] Click now on the **`Create Task`** button<br/>

- [ ] We configured now our 3 tasks for this job<br/>

- [ ] On the right, we see the schedule section that allows us to schedule our job<br/>

- [ ] Click on the **`Edit Schedule`** button to explore the scheduling option<br/>

- [ ] We can change here the trigger type to: **`Scheduled`**. And configure the schedule for this job

    :::info
        You can edit the CRON syntax as well
    :::

    :::note
        For this demo, we will not set any schedule, so let us cancel this window
    :::

- [ ] From here, you can also set email notification. So, you can be alerted on the job's start, success and failure<br/>

- [ ] In the permissions section, you can control who can run, manage or review the jobs, either a user or a group of users

    :::info
        This also allows to change the owner of the job to another user but of course not to a group of users
    :::

- [ ] Let us click the Run Now button to start our job<br/>

- [ ] You can see your runs of your job on the job "Runs" tab<br/>

- [ ] The running jobs will be under the Active Runs section and the finished jobs will be under the Completed Runs section<br/>

- [ ] Let us click on the **`Start Time`** link to open the current run of this job<br/>

- [ ] The visualization for tasks will update in real time to reflect which tasks are actively running<br/>

- [ ] Let us click on the first task to show its results

    :::info
        Here we can see that we landed the parquet file number 7.
    :::

- [ ] If we come back to the run output and click on the DLT task<br/>

- [ ] DLT pipelines scheduled as tasks do not directly render the results in the runs UI<br/>

- [ ] Instead, you can click on this link to be directed back to the DLT pipeline UI to see the results<br/>

- [ ] Here's the result of running our DLT pipeline on the new data<br/>

- [ ] If we click on the **`Pipeline Results`** task, we can see the results of all the cells in our notebook<br/>

- [ ] Let us see another scenario where we have some bad codes in this notebook that cause our job to fail<br/>

- [ ] Let us query a table that does not exist<br/>

- [ ] Let us run our job again<br/>

- [ ] If we click on the Pipeline Results task, we see the Table Not Found error<br/>

- [ ] Let us correct this error and see how we can fix our job<br/>

- [ ] If we come back to our job, and click on the failed run. We can see that we have the Repair Run button<br/>

    :::info
        This is a great option that will allow us to rerun only the failed tasks
    :::

- [ ] Let us click this button<br/>

- [ ] Here, it shows the task to be rerun

    :::info
        In our case, it's only the pipeline results task
    :::

- [ ] Let us click now **`Repair Run`**<br/>