# 3.2 Query Files (Hands On)

- [ ] In this notebook, you will learn how to extract data directly from files using Spark SQL on Databricks<br/>

- [ ] For this demonstration, we will work on a bookstore dataset. Here we can see the schema for this bookstore dataset<br/>

- [ ] We have 3 tables, customers, books and orders<br/>

- [ ] Let us start by running this helping notebook that will download and copy the dataset to our Databricks file system<br/>

- [ ] The customer data is coming in a JSON format

## How to list the files in a directory

- [ ] Let us list the files in the **`customers`** directory

    :::info
        ```python {2}
        files = dbutils.fs.ls(
            f"{dataset_bookstore}/customers-json"
        )
        
        display(files)
        ```
    :::

- [ ] We can see that there are 6 JSON files in this directory

## How to query a JSON file

- [ ] To query a single JSON file, we use **`SELECT * FROM json`**, and we specify the full path for this JSON file

    :::info
        Notice here that we are using back ticks and not single quotes around the path

        ```sql
        SELECT * FROM  json.`${dataset.bookstore}/customers-json/export_001.json`
        ```
    :::

- [ ] We managed to read the table, and here we can see the different columns of our table, the customer's ID, the email, the profile information, which itself is JSON string and the last updated timestamp<br/>

- [ ] We can see also that our preview display shows all the 300 records of our source file<br/>

- [ ] We can also use a wildcard character (**`*`**) to query multiple files simultaneously

    :::info
        ```sql
        SELECT * FROM  json.`${dataset.bookstore}/customers-json/export_*.json`
        ```

        For example, here we are querying all the JSON files, starting with the name **`export_`**<br/>

        By default, the preview display shows only the first 1000 records
    :::

## How to query a complete directory

- [ ] We can query a complete directory of files, assuming all the files in the directory have the same format and schema<br/>

- [ ] Here we will specify simply the directory path rather than an individual file.
  
    :::info
        ```sql
        SELECT COUNT(*) FROM json.`{dataset.bookstore}/customers-json`
        ```
    :::

- [ ] When reading multiple files, it is useful to add the **`input_file_name()`** function, which is a built-in Spark SQL command that records the source data file for each record

    :::info
        ```sql {2}
        SELECT 
            input_file_name() as source_file
        FROM json.`{dataset.bookstore}/customers-json`;
        ```
    :::

    :::info
        This can be especially helpful if troubleshooting problems in the source data become necessary
    :::

- [ ] In addition to our columns, we have also the source file column<br/>

- [ ] Another interesting option here is to use the text format, which allows you to query and text-based files like **`JSON`**, **`CSV`**, **`TSV`**, or **`TEXT`** format

    :::info
        ```sql {3}
        SELECT 
            input_file_name() as source_file
        FROM text.`{dataset.bookstore}/customers-json`
        ```

        This load each line of the file as a row with one string column, **`Value`**
    :::

- [ ] This can be useful when data could be corrupted. And we need to use, in such cases, some custom text parsing function to extract data<br/>

- [ ] We can use **`binaryFile`** to extract the raw bytes and some metadata of files

    :::info
        ```sql {3}
        SELECT 
            input_file_name() as source_file
        FROM binaryFile.`{dataset.bookstore}/customers-json`
        ```
    :::

- [ ] As we can see here, this gives us the path of the file, the modification time, the length and the content, which is the binary representation of the file

## How to query and parse a CSV file

- [ ] Let us now switch to reading books data which is coming in CSV format. In the same way we will use the SELECT statement, but this time with the CSV format

    :::info
        ```sql {3}
        SELECT 
            input_file_name() as source_file
        FROM csv.`{dataset.bookstore}/customers-json`
        ```
    :::

    :::warning
        We managed to read the data. However, it is not well parsed<br/>

        The **`header`** row is being extracted as a table row and all columns are being loaded in a single column<br/>
        
        It seems that this is because of the **`delimiter`** of the file, which is in our case, a semicolon instead of a comma
    :::

- [ ] Querying files in this way works well only with self-describing formats

- [ ] Formats that have well-defined schema like JSON and parquet. However, for other formats like CSV where there is no schema defined, this does not work and we need another way that allows us to provide additional configuration and schema declaration<br/>

## How to query files without a schema defined such as CSV

- [ ] One solution is to create a table with the **`USING`** keyword

    :::info
        This allows us to create a table against an external sources like CSV format
    :::

- [ ] Here we need to specify the table schema. I mean, the column names and types, the file format, which is in our case, CSV and whether if there is a header in the source files, and the delimiter used to separate fields, in our case, it's a semicolon<br/>

- [ ] We need to specify the location to the files directory

    :::info
        ```sql {9-10}
        CREATE TABLE books_csv (
            book_id STRING,
            title STRING,
            author STRING,
            category STRING,
            price DOUBLE
        ) USING CSV 
        OPTIONS (
            header="true",
            delimiter=";"
        ) LOCATION "${dataset.bookstore}/books-csv"
        ```
    :::

- [ ] Let's now query this table

    :::info
        ```sql
        SELECT * FROM books_csv
        ```
    :::

- [ ] We have managed to read the books that are in the CSV files

    :::info
        Remember, when working with CSV files as a data source, it is important to ensure that column orders does not change if additional data files will be added to the source directory<br/>

        Spark will always load data and apply column names and data types in the order specified during table creation
    :::

- [ ] Let us now run **`DESCRIBE EXTENDED`** to see some information on our table

    :::info
        ```sql
        DESCRIBE EXTENDED books_csv
        ```
    :::

- [ ] Here we can see that we have created an external table. And this table is not a Delta table

    :::info
        It's a table that's referring directly to the CSV files. It means that no data has moved during table creation
    :::

- [ ] All the metadata and options passed during table creation will be persisted to the metadata, ensuring that data in the location will always be read with these options<br/>

- [ ] Let us now see the impact of not having a Delta Table<br/>

- [ ] All the guarantees and features that we have them usually when work with Delta Tables we will no longer having them with external data sources like CSV

    :::info
        For example, Delta Lake Tables guarantee that you always query the most recent version of your source data, while tables registered against other data sources like CSV may represent older cached versions
    :::

- [ ] Let us add some new CSV file to our directory and see what will happen<br/>

- [ ] Let us check how many CSV files we have in the directory<br/>

- [ ] Here we will use a Spark Dataframe API that allows us to write data in a specific format like CSV

    :::info
        ```python
        spark.read.table(
            "books_csv"
        ).write.mode(
            "append"
        ).format(
            "csv"
        ).option(
            "header", "true"
        ).option(
            "delimiter", ";"
        ).save(f"{dataset_bookstore}/books-csv")
        ```
    :::

- [ ] We are going to read our books table we have just created<br/>

- [ ] We are going to rewrite the table data in new additional CSV files in the same directory<br/>

- [ ] Let us now see how many CSV files in the directory

    :::info
        ```python
        files = dbutils.fs.ls(
            f"{dataset_bookstore}/books-csv"
        )
        
        display(files)
        ```
    
        There are extra CSV files that have been written to the directory by Spark
    :::

- [ ] If we calculate the number of books in our table, we should see 24 rows instead of 12

    :::info
        ```sql
        SELECT COUNT(*) FROM books_csv
        ```
    :::

    :::warning
        Even with the new data has been successfully written to the table directory, we are still unable to see this new data<br/>

        This is because Spark automatically cached the underlying data in local storage to ensure that on subsequent queries, Spark will provide the optimal performance by just querying this local cache
    :::

- [ ] This external CSV file is not configured to tell Spark that it should refresh this data

## How to refresh the table

- [ ] We can manually refresh the cache of our data by running the REFRESH table command

    :::info
        ```sql
        REFRESH TABLE books_csv
        ```
    :::

- [ ] Refreshing a table will invalidate its cache, meaning that we will need to scan our original data source and pull all data back into memory

    :::info
        For a very large dataset, this may take a significant amount of time
    :::

- [ ] Let us now check again the number of books in our table. And indeed, refreshing the table now we see that we have 24 books

    :::info
        ```sql
        SELECT COUNT(*) FROM books_csv
        ```
    :::

- [ ] To create Delta tables where we load data from external sources, we use **`CREATE TABLE AS SELECT`** statement or **`CTAS`** statements

    :::info
        ```sql
        CREATE TABLE customers As
        SELECT * FROM json.`${dataset.bookstore}/customers-json`;
        
        DESCRIBE EXTENDED customers;
        ```
    :::

- [ ] Here, we create and populate customer data table using data retrieved from this input query<br/>

- [ ] These will extract the data from the JSON files and load them into the table, customers<br/>

- [ ] From the table metadata, we can see that we are indeed creating a Delta Table, and it is also a managed table<br/>

- [ ] We can see that schema has been inferred automatically from the query results<br/>

- [ ] **`CTAS`** statements automatically infer schema information from a query results and do not support manual schema declaration.

    :::info
        This means that **`CTAS`** statements are useful for external data ingestion from sources with well-defined schema such as parquet files and tables
    :::

    :::warning
        **`CTAS`** statements do not support specifying additional file options which presents significant limitation when trying to ingest data from CSV files
    :::

- [ ] We have successfully created a Delta table here, however, the data is not well-parsed<br/>

- [ ] To correct this, we need to first to use a reference to the files that allow us to specify options<br/>

- [ ] This is what we are doing here by creating this temporary view that allows us to specify file options

    :::info
        ```sql {9-12}
        CREATE TEMP VIEW books_tmp_vw (
            book_id STRING,
            title STRING,
            author STRING,
            category STRING,
            price DOUBLE
        ) USING CSV 
        OPTIONS (
            path = "${dataset.bookstore}/books-csv.export_*.csv",
            header="true",
            delimiter=";"
        );
        
        CREATE TABLE books AS 
        SELECT * FROM books_tmp_vw;
        
        SELECT * FROM books;
        ```
    :::

- [ ] We will use this temporary view as the source for our **`CTAS`** statement to successfully register the Delta table

    :::info
        ```sql {14-15}
        CREATE TEMP VIEW books_tmp_vw (
            book_id STRING,
            title STRING,
            author STRING,
            category STRING,
            price DOUBLE
        ) USING CSV 
        OPTIONS (
            path = "${dataset.bookstore}/books-csv.export_*.csv",
            header="true",
            delimiter=";"
        );
        
        CREATE TABLE books AS 
        SELECT * FROM books_tmp_vw;
        
        SELECT * FROM books;
        ```

        Notice here we are retrieving only 12 records because we use the wildcard in the path location
    :::

- [ ] Let us finally check the metadata of our Delta Table<br/>

- [ ] It's a Delta table where we have extracted all the data from the CSV files and loaded them into this location

    :::info
        ```sql
        DESCRIBE EXTENDED books;
        ```
    :::