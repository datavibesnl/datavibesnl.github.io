# 3.1 Querying Files

- [ ] In this video, we will talk about querying files in Databricks<br/>

## How to extract data directly from files

- [ ] You will learn how to extract data directly from files using Spark SQL and different methods to review raw files contents<br/>

- [ ] We will see how to use the Spark SQL to configure options for extracting data from external sources and how to use **`CTAS`** statements to create Delta Lake tables<br/>

- [ ] To query file content, we can simply use a SELECT statement. **`SELECT * FROM`** a file format, and we specify the file path

    :::info
        ```sql
        SELECT * FROM file_format.`/path/to/file`
        ```
    
        Make a special note of the use of back ticks (**`**) and not single quotes around the path<br/>
        This work well with self-describing formats that have well-defined schema like JSON and parquet
    :::

- [ ] It is not very useful with non-describing formats like CSV<br/>

- [ ] A path file could be a single file. Or we can use wildcard (**`*`**) character to read multiple simultaneously. Or simply reading the whole directory

    :::info
        Assuming that all of the files in the directory have the same format and schema
    :::

- [ ] Here is an example to query a JSON file

    :::info
        ```sql
        SELECT * FROM json.`/path/file_name.json`
        ```
    :::

- [ ] When working with text-based files which include **`JSON`**, **`CSV`**, **`TSV`** and **`TXT`** format, You can use the "text" format to extract data as raw strings

    :::info
        ```sql
        SELECT * FROM text.`/path/to/file`
        ```
    :::

- [ ] This can be useful when input data could be corrupted. In this case, we extract the data as raw string and we apply custom text parsing functions to extract values from text files<br/>

- [ ] In some cases, we need the binary representation of files content, for example, when dealing with images and unstructured data<br/>

- [ ] Here we can use simply **`binaryFile`** as a format

    :::info
        ```sql
        SELECT * FROM binaryFile.`/path/to/file`
        ```
    :::

- [ ] After extracting data from external data sources, we need to load them into the Lakehouse which ensures that all of the benefits of Databricks platform can be fully leveraged<br/>

- [ ] To load data from files into Delta tables. We use **`CTAS`** statements, which is **`CREATE TABLE AS SELECT`** query

    :::info
        ```sql {2}
        CREATE TABLE table_name
        AS SELECT * FROM file_format.`/path/to/file`
        ```
    :::

- [ ] Here we are querying data from files directly. <br/>

- [ ] CTAS statements inferior schema information from query results and do not support manual schema declaration

    :::info
        This means the CTAS statements are useful for external data injection from sources with well-defined schema such as parquet files and tables
    :::

    :::warning
        CTAS statements also do not support specifying additional file options. And this is why this statement presents significant limitation when trying to ingest data from CSV files
    :::

## How to use additional file options with the USING option

- [ ] We need another solution that supports options. This solution is the regular **`CREATE TABLE`** statement, but with the **`USING`** keyword<br/>

- [ ] By adding the **`USING`** keyword, we specify the external data source type, for example CSV format and with any additional options

    :::info
        ```sql {3}
        CREATE TABLE table_name (
            column_name1 col_type1
        ) USING data_source
        OPTIONS (
            key1=val1, 
            key2=val2
        )
        LOCATION = path
        ```
    :::

- [ ] You need to specify a location to where these files are stored

    :::info
        ```sql {8}
        CREATE TABLE table_name (
            column_name1 col_type1
        ) USING data_source
        OPTIONS (
            key1=val1, 
            key2=val2
        )
        LOCATION = path
        ```
    :::

- [ ] With this command, we are always creating an external table<br/>

- [ ] The table here is just a reference to the files. Unlike with CTAS statements, here there is no data moving during table creation<br/>

- [ ] We are just pointing to files stored in an external location

    :::info
        These files are kept in its original format, which means we are creating here a non-Delta table
    :::

## How to create a table from a CSV file

- [ ] Here is an example of creating a table using CSV external source<br/>

- [ ] We are pointing to CSV files exist in an external location<br/>

- [ ] We are specifying the options for reading the files

    :::info
        Like the fact that there is a **`header`** present in the files and the **`delimiter`** is a semicolon (**`;`**)


        ```sql {5-6}
        CREATE TABLE table_name (
            col_name1 col_type1
        ) USING CSV
        OPTIONS (
            header="true",
            delimiter=";"
        ) LOCATION = path
        ```
    :::

- [ ] We are providing the location to these CSV files

    :::info
        ```sql {7}
        CREATE TABLE table_name (
            col_name1 col_type1
        ) USING CSV
        OPTIONS (
            header="true",
            delimiter=";"
        ) LOCATION = path
        ```
    :::

## How to create a table using JDBC connection

- [ ] Another example is to create a table using JDBC connection to refer to data in an external SQL database<br/>

- [ ] We provide the necessary **`OPTIONS`** like the connection string (**`url`**), the **`username`**, and the **`password`** for this database, and of course, the database table (**`dbtable`**) containing the data

    :::info
        ```sql {4-9}
        CREATE TABLE table_name (
            col_name1 col_type1
        ) USING JDBC
        OPTIONS (
            url="jdbc:sqlite://hostname:port",
            dbtable="database.table",
            user="username",
            password="pwd"
        )
        ```
    :::

## Limitation of tables from external data sources

- [ ] A table with external data source has a limitation<br/>

- [ ] It is not a Delta table. It means the performance and the features of Delta Lake are no more guaranteed, like time travel feature, and the guarantee that we are always reading the most recent version of the data<br/>

- [ ] If you are referring to a huge database table, this also can cause performance issues

    :::info
        Fortunately, we have a solution for this limitation. The solution is simply to create a temporary view, referring to the external data source, and then query this temporary view to create a table using CTAS statements<br/>

        In this way, we are extracting the data from the external data source and load it in a Delta table<br/>

        As you can see, with **`CTAS`** statement, you can not only query files, but you can query any object like a temporary view in this case

        ```sql
        CREATE TEMP VIEW temp_view_name (
            col_name1 col_type1
        ) USING data_source
        OPTIONS (
            key1=val1, 
            key2=val2
        );
        
        CREATE TABLE table_name
        AS SELECT * FROM temp_view_name
        ```
    :::