# 2.4 Apply Advanced Delta Features (Hands

- [ ] In this notebook, we will see some advanced concepts in Delta Lake, such as the time travel feature and we will see **`OPTIMIZE`** and **`VACUUM`** command

## How to read the history of a table

- [ ] Let us review again our table history

    :::info
        ```sql
        DESCRIBE HISTORY employees
        ```
    :::

- [ ] Here we can see the 3 versions of our table

## How to use the Time Travel feature with **VERSION AS OF**

- [ ] In Delta Lake, we can easily query previous versions of our table, and this feature of time travel is possible thanks to those extra data files that had been marked as removed in our transaction log<br/>

- [ ] Let's say we want to access our data before the update operation, which is version number 1<br/>

- [ ] We can simply use **`SELECT`** query with **`VERSION AS OF`** keyword, and we specify the version number, in our case it is version 1

    :::info
        ```sql {2}
        SELECT * FROM employees
        VERSION AS OF 1
        ```
    :::

- [ ] Here we can see our data before the update operation

    :::info
        Another alternative syntax is to use **`@v`** followed by the version number
    :::

## How to restore data with the **RESTORE** command

- [ ] Imagine this scenario, we deleted our data and we need to restore them

    :::info
        ```sql
        DELETE FROM employees
        ```
    :::

    :::note
        Here, -1 means that all data has been removed
    :::

- [ ] Let us confirm this. So, our table data has been indeed removed <br/>

- [ ] We can simply roll back to a previous version before deletion using **`RESTORE TABLE`** command

    :::info
        ```sql
        RESTORE TABLE employees TO VERSION AS OF 2
        ```
    :::

- [ ] Let us explore what really happened in our table

    :::info
        ```sql
        DESCRIBE HISTORY employees
        ```
    :::

- [ ] As you can see, the **`RESTORE`** command has been recorded as a transaction

## How to compact small files with **OPTIMIZE** and z-order indexing with **ZORDER BY**

- [ ] Let us now talk about the **`OPTIMIZE`** command and how to compact small files and do Z-order indexing <br/>

- [ ] Since Spark work in parallel, you usually end up by writing too many small files. Having many small data files negatively affect the performance of the Delta Table

    :::note
        In our case, we have 4 small data files
    :::

- [ ] To resolve this issue, we can use **`OPTIMIZE`** command that combines files toward an optimal size <br/>

- [ ] **`OPTIMIZE`** will replace existing data files by combining records and rewriting the results

    :::info
        ```sql
        OPTIMIZE employees ZORDER BY id
        ```
    :::

- [ ] Here, we can see that our 4 data files have been soft deleted, and a new file has been added that compacts those 4 files. <br/>

- [ ] In addition, you may notice that we added the **`ZORDER`** indexing with our **`OPTIMIZE`** command. Z-order indexing speeds up data retrieval when filtering on provided fields, by grouping data with similar values within the same data files

    :::info
        In our case, we do z-order by the **`id`** column. However, on such a small data set, it does not provide any benefit
    :::

- [ ] Let us confirm the output of the **`OPTIMIZE`** command by running **`DESCRIBE DETAIL`** on our table

    :::info
        ```sql
        DESCRIBE DETAIL employees
        ```
    :::

- [ ] Here, we can see that the number of files in the current version is only one. Let us see how to **`OPTIMIZE`** operation has been recorded in our table history

    :::info
        ```sql
        DESCRIBE HISTORY employees
        ```
    :::

- [ ] As expected, **`OPTIMIZE`** command created another version in our table, meaning that version 5 is the most recent version of our table <br/>

- [ ] Let us now explore the data files in our table directory

    :::info
        ```sh
        %fs ls 'dbfs:/user/hive/warehouse/employees'
        ```
    :::

- [ ] Here, we can see that there are 7 data files. But, we know, that our current table version referencing only one file after the **`OPTIMIZE`** operation. It means that other data files are unused files, and we can simply clean them up <br/>

- [ ] We can manually remove old data files using the **`VACUUM`** command. Let run this command and see what happens 

    :::info
        ```sql
        VACUUM employees
        ```
    :::

- [ ] If we check the table directory again, we see that nothing happened. The data files are still there. This is because we need to specify a retention period, and by default this retention period is 7 days

    :::info
        That means that **`VACUMM`** operation will prevent us from deleting files than 7 days old, just to ensure that no long running operations are still referencing any of the files to be deleted
    :::

- [ ] If we try to execute **`VACUUM`** command with a retention of 0 hour for keeping only the current version, this will not work because, the default threshold is 7 days

    :::info
        ```sql
        VACUUM employees RETAIN 0 HOURS
        ```
    :::

- [ ] In this demo, we will do a work around for demonstration purposes only

    :::note
        You should not do this in production
    :::

- [ ] The idea is to turn off the retention duration check

    :::info
        ```sh
        SET spark.databricks.delta.retentionDurationCheck.enabled = false;
        ```
    :::

- [ ] We can run our **`VACUUM`** command<br/>

    :::info
        ```sql
        VACUUM employees RETAIN 0 HOURS
        ```
    :::

- [ ] Let us explore again the table directory

    :::info
        ```sh
        %fs ls "dbfs:/user/hive/warehouse/employees"
        ```
    :::

- [ ] Files have been successfully deleted. 6 data files have been removed

    :::info
        Those files were really useful for Delta Lake time travel feature
    :::

- [ ] We are no longer able to access old data versions<br/>

- [ ] We can easily confirm this by querying an old table version<br/>

- [ ] We got here a file not found exception, because the data files for this version no longer exist<br/>

- [ ] Finally, let us permanently delete the table with its data from the Lakehouse<br/>

- [ ] For this, we use the **`DROP TABLE`** command, like in SQL. The table has been successfully deleted<br/>