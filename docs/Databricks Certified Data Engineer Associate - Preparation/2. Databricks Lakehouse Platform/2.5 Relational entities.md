# 2.5 Relational entities

- [ ] In this video, we will talk about relational entities on Databricks<br/>

- [ ] You will understand how databases and tables work in Databricks<br/>

- [ ] You will understand also how the **`LOCATION`** keyword impact the default storage directory

## Databases

- [ ] In Databricks, a database is actually a schema in Hive metastore<br/>

- [ ] In order to create a database, you could use either **`CREATE DATABASE`** syntax or instead use **`CREATE SCHEMA`** keyword, which is exactly the same

### What is really a Hive metastore in Databricks?

- [ ] A Hive metastore is a repository of metadata that stores information for data structure, such as databases, tables and partitions<br/>

- [ ] It holds metadata about your table and data, such as:<br/>
    - The table definition<br/>
    - The format of the data<br/> 
    - Where the data is actually stored in the underlying storage<br/>

- [ ] Every Databricks workspace has a central Hive metastore accessible by all clusters to persist table metadata

    :::info
        By default, you have a database called "default"
    :::

- [ ] To create some tables in this default database, we use the **`CREATE TABLE`** statement without specifying any database name

    :::info
        The table definition would be under the default database in the Hive metastore
    :::

- [ ] The table data will be located under the hive default directory, which is **`/user/hive/warehouse`**<br/>

- [ ] In addition to the default database, we can create other databases<br/>

- [ ] To do so we use the **`CREATE DATABASE`** or **`CREATE SCHEMA`** syntax<br/>

- [ ] The database will be created in the Hive metastore and the database folder will be under the hive default directory

    :::note
        Notice that the database folder has an extension **`.db`** to distinguish it from the tables directories
    :::

- [ ] We can use this database to create some tables. The table's definition will be in the Hive metastore and the data files would be under the database folder in the Hive default directory

### How to create databases outside of the default hive directory

- [ ] It is possible to create databases outside of the default hive directory<br/>

- [ ] We use the **`CREATE SCHEMA`** syntax with the **`LOCATION`** keyword to specify the path in which the database will be stored

    :::info
        ```sql {2}
        CREATE SCHEMA db_y
        LOCATION 'dbfs:/custom/path/db_y.db'
        ```
    :::

    :::warning
        The database definition would be as usual in the hive metastore. While the database folder would be in the specified custom location outside of the hive default directory
    :::

- [ ] As usual, we can use this database to create some tables. While the table definition will be in the hive metastore, the actual data files for these tables will be stored in the database folder in that custom location

## Tables

- [ ] In Databricks, there are two types of tables: **Managed tables** and **External tables**

### Managed Tables

- [ ] A **managed table** is when the table is created in the storage under the database directory, which is the default case

    :::info
        For a managed table, Hive owns both the metadata and table data, which means that it manages the lifecycle of the table. When you drop a managed table, the underlying data files will be deleted
    :::

### External Tables

- [ ] An external table is when the table is created in the storage outside the database directory in a path specified by the **`LOCATION`** keyword

    :::info
        Hive owns only the table metadata, but not the underlying data files. So, when you drop an external table, the underlying data files will not be deleted
    :::

- [ ] We can simply create an external table in the default database simply by using the **`CREATE TABLE`** statement with the **`LOCATION`** keyword<br/>

- [ ] The definition for this external table will be in the hive metastore under the default database. While the data files will be stored in the specified external location<br/>

- [ ] In the same way, we can create an external table in any database<br/>

- [ ] We specify the database name with the keyword **`USE`**, and we create the table with the **`LOCATION`** keyword, followed by the path to where this external table needs to be stored

    :::info
        ```sql {1,3}
        USE db_x;
        CREATE TABLE table3
        LOCATION 'dbfs:/some/path_2/x_tabl3';
        ```
    :::

- [ ] We could choose the same path as the one for the default database or simply use another location like in this case

    :::info
        The table definition will be in the hive metastore while the data files will be in the specified external location
    :::

- [ ] Even if the database was created in a custom location outside of the hive default directory. We can normally create external table in this database. Again, we choose the database by the **`USE`** keyword and we create the external table with the **`LOCATION`** keyword