# 2.1 Delta Lake

## What is Delta Lake?

- [ ] Delta Lake is an **open-source storage framework** that bring reliability to data lakes <br/>

- [ ] Data lakes have many limitations, such as **data inconsistency** and **performance issues** <br/>

- [ ] Delta lake technology helps overcoming these challenges <br/>

- [ ] It's a storage framework or a storage layer, but it not a storage format or a storage medium <br/>

- [ ] It enables building lakehouse architecture

    :::info
        Lakehouse is a platform that unifies both data warehouse and advanced analytics
    :::

- [ ] Delta Lake is a component which is deployed on the cluster as part of the Databricks runtime <br/>

- [ ] If you are creating a Delta Lake table, it gets stored on the storage in one or more data files in **`parquet`** format <br/>

- [ ] Along with these files, Delta Lake stores a **transaction log** as well <br/>

## What is the Delta Lake transaction log?

- [ ] The Delta Lake transaction log (also known as Delta Log) is ordered records of every transaction performed on the table since its creation.<br/>

- [ ] It serves as a single source of truth

    :::info
        Every time you query the table, Spark checks this transaction log to retrieve the most recent version of the data
    :::

- [ ] Each committed transaction is recorded in a JSON file

    :::info
        It contains the **operation** that has been performed, whether, for example, it's an **`INSERT`** or **`UPDATE`** and the predicates such as **conditions** and **filters** used during this operation in addition to all the files that have been affected because of this operation
    :::

## Example of Delta Lake Transaction Logs

### Scenario 1: Inserting new files

- [ ] In this scenario we have a **writer** process and a **reader** process

#### Writer process

- [ ] Once the **writer** process starts, it stores the Delta Lake table in two data files in a **`parquet`** format <br/>

- [ ] As soon as the **writer** process finishes writing, it adds the transaction log as **`000.json`** into the **`_delta_log`** directory

#### Reader process

- [ ] A **reader** process always starts by reading a transaction log

    :::info
        In this case, it reads the **`000.json`** transaction log that contains information of the **`File1.parquet`** and **`File2.parquet`** 
    :::

### Scenario 2: Updating existing files

#### Writer process
- [ ] In our second scenario, the **writer** process wants to update a record which presents in **`File1.parquet`**, but in Delta Lake, instead of updating the record in the file itself, it will make a copy of this file and make the necessary updates in the new file called **`File3.parquet`**<br/>

- [ ] It then updates the log by writing a new JSON file. This new log file knows that **`File1.parquet`** is no longer needed

#### Reader process
- [ ] The **reader** process reads the transaction log that tells that only **`File2.parquet`** and **`File3.parquet`** are part of the current table version so it can start reading them

### Scenario 3: Handling write and read simultaneously

- [ ] Let us see one more scenario. Here, both processes want to work at the same time <br/>

#### Writer and Reader process

- [ ] The **writer** process starts writing the **`File4.parquet`**. On the other hand, the **reader** process reads the transaction log that only has information about **`File2.parquet`** and **`File3.parquet`** and not **`File4.parquet`** as it is not fully written yet <br/>

- [ ] It starts reading those two files, **`File2.parquet`** and **`File3.parquet`** which represent the most recent data at the moment

    :::info
        *Delta Lake guarantees that you will always get the most recent version of the data*
    :::

#### Reader process

- [ ] Your **read** operation will never have a deadlock state or conflicts with any ongoing operation on the table <br/>

#### Writer process

- [ ] The **writer** process finishes and it adds a new file to the log

### Scenario 4: Handling errors when writing files

#### Writer process
- [ ] The **writer** process starts writing the **`File5.parquet`** to the lake, but this time there is an error in the job, which leads to adding an incomplete file <br/>

- [ ] Because of this failure, Delta Lake module does not write any information to the log <br/>

#### Reader process
- [ ] The **reader** process reads the transaction log that has no information about that incomplete **`File5.parquet`**

    :::note
        The reader process will read only **`File2.parquet`**, **`File3.parquet`**, and **`File4.parquet`**
    :::

- [ ] As you can see Delta Lake guarantees that you will never read dirty data

## Delta Lake Advantages

- [ ] The transaction log allows Delta Lake to perform **ACID** transactions on data lakes <br/>

- [ ] It allows also to handle scalable metadata <br/>

- [ ] This log also provides the full audit trail of all the changes that have happened on the table <br/>

- [ ] The underlying file format for Delta is nothing but **`.parquet`** and **`.json`** format <br/>