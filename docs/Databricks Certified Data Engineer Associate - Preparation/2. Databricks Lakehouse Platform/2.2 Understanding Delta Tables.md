# 2.2 Understanding Delta Tables (Hands On)

## Introduction

- [ ] In this notebook, we will work with Delta Lake tables <br/>

## Create an empty delta lake table

- [ ] Let us first create an empty Delta Lake table <br/>

- [ ] Like in SQL, you just need a **`CREATE TABLE`** statement, a table name in our case, **`employees`**, and a table schema. Here the **`id`** of type **`INTEGER`**, **`name`** as **`STRING`**, **`salary`** as **`DOUBLE`**.

    :::info
        ```sql
        CREATE TABLE employees
        -- USING DELTA
        (
            id INT, 
            name STRING, 
            salary DOUBLE
        );
        ```
    :::

    :::info
        Delta Lake is the default format and you don't need to specify the keyword USING DELTA, so we can simply remove it
    :::

- [ ] The table has been created. Let's confirm this <br/>

## View the delta table inside the Data tab

- [ ] Go to the **`Data`** tab. Here, in the default database, we can see that the table **`employees`** has been created. <br/>

- [ ] Here we can see the schema of the table, our three columns:
    
    - **`id`**;
    - **`name`**;
    - **`salary`** and other metadata information

- [ ] Come back to our notebook <br/>

## Insert some records into the delta table

- [ ] We will insert some records all in a single transaction. Again, like in SQL, we will use **`INSERT INTO`** statements

    :::info
        ```sql
        INSERT INTO employees (
            id, name, salary
        ) VALUES (
            1, "Adam", 3500.0
        ),
        (
            2, "Sarah", 4020.5
        ),
        (
            3, "John", 2999.3
        ),
        (
            4, "Thomas", 4000.3
        ),
        (
            5, "Anna", 2500.0
        ),
        (
            6, "Kim", 6200.3
        )
        ```
    :::

- [ ] Here we can see that we have successfully inserted six records <br/>

- [ ] We can simply query the table using a standard **`SELECT`** statement

    :::info
        ```sql
        SELECT * FROM employees;
        ```
    :::

## How to view metadata information about the newly created table

- [ ] Let us now see some metadata information about our table <br/>

- [ ] Here we will use the **`DESCRIBE DETAIL`** command on our table

    :::info
        It is an important command that allows us to explore table metadata

        ```sql
        DESCRIBE DETAIL employees
        ```
    :::

- [ ] As you can see, there are many important information regarding our table

    :::info
        For example, we can see the location of the table. It is the location where the table files are really stored<br/>

        We have also the number of file filed, which indicates the number of data files in the current table version
    :::

## How to explore the files inside the table location

- [ ] Copy the table location and explore the files using the **`%fs`** magic command

    :::info
        The directory contains 4 data files in **`parquet`** format<br/>

        Why do we have 4 files for a single insert operation?<br/>

        This is because Spark work in parallel. If we check our cluster, we see that our cluster has 4 cores, so there were 4 executors working at the same time to insert our 6 new records
    :::

- [ ] Let us now come back to our notebook and see the scenario of **`UPDATE`** operations

    :::info
        In this scenario, we need to update the salary of all employees having a name that starts with the letter A by adding 100 to their salary
    :::

- [ ] Here we can see that there are two records affected by the **`UPDATE`** operation

    :::info
        ```sql
        UPDATE employees
        SET salary = salary + 100
        WHERE name LIKE "A%"
        ```
    :::

- [ ] Let us query the table again to see the updated data

    :::info
        ```sql
        SELECT * FROM employees
        ```
    :::

- [ ] Let us now see what happened in the table directory

    :::info
        ```sh
        %fs ls 'dbfs:/user/hive/warehouse/employees'
        ```
    :::

- [ ] We can see that there are two files have been added to the directory

    :::info
        As we said, rather than updating the records in the files themselves, we make a copy of them
    :::

- [ ] Delta use the transaction log to indicate which files are valid in the current version of the table<br/>

- [ ] Let us confirm this by running the **`DESCRIBE DETAIL`** command

    :::info
        ```sql
        DESCRIBE DETAIL employees
        ```
    :::

- [ ] As you can see, the number of files are **`4`** and not **`6`**, and they are the 4 files that represent the current version of the table<br/>

- [ ] It contains the new files updated after our update command<br/>

- [ ] If we query our delta table again. The query engine uses the transaction logs to resolve all the files that are valid in the current version and ignore all other data files<br/>

- [ ] Since the transaction log also stores all the changes to the data table, we can easily review the table history using the **`DESCRIBE HISTORY`** command

    :::info
        ```sql
        DESCRIBE HISTORY employees
        ```
    :::

- [ ] As you can see, there are 3 versions of the table:

    - Version 0: **`CREATE TABLE`** command;
    - Version 1: **`INSERT`** command;
    - Version 2: **`UPDATE`** command

- [ ] As you can see, thanks to the transaction log, we have the full history of all operations that have happened on the table. <br/>

- [ ] The transaction log is located under the **`_delta_log`** folder in the table directory

    :::info
        ```sh
        %fs ls 'dbfs:/user/hive/warehouse/employees/_delta_log'
        ```
    :::

- [ ] Let us explore this folder. Each transaction is a new JSON file being written to the Delta Lake transaction log<br/>

- [ ] Here you can see that there are 3 JSON files representing the 3 transactions we have made on the table. Starting from 0, the other file in the directory are just the checksum of the JSON files<br/>

- [ ] Let us look inside the last file representing the update transaction.

    :::info
        ```sh
        %fs head 'dbfs:/user/hive/warehouse/employees/_delta_log/0000000000000000000000002.json
        ```

        For example, with the **`add`** tag, you can see the new files that have been written to our table. And with the **`remove`** tag, you can see the list of files that have been soft deleted from our table. It means the file no longer should be included in the table
    :::