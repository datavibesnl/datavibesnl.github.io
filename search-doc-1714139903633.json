{"searchDocs":[{"title":"7.1 Certification Overview","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Certification Overview/7.1 Certification Overview","content":"","keywords":"","version":"Next"},{"title":"Exam Questions​","type":1,"pageTitle":"7.1 Certification Overview","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Certification Overview/7.1 Certification Overview#exam-questions","content":"  The question expected during the exam will be distributed in the following way 11 questions out of 45 on the use and the benefits of using the Databricks Lakehouse platform 13 questions on building ETL pipelines with Apache Spark SQL and Python 10 questions on processing data incrementally 7 questions on building production pipelines in addition to Databricks SQL queries and Dashboards 4 questions on data governance and security practices  ","version":"Next","tagName":"h2"},{"title":"Out-of-scope​","type":1,"pageTitle":"7.1 Certification Overview","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Certification Overview/7.1 Certification Overview#out-of-scope","content":"  The following are not expected on the associate level data engineer exam All topics related to Spark interns, Databricks CLI and API Change Data Capture Data Modeling Concepts Data Protection Regulations Monitoring and logging production jobs, dependency management and testing During the exam, Data manipulation code will be always provided in SQL when possible In all other cases, code will be in Python Databricks certification are taken through webassessor platform via this link info https://www.webassessor.com/databricks You just need to sign up in order to schedule and take your exam During this exam, you will be monitored via wecam by a webassessor proctor You will be asked to provide a valid photo-based identification The proctor will be monitoring you during the exam and he can provide you with technical support if needed There will be no test aids during the exam The certification exams are automatically graded and you will receive your pass or fail grade immediately The badge and certificate will be received within 24 hours of passing the exam You will receive them via credentials.databricks.com The exam questions are multiple choice questions note This means there is only one correct answer for each question The exam has two types of questions either conceptual or code-based questions Databricks offers a practice for the Associate-Level Data Engineer exam ","version":"Next","tagName":"h2"},{"title":"6.1 Data Objects Privileges","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.1 Data Objects Privileges","content":"","keywords":"","version":"Next"},{"title":"Data objects​","type":1,"pageTitle":"6.1 Data Objects Privileges","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.1 Data Objects Privileges#data-objects","content":"  Let us see what our other object types we have in Databricks Databricks allows you to configure permissions for the following object types  Object\tScopeCATALOG\tControls access to the entire data catalog SCHEMA\tControls access to a database TABLE\tControls access to a managed or external table VIEW\tControls access to SQL views FUNCTION\tControls access to a named function ANY FILE\tControls access to the underlying filesystem  ","version":"Next","tagName":"h2"},{"title":"Privileges​","type":1,"pageTitle":"6.1 Data Objects Privileges","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.1 Data Objects Privileges#privileges","content":"  The following privileges can be configured on the data objects  Privilege\tAbilitySELECT\tRead access to an object MODIFY\tAdd, delete and modify data to or from an object CREATE\tCreate an object READ_METADATA\tView an object and its metadata USAGE\tNo effect required to perform any action on a database object ALL PRIVILEGES\tGives all privileges  ","version":"Next","tagName":"h2"},{"title":"Granting Privileges by Role​","type":1,"pageTitle":"6.1 Data Objects Privileges","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.1 Data Objects Privileges#granting-privileges-by-role","content":" Role\tCan grant access privileges forDatabricks administrator\tAll objects in the catalog and the underlying filesystem Catalog owner\tAll objects in the catalog Database owner\tAll objects in the database Table owner\tOnly the table ...\t...  ","version":"Next","tagName":"h2"},{"title":"More operations​","type":1,"pageTitle":"6.1 Data Objects Privileges","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.1 Data Objects Privileges#more-operations","content":"  In addition to GRANT operation, you have also other useful operations to manage object privileges info You can deny and revoke privileges ","version":"Next","tagName":"h2"},{"title":"6.3 Unity Catalog","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.3 Unity Catalog","content":"","keywords":"","version":"Next"},{"title":"What is Unity Catalog?​","type":1,"pageTitle":"6.3 Unity Catalog","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.3 Unity Catalog#what-is-unity-catalog","content":"  Unity Catalog is a centralized governance solution across all your workspaces on any cloud It unifies governance for all data and AI assets in your Lakehouse including: Files;Tables;Machine learning models;Dashboards info These can be simply achieved using SQL language With Unity Catalog, you define your data access rules once across multiple workspaces and clouds  ","version":"Next","tagName":"h2"},{"title":"Architecture​","type":1,"pageTitle":"6.3 Unity Catalog","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.3 Unity Catalog#architecture","content":"  Before Unity Catalog, users and groups were defined per workspace Access control was managed via the Hive metastore within the workspace Unity Catalog sits out of the workspace and accessed via a user interface called the Account Console Users and the groups for Unity Catalog are managed through this account console and assign it to one or more workspaces Metastores are likewise separated out of the workspaces and managed through the account console where they can be assigned to the workspaces A Unity Catalog metastore can be assigned to more than one workspace, enabling multiple workspaces to share the same DBFS storage and the same access control lists  ","version":"Next","tagName":"h2"},{"title":"Unity Catalog 3-level namespace​","type":1,"pageTitle":"6.3 Unity Catalog","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.3 Unity Catalog#unity-catalog-3-level-namespace","content":"  We saw previously the traditional two-level namespaces used to address tables within the schemas Unity Catalog introduces a third level, which is catalogs Let us understand better the hierarchy of Unity Catalog The metastore is the top level logical container in Unity Catalog It represents metadata, that is, information about the objects being managed by the metadata, as well as the access control list that govern access to those objects In a metastore, you have catalog, which is the top level container for data objects in Unity Catalog, and forms the first part of the three level namespace we just saw info Don't confuse Unity Catalog metastore with the Hive metastore The Hive store is the default metastore linked to each Databricks workspace info While it may seem functionality similar to a Unity Catalog metastore, Unity Catalog metastores offers improved security and advanced features Unity Catalog metastore can have as many catalogs as desired  ","version":"Next","tagName":"h2"},{"title":"Unity Catalog Hierarchy​","type":1,"pageTitle":"6.3 Unity Catalog","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.3 Unity Catalog#unity-catalog-hierarchy","content":"  Catalogs contain schemas A schema, also known as database, is the second part of the three-level namespace Schemas usually contain data assets like tables, views and functions, forming the third part of the three-level namespace You need to catalog also support authentication to the underlying cloud storage through Storage Credentials Storage Credentials apply to an entire storage container External Locations represent the storage directories within a cloud storage container In addition, Unity Catalog adds Shares and Recipients which are related to Delta Sharing Shares are collections of tables shared with one or more recipient info Data sharing is out of scope for this course  ","version":"Next","tagName":"h2"},{"title":"Unity Catalog Identities​","type":1,"pageTitle":"6.3 Unity Catalog","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.3 Unity Catalog#unity-catalog-identities","content":"  In Unity Catalog, we have three types of identities   Users are individual physical users which are uniquely identified by their email addresses A user can have an admin role to perform several administrative tasks important to Unity Catalog, such as managing and assigning metastores to workspaces and managing other users A service principle is an individual identity for use with automated tools and applications Service principles are uniquely identified by Application ID info Like users, service principles can have an admin role which allow them to programmatically carry out administrative tasks We have Groups that collect users and service principles into a single entity Groups can be nested with other groups info For example, a parent group called Employees can contain two inner groups: HR and Finance groups  ","version":"Next","tagName":"h2"},{"title":"Identity Federation​","type":1,"pageTitle":"6.3 Unity Catalog","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.3 Unity Catalog#identity-federation","content":"  Databricks identities exist at two levels: at account-level and at workspace-level Unity Catalog supports a feature called Identity Federation, where identities are simply created once in the accounts console They can be assigned to one or more workspaces as needed Identity Federation eliminates the need to manually create and maintain copies of identities at workspace-levels  ","version":"Next","tagName":"h2"},{"title":"Privileges​","type":1,"pageTitle":"6.3 Unity Catalog","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.3 Unity Catalog#privileges","content":"  Unity Catalog has CREATE, USAGE, SELECT and MODIFY privileges In addition, we have also privileges related to the underlying storage which are READ FILES and WRITE FILES which replace the ANY FILE privilege we saw previously with Hive metastore We have EXECUTE privilege to allow executing user defined functions  ","version":"Next","tagName":"h2"},{"title":"Security Model​","type":1,"pageTitle":"6.3 Unity Catalog","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.3 Unity Catalog#security-model","content":"  Unity Catalog uses a different security model than Hive metastores for granting privileges There are different privileges types, and extra securable objects and principles We continue to use GRANT statement in order to give a privilege on a secure object to a principle info GRANT Privilege ON Securable_Object TO Principal   ","version":"Next","tagName":"h2"},{"title":"Accessing legacy Hive metastore​","type":1,"pageTitle":"6.3 Unity Catalog","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.3 Unity Catalog#accessing-legacy-hive-metastore","content":"  Unity Catalog is additive, this means that your legacy Hive metastore is still accessible once Unity Catalog is enabled Regardless of the Unity Catalog metastore assigned to the workspace, the catalog named hive_metastore always provide access to the Hive metastore local to that workspace  ","version":"Next","tagName":"h2"},{"title":"Features​","type":1,"pageTitle":"6.3 Unity Catalog","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.3 Unity Catalog#features","content":"  Unity Catalog also has a built-in data search and discovery Unity Catalog also provides automated lineage where you can identify the origin of your data and where it is used across all data types like tables, notebooks, workflows and dashboards Unity Catalog unifies existing legacy catalogs info There is no hard migration needed when enabling Unity Catalog In order to access the account console, you can log in as an account administrator via this link info https://accounts.cloud.databricks.com  ","version":"Next","tagName":"h2"},{"title":"2.2 Understanding Delta Tables (Hands On)","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.2 Understanding Delta Tables","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"2.2 Understanding Delta Tables (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.2 Understanding Delta Tables#introduction","content":"  In this notebook, we will work with Delta Lake tables   ","version":"Next","tagName":"h2"},{"title":"Create an empty delta lake table​","type":1,"pageTitle":"2.2 Understanding Delta Tables (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.2 Understanding Delta Tables#create-an-empty-delta-lake-table","content":"  Let us first create an empty Delta Lake table Like in SQL, you just need a CREATE TABLE statement, a table name in our case, employees, and a table schema. Here the id of type INTEGER, name as STRING, salary as DOUBLE. info CREATE TABLE employees -- USING DELTA ( id INT, name STRING, salary DOUBLE ); info Delta Lake is the default format and you don't need to specify the keyword USING DELTA, so we can simply remove it The table has been created. Let's confirm this   ","version":"Next","tagName":"h2"},{"title":"View the delta table inside the Data tab​","type":1,"pageTitle":"2.2 Understanding Delta Tables (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.2 Understanding Delta Tables#view-the-delta-table-inside-the-data-tab","content":"  Go to the Data tab. Here, in the default database, we can see that the table employees has been created. Here we can see the schema of the table, our three columns: id;name;salary and other metadata information Come back to our notebook   ","version":"Next","tagName":"h2"},{"title":"Insert some records into the delta table​","type":1,"pageTitle":"2.2 Understanding Delta Tables (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.2 Understanding Delta Tables#insert-some-records-into-the-delta-table","content":"  We will insert some records all in a single transaction. Again, like in SQL, we will use INSERT INTO statements info INSERT INTO employees ( id, name, salary ) VALUES ( 1, &quot;Adam&quot;, 3500.0 ), ( 2, &quot;Sarah&quot;, 4020.5 ), ( 3, &quot;John&quot;, 2999.3 ), ( 4, &quot;Thomas&quot;, 4000.3 ), ( 5, &quot;Anna&quot;, 2500.0 ), ( 6, &quot;Kim&quot;, 6200.3 ) Here we can see that we have successfully inserted six records We can simply query the table using a standard SELECT statement info SELECT * FROM employees;   ","version":"Next","tagName":"h2"},{"title":"How to view metadata information about the newly created table​","type":1,"pageTitle":"2.2 Understanding Delta Tables (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.2 Understanding Delta Tables#how-to-view-metadata-information-about-the-newly-created-table","content":"  Let us now see some metadata information about our table Here we will use the DESCRIBE DETAIL command on our table info It is an important command that allows us to explore table metadata DESCRIBE DETAIL employees As you can see, there are many important information regarding our table info For example, we can see the location of the table. It is the location where the table files are really stored We have also the number of file filed, which indicates the number of data files in the current table version  ","version":"Next","tagName":"h2"},{"title":"How to explore the files inside the table location​","type":1,"pageTitle":"2.2 Understanding Delta Tables (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.2 Understanding Delta Tables#how-to-explore-the-files-inside-the-table-location","content":"  Copy the table location and explore the files using the %fs magic command info The directory contains 4 data files in parquet format Why do we have 4 files for a single insert operation? This is because Spark work in parallel. If we check our cluster, we see that our cluster has 4 cores, so there were 4 executors working at the same time to insert our 6 new records Let us now come back to our notebook and see the scenario of UPDATE operations info In this scenario, we need to update the salary of all employees having a name that starts with the letter A by adding 100 to their salary Here we can see that there are two records affected by the UPDATE operation info UPDATE employees SET salary = salary + 100 WHERE name LIKE &quot;A%&quot; Let us query the table again to see the updated data info SELECT * FROM employees Let us now see what happened in the table directory info %fs ls 'dbfs:/user/hive/warehouse/employees' We can see that there are two files have been added to the directory info As we said, rather than updating the records in the files themselves, we make a copy of them Delta use the transaction log to indicate which files are valid in the current version of the table Let us confirm this by running the DESCRIBE DETAIL command info DESCRIBE DETAIL employees As you can see, the number of files are 4 and not 6, and they are the 4 files that represent the current version of the table It contains the new files updated after our update command If we query our delta table again. The query engine uses the transaction logs to resolve all the files that are valid in the current version and ignore all other data files Since the transaction log also stores all the changes to the data table, we can easily review the table history using the DESCRIBE HISTORY command info DESCRIBE HISTORY employees As you can see, there are 3 versions of the table: Version 0: CREATE TABLE command;Version 1: INSERT command;Version 2: UPDATE command As you can see, thanks to the transaction log, we have the full history of all operations that have happened on the table. The transaction log is located under the _delta_log folder in the table directory info %fs ls 'dbfs:/user/hive/warehouse/employees/_delta_log' Let us explore this folder. Each transaction is a new JSON file being written to the Delta Lake transaction log Here you can see that there are 3 JSON files representing the 3 transactions we have made on the table. Starting from 0, the other file in the directory are just the checksum of the JSON files Let us look inside the last file representing the update transaction. info %fs head 'dbfs:/user/hive/warehouse/employees/_delta_log/0000000000000000000000002.json For example, with the add tag, you can see the new files that have been written to our table. And with the remove tag, you can see the list of files that have been soft deleted from our table. It means the file no longer should be included in the table ","version":"Next","tagName":"h2"},{"title":"2.1 Delta Lake","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.1 Delta Lake","content":"","keywords":"","version":"Next"},{"title":"What is Delta Lake?​","type":1,"pageTitle":"2.1 Delta Lake","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.1 Delta Lake#what-is-delta-lake","content":"  Delta Lake is an open-source storage framework that bring reliability to data lakes Data lakes have many limitations, such as data inconsistency and performance issues Delta lake technology helps overcoming these challenges It's a storage framework or a storage layer, but it not a storage format or a storage medium It enables building lakehouse architecture info Lakehouse is a platform that unifies both data warehouse and advanced analytics Delta Lake is a component which is deployed on the cluster as part of the Databricks runtime If you are creating a Delta Lake table, it gets stored on the storage in one or more data files in parquet format Along with these files, Delta Lake stores a transaction log as well   ","version":"Next","tagName":"h2"},{"title":"What is the Delta Lake transaction log?​","type":1,"pageTitle":"2.1 Delta Lake","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.1 Delta Lake#what-is-the-delta-lake-transaction-log","content":"  The Delta Lake transaction log (also known as Delta Log) is ordered records of every transaction performed on the table since its creation. It serves as a single source of truth info Every time you query the table, Spark checks this transaction log to retrieve the most recent version of the data Each committed transaction is recorded in a JSON file info It contains the operation that has been performed, whether, for example, it's an INSERT or UPDATE and the predicates such as conditions and filters used during this operation in addition to all the files that have been affected because of this operation  ","version":"Next","tagName":"h2"},{"title":"Example of Delta Lake Transaction Logs​","type":1,"pageTitle":"2.1 Delta Lake","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.1 Delta Lake#example-of-delta-lake-transaction-logs","content":" ","version":"Next","tagName":"h2"},{"title":"Scenario 1: Inserting new files​","type":1,"pageTitle":"2.1 Delta Lake","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.1 Delta Lake#scenario-1-inserting-new-files","content":"  In this scenario we have a writer process and a reader process  Writer process​   Once the writer process starts, it stores the Delta Lake table in two data files in a parquet format As soon as the writer process finishes writing, it adds the transaction log as 000.json into the _delta_log directory  Reader process​   A reader process always starts by reading a transaction log info In this case, it reads the 000.json transaction log that contains information of the File1.parquet and File2.parquet  ","version":"Next","tagName":"h3"},{"title":"Scenario 2: Updating existing files​","type":1,"pageTitle":"2.1 Delta Lake","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.1 Delta Lake#scenario-2-updating-existing-files","content":" Writer process​   In our second scenario, the writer process wants to update a record which presents in File1.parquet, but in Delta Lake, instead of updating the record in the file itself, it will make a copy of this file and make the necessary updates in the new file called File3.parquet It then updates the log by writing a new JSON file. This new log file knows that File1.parquet is no longer needed  Reader process​   The reader process reads the transaction log that tells that only File2.parquet and File3.parquet are part of the current table version so it can start reading them  ","version":"Next","tagName":"h3"},{"title":"Scenario 3: Handling write and read simultaneously​","type":1,"pageTitle":"2.1 Delta Lake","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.1 Delta Lake#scenario-3-handling-write-and-read-simultaneously","content":"  Let us see one more scenario. Here, both processes want to work at the same time   Writer and Reader process​   The writer process starts writing the File4.parquet. On the other hand, the reader process reads the transaction log that only has information about File2.parquet and File3.parquet and not File4.parquet as it is not fully written yet It starts reading those two files, File2.parquet and File3.parquet which represent the most recent data at the moment info Delta Lake guarantees that you will always get the most recent version of the data  Reader process​   Your read operation will never have a deadlock state or conflicts with any ongoing operation on the table   Writer process​   The writer process finishes and it adds a new file to the log  ","version":"Next","tagName":"h3"},{"title":"Scenario 4: Handling errors when writing files​","type":1,"pageTitle":"2.1 Delta Lake","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.1 Delta Lake#scenario-4-handling-errors-when-writing-files","content":" Writer process​   The writer process starts writing the File5.parquet to the lake, but this time there is an error in the job, which leads to adding an incomplete file Because of this failure, Delta Lake module does not write any information to the log   Reader process​   The reader process reads the transaction log that has no information about that incomplete File5.parquet note The reader process will read only File2.parquet, File3.parquet, and File4.parquet As you can see Delta Lake guarantees that you will never read dirty data  ","version":"Next","tagName":"h3"},{"title":"Delta Lake Advantages​","type":1,"pageTitle":"2.1 Delta Lake","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.1 Delta Lake#delta-lake-advantages","content":"  The transaction log allows Delta Lake to perform ACID transactions on data lakes It allows also to handle scalable metadata This log also provides the full audit trail of all the changes that have happened on the table The underlying file format for Delta is nothing but .parquet and .json format  ","version":"Next","tagName":"h2"},{"title":"2.3 Advanced Delta Lake Features","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.3 Advanced Delta Lake Features","content":"","keywords":"","version":"Next"},{"title":"Time travel feature in Delta Lake​","type":1,"pageTitle":"2.3 Advanced Delta Lake Features","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.3 Advanced Delta Lake Features#time-travel-feature-in-delta-lake","content":"  With Delta, every operation on the table is automatically versioned, which provides the full audit trail of all the changes that have happened on the table You can look at the history of the table in SQL using DESCRIBE HISTORY command In addition, we can query older versions of the table. This can be done in two different ways  Use a timestamp, So, in a SELECT statement we use the keyword TIMESTAMP AS OF and we provide the time or date string info SELECT * FROM my_table TIMESTAMP AS OF &quot;2019-01-01&quot; Use a version number Since every operation on the table has a version number, you can use this version number to travel back in time as well. Here using the keyword VERSION AS OF, or simply @v, which is the short syntax info SELECT * FROM my_table VERSION AS OF 36 SELECT * FROM my_table @v36    Time travel also makes it easy to do rollbacks in case of bad writes info For example, if your pipeline job had a bug that accidentally deleted user information, you can easily fix this using the RESTORE TABLE command. Either to restore the table to a specific timestamp or to a specific version number RESTORE TABLE my_table TO TIMESTAMP AS OF &quot;2019-01-01&quot; RESTORE TABLE my_table TO VERSION AS OF 36   ","version":"Next","tagName":"h2"},{"title":"Compacting small files feature in Delta Lake​","type":1,"pageTitle":"2.3 Advanced Delta Lake Features","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.3 Advanced Delta Lake Features#compacting-small-files-feature-in-delta-lake","content":"  Delta Lake can improve the speed of read queries from a table One way to improve this speed is by compacting small files into larger ones You trigger compaction simply by running the OPTIMIZE command info OPTIMIZE my_table For example, if you have many files by running the OPTIMIZE command, they will be compacted in one or more larger files which improves the table performance  ","version":"Next","tagName":"h2"},{"title":"Z-order indexing in Delta Lake​","type":1,"pageTitle":"2.3 Advanced Delta Lake Features","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.3 Advanced Delta Lake Features#z-order-indexing-in-delta-lake","content":"  With OPTIMIZE, we can also do z-order indexing. Z-order indexing in Delta Lake is about co-locating and reorganizing column information in the same set of files This can be done by adding the ZORDER BY keyword to the OPTIMIZE command, followed by one or more column name info OPTIMIZE my_table ZORDER BY colum_name For example, if you have a numerical column in the data files, ID for example, by applying the ZORDER on this column, the first compacted file will contain values from 1 to 50, while the other one will contain values from 51 to 100 Z-Order indexing is used by data skipping algorithm to extremely reduce the amount of data that need to be read info In our example, if you query an ID, say 30, Delta is sure now that ID 30 is in file number 1, so it can easily skip the scanning the file number 2, which will sav a huge amount of time  ","version":"Next","tagName":"h2"},{"title":"Vacuum a Delta Table​","type":1,"pageTitle":"2.3 Advanced Delta Lake Features","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.3 Advanced Delta Lake Features#vacuum-a-delta-table","content":"  You might be wondering what about unused data files like uncommitted files and files that are no longer in the latest state of the transaction log for the table? Delta Lake allows you to do garbage collection by using VACUUM command With VACUUM command, you just need to specify the threshold of retention period for the files, so you delete all the files older than this threshold info VACUUM table_name [retention period] By default, the threshold is 7 days. This means that VACUUM operation will prevent you from deleting files less than 7 days old. Just to be sure that no longer running operations are still referencing any of the files to be deleted note Once you run a VACUUM on a Delta table, you lose the ability to time travel back to a version older than the specified retention period, simply because the data files are no longer exist ","version":"Next","tagName":"h2"},{"title":"6.2 Managing Permissions (Hands On)","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.2 Managing Permissions Hands On","content":"","keywords":"","version":"Next"},{"title":"How to configure permissions to a new database and table​","type":1,"pageTitle":"6.2 Managing Permissions (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.2 Managing Permissions Hands On#how-to-configure-permissions-to-a-new-database-and-table","content":"  In order to allow other users to access this new database and table, let us configure their permissions We will create a new query Let us start by granting several privileges on the whole hr database to a group of users called hr_team All members in this group will have the ability to read, and modify the data, access metadata information, and also the ability to create a new object like tables and views in this database info GRANT SELECT, MODIFY, READ_METADATA, CREATE ON SCHEMA hr_db TO hr_team; For users to perform any action on a database object, they must have an additional privilege, which is the USAGE privilege info GRANT USAGE ON SCHEMA hr_db TO hr_team; Without this privilege, the objects in the database cannot be used We can run a specific SQL command simply by selecting it and click Run Selected  ","version":"Next","tagName":"h2"},{"title":"How to grant permissions to individual users​","type":1,"pageTitle":"6.2 Managing Permissions (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.2 Managing Permissions Hands On#how-to-grant-permissions-to-individual-users","content":"  We can also assign privileges to individual users Here, for example, we are granting a read access on our view object to a user from outside of the HR team info GRANT SELECT ON VIEW hr_db.paris_employees_vw TO 'adam@mycompany.com' Let us select this query and click Run Selected Let us review the assigned permissions using the SHOW GRANTS command info SHOW GRANTS ON SCHEMA hr_db; The HR team has all the granted privileges. And me, I am the owner of this database as I was the one who created it We can also show the granted privileges on our view info SHOW GRANTS ON VIEW hr_db.paris_employees_vw; Here we can see the user Adam indeed has the SELECT privilege on this view The HR team inherited the database privileges  ","version":"Next","tagName":"h2"},{"title":"How to use Data Explorer to manage permissions​","type":1,"pageTitle":"6.2 Managing Permissions (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.2 Managing Permissions Hands On#how-to-use-data-explorer-to-manage-permissions","content":" ","version":"Next","tagName":"h2"},{"title":"How to use Data Explorer to view permissions​","type":1,"pageTitle":"6.2 Managing Permissions (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.2 Managing Permissions Hands On#how-to-use-data-explorer-to-view-permissions","content":"  In addition to SQL editor here, we can also use the Data Explorer to manage permissions From the left side navigator, select the Data tab to access the Data Explorer The Data Explorer allows users and admins to navigate different data objects like databases, tables and views, explore data schema, metadata and history in addition to setting and modify permissions From here we can find the database we created previously By clicking on the database name, it displays a list of the containing tables and views on the left hand side On the right, you will see some details about the database, like the owner information Use the Permissions tab to review who currently has permissions on this database info We see here the granted privileges for the HR team group You can select a privilege here and click on Revoke to remove this privilege info The privilege has been successfully revoked From here, we can also change the owner If you click here, you have the option to edit the owner info An owner can be set as an individual or a group Let us set the owner to admins, which is the default group containing all workspace's administrators As you can see, the admin group now is the owner of this database  ","version":"Next","tagName":"h3"},{"title":"How to use Data Explorer to grant permissions​","type":1,"pageTitle":"6.2 Managing Permissions (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.2 Managing Permissions Hands On#how-to-use-data-explorer-to-grant-permissions","content":"  From this window you can also grant permissions Let's say we would like to allow all user to review metadata about this database Click the Grant button Select All Users group Choose both READ_METADATA and USAGE privileges Click on Grant Similarly, we can manage permissions for tables and views Click on the table name Click on the Permissions tab From here, let us, for example, give all users the ability to query this table Click first on Grant button Select all users group Choose the SELECT privilege Click Grant The Data Explorer is really useful and powerful tool to manage your data objects info At present only the ANY FILE object cannot be set from the data explorer. You need to use the SQL editor instead What's interesting about Databricks is it is that you can see all the SQL queries run behind the Data Explorer Navigate to the **Query History **in the left sidebar As you can see, query history shows all the queries run in the Databricks SQL, including the Data Explorer  ","version":"Next","tagName":"h3"},{"title":"2.5 Relational entities","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.5 Relational entities","content":"","keywords":"","version":"Next"},{"title":"Databases​","type":1,"pageTitle":"2.5 Relational entities","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.5 Relational entities#databases","content":"  In Databricks, a database is actually a schema in Hive metastore In order to create a database, you could use either CREATE DATABASE syntax or instead use CREATE SCHEMA keyword, which is exactly the same  ","version":"Next","tagName":"h2"},{"title":"What is really a Hive metastore in Databricks?​","type":1,"pageTitle":"2.5 Relational entities","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.5 Relational entities#what-is-really-a-hive-metastore-in-databricks","content":"  A Hive metastore is a repository of metadata that stores information for data structure, such as databases, tables and partitions It holds metadata about your table and data, such as: The table definition The format of the data Where the data is actually stored in the underlying storage Every Databricks workspace has a central Hive metastore accessible by all clusters to persist table metadata info By default, you have a database called &quot;default&quot; To create some tables in this default database, we use the CREATE TABLE statement without specifying any database name info The table definition would be under the default database in the Hive metastore The table data will be located under the hive default directory, which is /user/hive/warehouse In addition to the default database, we can create other databases To do so we use the CREATE DATABASE or CREATE SCHEMA syntax The database will be created in the Hive metastore and the database folder will be under the hive default directory note Notice that the database folder has an extension .db to distinguish it from the tables directories We can use this database to create some tables. The table's definition will be in the Hive metastore and the data files would be under the database folder in the Hive default directory  ","version":"Next","tagName":"h3"},{"title":"How to create databases outside of the default hive directory​","type":1,"pageTitle":"2.5 Relational entities","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.5 Relational entities#how-to-create-databases-outside-of-the-default-hive-directory","content":"  It is possible to create databases outside of the default hive directory We use the CREATE SCHEMA syntax with the LOCATION keyword to specify the path in which the database will be stored info CREATE SCHEMA db_y LOCATION 'dbfs:/custom/path/db_y.db' warning The database definition would be as usual in the hive metastore. While the database folder would be in the specified custom location outside of the hive default directory As usual, we can use this database to create some tables. While the table definition will be in the hive metastore, the actual data files for these tables will be stored in the database folder in that custom location  ","version":"Next","tagName":"h3"},{"title":"Tables​","type":1,"pageTitle":"2.5 Relational entities","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.5 Relational entities#tables","content":"  In Databricks, there are two types of tables: Managed tables and External tables  ","version":"Next","tagName":"h2"},{"title":"Managed Tables​","type":1,"pageTitle":"2.5 Relational entities","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.5 Relational entities#managed-tables","content":"  A managed table is when the table is created in the storage under the database directory, which is the default case info For a managed table, Hive owns both the metadata and table data, which means that it manages the lifecycle of the table. When you drop a managed table, the underlying data files will be deleted  ","version":"Next","tagName":"h3"},{"title":"External Tables​","type":1,"pageTitle":"2.5 Relational entities","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.5 Relational entities#external-tables","content":"  An external table is when the table is created in the storage outside the database directory in a path specified by the LOCATION keyword info Hive owns only the table metadata, but not the underlying data files. So, when you drop an external table, the underlying data files will not be deleted We can simply create an external table in the default database simply by using the CREATE TABLE statement with the LOCATION keyword The definition for this external table will be in the hive metastore under the default database. While the data files will be stored in the specified external location In the same way, we can create an external table in any database We specify the database name with the keyword USE, and we create the table with the LOCATION keyword, followed by the path to where this external table needs to be stored info USE db_x; CREATE TABLE table3 LOCATION 'dbfs:/some/path_2/x_tabl3'; We could choose the same path as the one for the default database or simply use another location like in this case info The table definition will be in the hive metastore while the data files will be in the specified external location Even if the database was created in a custom location outside of the hive default directory. We can normally create external table in this database. Again, we choose the database by the USE keyword and we create the external table with the LOCATION keyword ","version":"Next","tagName":"h3"},{"title":"2.4 Apply Advanced Delta Features (Hands On)","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.4 Apply Advanced Delta Features","content":"","keywords":"","version":"Next"},{"title":"How to read the history of a table​","type":1,"pageTitle":"2.4 Apply Advanced Delta Features (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.4 Apply Advanced Delta Features#how-to-read-the-history-of-a-table","content":"  Let us review again our table history info DESCRIBE HISTORY employees Here we can see the 3 versions of our table  ","version":"Next","tagName":"h2"},{"title":"How to use the Time Travel feature with VERSION AS OF​","type":1,"pageTitle":"2.4 Apply Advanced Delta Features (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.4 Apply Advanced Delta Features#how-to-use-the-time-travel-feature-with-version-as-of","content":"  In Delta Lake, we can easily query previous versions of our table, and this feature of time travel is possible thanks to those extra data files that had been marked as removed in our transaction log Let's say we want to access our data before the update operation, which is version number 1 We can simply use SELECT query with VERSION AS OF keyword, and we specify the version number, in our case it is version 1 info SELECT * FROM employees VERSION AS OF 1 Here we can see our data before the update operation info Another alternative syntax is to use @v followed by the version number  ","version":"Next","tagName":"h2"},{"title":"How to restore data with the RESTORE command​","type":1,"pageTitle":"2.4 Apply Advanced Delta Features (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.4 Apply Advanced Delta Features#how-to-restore-data-with-the-restore-command","content":"  Imagine this scenario, we deleted our data and we need to restore them info DELETE FROM employees note Here, -1 means that all data has been removed Let us confirm this. So, our table data has been indeed removed We can simply roll back to a previous version before deletion using RESTORE TABLE command info RESTORE TABLE employees TO VERSION AS OF 2 Let us explore what really happened in our table info DESCRIBE HISTORY employees As you can see, the RESTORE command has been recorded as a transaction  ","version":"Next","tagName":"h2"},{"title":"How to compact small files with OPTIMIZE and z-order indexing with ZORDER BY​","type":1,"pageTitle":"2.4 Apply Advanced Delta Features (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.4 Apply Advanced Delta Features#how-to-compact-small-files-with-optimize-and-z-order-indexing-with-zorder-by","content":"  Let us now talk about the OPTIMIZE command and how to compact small files and do Z-order indexing Since Spark work in parallel, you usually end up by writing too many small files. Having many small data files negatively affect the performance of the Delta Table note In our case, we have 4 small data files To resolve this issue, we can use OPTIMIZE command that combines files toward an optimal size OPTIMIZE will replace existing data files by combining records and rewriting the results info OPTIMIZE employees ZORDER BY id Here, we can see that our 4 data files have been soft deleted, and a new file has been added that compacts those 4 files. In addition, you may notice that we added the ZORDER indexing with our OPTIMIZE command. Z-order indexing speeds up data retrieval when filtering on provided fields, by grouping data with similar values within the same data files info In our case, we do z-order by the id column. However, on such a small data set, it does not provide any benefit Let us confirm the output of the OPTIMIZE command by running DESCRIBE DETAIL on our table info DESCRIBE DETAIL employees Here, we can see that the number of files in the current version is only one. Let us see how to OPTIMIZE operation has been recorded in our table history info DESCRIBE HISTORY employees As expected, OPTIMIZE command created another version in our table, meaning that version 5 is the most recent version of our table Let us now explore the data files in our table directory info %fs ls 'dbfs:/user/hive/warehouse/employees' Here, we can see that there are 7 data files. But, we know, that our current table version referencing only one file after the OPTIMIZE operation. It means that other data files are unused files, and we can simply clean them up We can manually remove old data files using the VACUUM command. Let run this command and see what happens info VACUUM employees If we check the table directory again, we see that nothing happened. The data files are still there. This is because we need to specify a retention period, and by default this retention period is 7 days info That means that VACUMM operation will prevent us from deleting files than 7 days old, just to ensure that no long running operations are still referencing any of the files to be deleted If we try to execute VACUUM command with a retention of 0 hour for keeping only the current version, this will not work because, the default threshold is 7 days info VACUUM employees RETAIN 0 HOURS In this demo, we will do a work around for demonstration purposes only note You should not do this in production The idea is to turn off the retention duration check info SET spark.databricks.delta.retentionDurationCheck.enabled = false; We can run our VACUUM command info VACUUM employees RETAIN 0 HOURS Let us explore again the table directory info %fs ls &quot;dbfs:/user/hive/warehouse/employees&quot; Files have been successfully deleted. 6 data files have been removed info Those files were really useful for Delta Lake time travel feature We are no longer able to access old data versions We can easily confirm this by querying an old table version We got here a file not found exception, because the data files for this version no longer exist Finally, let us permanently delete the table with its data from the Lakehouse For this, we use the DROP TABLE command, like in SQL. The table has been successfully deleted  ","version":"Next","tagName":"h2"},{"title":"2.8 Views","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.8 Views","content":"","keywords":"","version":"Next"},{"title":"Types of Views​","type":1,"pageTitle":"2.8 Views","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.8 Views#types-of-views","content":"  There are three types of views in Databricks  Stored Views Like tables stored views are presented in the database To define a view, we use the CREATE VIEW statement with the AS keyword followed by your SQL query Once created, you can query your view with a standard SELECT statement, just as if it were a table info CREATE VIEW view_name AS query Temporary Views Temporary view is tied to a Spark session, so it dropped when the session ends To create a temporary view, we simply add TEMP or TEMPORARY keyword to CREATE VIEW command info CREATE TEMP VIEW view_name AS query When a SparkSession is created in Databricks There are several scenarios in which a new Spark session is created info For example, when opening a new notebook, a new session is created. Also, when detaching and reattaching a notebook to a cluster. And after installing a Python package, which lead to restarting the Python interpreter. Or simply after restarting the cluster. Global Temporary Views It behaves much like other views, but differs in one important way. It is tied to the cluster As long as the cluster is running, any notebook attached to the cluster can access its global temporary views To define a global temporary view, we simply add GLOBAL TEMP to the command Global temporary views are added to a cluster's temporary database called global_temp info CREATE GLOBAL TEMP VIEW view_nameAS query SELECT * FROM global_temp.view_name When you query this view in a SELECT statement, you need to use the global_temp database qualifier  ","version":"Next","tagName":"h2"},{"title":"Comparison between views​","type":1,"pageTitle":"2.8 Views","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.8 Views#comparison-between-views","content":" (Stored) Views\tTemp Views\tGlobal Temp ViewsPersisted in DB\tSession-scoped\tCluster-scoped Dropped only by DROP VIEW\tDropped when session ends\tDropped when cluster restarted CREATE VIEW\tCREATE TEMP VIEW\tCREATE GLOBAL TEMP VIEW ","version":"Next","tagName":"h2"},{"title":"2.7 Set Up Delta Tables","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.7 Set Up Delta Tables","content":"","keywords":"","version":"Next"},{"title":"Table Constraints​","type":1,"pageTitle":"2.7 Set Up Delta Tables","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.7 Set Up Delta Tables#table-constraints","content":"  Once you create your Delta table, either with regular CREATE TABLE or CTAS statements, you can add constraints to your table Databricks currently supports two types of table constraints, NOT NULL constraints and CHECK constraints In both cases, you must ensure that there is no data violating the constraint is already in the table prior to defining the constraint Once a constraint has been added to a table, new data violating the constraint would result in write failure In this example, we add a Check constraint to the date column of our table. info Note that check constraints look like standard WHERE clauses you might use to filter a dataset ALTER TABLE orders ADD CONSTRAINT valid_date CHECK (date &gt; '2020-01-01')   ","version":"Next","tagName":"h2"},{"title":"Cloning Delta Lake Tables​","type":1,"pageTitle":"2.7 Set Up Delta Tables","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.7 Set Up Delta Tables#cloning-delta-lake-tables","content":"  What if you want to backup or make a copy of your delta table? For this Delta Lake has two options for efficiently copying Delta Tables  DEEP CLONE info Deep clone fully copies both data and metadata from a source table to a target The command is pretty simple. CREATE TABLE and you provide the name of the new target table, followed by DEEP CLONE keyboard and you indicate the name of the source table CREATE TABLE table_clone DEEP CLONE source_table This copy can occur incrementally Executing this command again can synchronize changes from the source to the target location. And because all the data must be copied over, this can take a while for large data sets SHALLOW CLONE info With SHALLOW CLONE, you can quickly create a copy of a table since it just copies the Delta transaction logs That means there is no data moving during shallow cloning CREATE TABLE table_clone SHALLOW CLONE source_table SHALLOW CLONE is a good option, for example, to test out applying changes on a table without the risk of modifying the current table   In either cases, deep or shallow, data modification applied to the cloned version of the table will be tracked and stored separately from the source, so it will not affect the source table ","version":"Next","tagName":"h2"},{"title":"3.1 Querying Files","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.1 Querying Files","content":"","keywords":"","version":"Next"},{"title":"How to extract data directly from files​","type":1,"pageTitle":"3.1 Querying Files","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.1 Querying Files#how-to-extract-data-directly-from-files","content":"  You will learn how to extract data directly from files using Spark SQL and different methods to review raw files contents We will see how to use the Spark SQL to configure options for extracting data from external sources and how to use CTAS statements to create Delta Lake tables To query file content, we can simply use a SELECT statement. SELECT * FROM a file format, and we specify the file path info SELECT * FROM file_format.`/path/to/file` Make a special note of the use of back ticks (`) and not single quotes around the path This work well with self-describing formats that have well-defined schema like JSON and parquet It is not very useful with non-describing formats like CSV A path file could be a single file. Or we can use wildcard (*) character to read multiple simultaneously. Or simply reading the whole directory info Assuming that all of the files in the directory have the same format and schema Here is an example to query a JSON file info SELECT * FROM json.`/path/file_name.json` When working with text-based files which include JSON, CSV, TSV and TXT format, You can use the &quot;text&quot; format to extract data as raw strings info SELECT * FROM text.`/path/to/file` This can be useful when input data could be corrupted. In this case, we extract the data as raw string and we apply custom text parsing functions to extract values from text files In some cases, we need the binary representation of files content, for example, when dealing with images and unstructured data Here we can use simply binaryFile as a format info SELECT * FROM binaryFile.`/path/to/file` After extracting data from external data sources, we need to load them into the Lakehouse which ensures that all of the benefits of Databricks platform can be fully leveraged To load data from files into Delta tables. We use CTAS statements, which is CREATE TABLE AS SELECT query info CREATE TABLE table_name AS SELECT * FROM file_format.`/path/to/file` Here we are querying data from files directly. CTAS statements inferior schema information from query results and do not support manual schema declaration info This means the CTAS statements are useful for external data injection from sources with well-defined schema such as parquet files and tables warning CTAS statements also do not support specifying additional file options. And this is why this statement presents significant limitation when trying to ingest data from CSV files  ","version":"Next","tagName":"h2"},{"title":"How to use additional file options with the USING option​","type":1,"pageTitle":"3.1 Querying Files","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.1 Querying Files#how-to-use-additional-file-options-with-the-using-option","content":"  We need another solution that supports options. This solution is the regular CREATE TABLE statement, but with the USING keyword By adding the USING keyword, we specify the external data source type, for example CSV format and with any additional options info CREATE TABLE table_name ( column_name1 col_type1 ) USING data_source OPTIONS ( key1=val1, key2=val2 ) LOCATION = path You need to specify a location to where these files are stored info CREATE TABLE table_name ( column_name1 col_type1 ) USING data_source OPTIONS ( key1=val1, key2=val2 ) LOCATION = path With this command, we are always creating an external table The table here is just a reference to the files. Unlike with CTAS statements, here there is no data moving during table creation We are just pointing to files stored in an external location info These files are kept in its original format, which means we are creating here a non-Delta table  ","version":"Next","tagName":"h2"},{"title":"How to create a table from a CSV file​","type":1,"pageTitle":"3.1 Querying Files","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.1 Querying Files#how-to-create-a-table-from-a-csv-file","content":"  Here is an example of creating a table using CSV external source We are pointing to CSV files exist in an external location We are specifying the options for reading the files info Like the fact that there is a header present in the files and the delimiter is a semicolon (;) CREATE TABLE table_name ( col_name1 col_type1 ) USING CSV OPTIONS ( header=&quot;true&quot;, delimiter=&quot;;&quot; ) LOCATION = path We are providing the location to these CSV files info CREATE TABLE table_name ( col_name1 col_type1 ) USING CSV OPTIONS ( header=&quot;true&quot;, delimiter=&quot;;&quot; ) LOCATION = path   ","version":"Next","tagName":"h2"},{"title":"How to create a table using JDBC connection​","type":1,"pageTitle":"3.1 Querying Files","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.1 Querying Files#how-to-create-a-table-using-jdbc-connection","content":"  Another example is to create a table using JDBC connection to refer to data in an external SQL database We provide the necessary OPTIONS like the connection string (url), the username, and the password for this database, and of course, the database table (dbtable) containing the data info CREATE TABLE table_name ( col_name1 col_type1 ) USING JDBC OPTIONS ( url=&quot;jdbc:sqlite://hostname:port&quot;, dbtable=&quot;database.table&quot;, user=&quot;username&quot;, password=&quot;pwd&quot; )   ","version":"Next","tagName":"h2"},{"title":"Limitation of tables from external data sources​","type":1,"pageTitle":"3.1 Querying Files","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.1 Querying Files#limitation-of-tables-from-external-data-sources","content":"  A table with external data source has a limitation It is not a Delta table. It means the performance and the features of Delta Lake are no more guaranteed, like time travel feature, and the guarantee that we are always reading the most recent version of the data If you are referring to a huge database table, this also can cause performance issues info Fortunately, we have a solution for this limitation. The solution is simply to create a temporary view, referring to the external data source, and then query this temporary view to create a table using CTAS statements In this way, we are extracting the data from the external data source and load it in a Delta table As you can see, with CTAS statement, you can not only query files, but you can query any object like a temporary view in this case CREATE TEMP VIEW temp_view_name ( col_name1 col_type1 ) USING data_source OPTIONS ( key1=val1, key2=val2 ); CREATE TABLE table_name AS SELECT * FROM temp_view_name  ","version":"Next","tagName":"h2"},{"title":"2.6 Databases and Tables on Databricks (Hands On)","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.6 Databases and Tables on Databricks (","content":"","keywords":"","version":"Next"},{"title":"How to create an external table​","type":1,"pageTitle":"2.6 Databases and Tables on Databricks (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.6 Databases and Tables on Databricks (#how-to-create-an-external-table","content":"  Let us now create an external table under the default database To create an external table, you need to simply to specify in the CREATE TABLE statement the LOCATION keyword followed by the path to where this table needs to be stored. info In our case, we store this table under /mnt/demo directory CREATE TABLE managed_default ( width INT, length INT, height INT ) LOCATION 'dbfs:/mnt/demo/external_default'; INSERT INTO managed_default VALUES ( 3 INT, 2 INT, 1 INT ) Let us take a look on the Data Explorer. Here we can see that the table has been created in the hive metastore Let us now run DESCRIBE EXTENDED on our external table info DESCRIBE EXTENDED external_default Here we can see that this table is indeed an external table, and it is created in the specified location under /mnt/demo. Let us now see what will happen if we drop the managed table The table has been successfully deleted. Let us confirm this by checking the table directory info %fs ls 'dbfs:/user/hive/warehouse/managed_default' DROP TABLE external_default In the hive metastore, we see that both tables are no longer exist info However, if we check the table directory. We see that the table directory and the data files are still there %fs ls 'dbfs:/mnt/demo/external_default' Since this table is created outside the database directory, the underlying data is not managed by Hive. So, dropping the table will not delete the underlying data files as we see here In addition to the default database, we can also create extra databases. To do so we can use either CREATE SCHEMA syntax or CREATE DATABASE syntax, which is actually the same info CREATE SCHEMA new_default Here we can see that the new database has been created Let us run the DESCRIBE DATABASE EXTENDED on our database to see some metadata information info DESCRIBE DATABASE EXTENDED new_default Here we can see that the database itself is created under the default Hive warehouse directory info Notice that the database has .db extension to differentiate it from other table folders in the same directory Let us create some tables in this new database Here we will create also a managed table and an external table info USE new_default; /* managed table */ CREATE TABLE managed_new_default ( width INT, length INT, height INT ); INSERT INTO managed_new_default VALUES ( 3 INT, 2 INT, 1 INT ); /* external table */ CREATE TABLE external_new_default ( width INT, length INT, height INT ) LOCATION 'dbfs:/mnt/demo/external_new_default'; INSERT INTO external_new_default VALUES ( 3 INT, 2 INT, 1 INT ); To create a new table in a database different then the default one, you need to specify the database tob e used through the USE keywords Let us run this command. In the Data Explorer, we see that the two tables have been created info DESCRIBE EXTENDED managed_new_default Here, we can see that this new table is indeed a managed table created in its database folder under the default hive warehouse directory The second table where we use the LOCATION keyword has been defined as external table under /mnt/demo location We can simply drop those two tables to see again that the table directory and the data files of the managed table have been all removed info DROP TABLE managed_new_default; DROP TABLE external_new_default; In the Data Explorer, we see that both tables have been dropped from the new database. However, as expected, the table directory and the data files of the external table are still there Let us finally create a database in a custom location outside of the Hive directory info CREATE SCHEMA custom LOCATION 'dbfs:/Shared/schemas/custom.db' As we can see in the Data Explorer, the database has been really created in the hive metastore. However, if we run the DESCRIBE DATABASE EXTENDED, we see that it is created in the custom location we have defined during the creation of the database, and it is different from the default hive directory info DESCRIBE DATABASE EXTENDED custom You can normally create managed and external tables in this database info USE custom; /* managed table */ CREATE TABLE managed_custom ( width INT, length INT, height INT ); INSERT INTO managed_custom VALUES ( 3 INT, 2 INT, 1 INT ); /* external table */ CREATE TABLE external_custom ( width INT, length INT, height INT ) LOCATION 'dbfs:/mnt/demo/external_custom'; INSERT INTO external_custom VALUES ( 3 INT, 2 INT, 1 INT ); In hive metastore, we can see the tables of our custom location The managed_custom table is indeed a managed table since it is created in the database folder located in a custom location The second table is an external table since it is created outside the database directory By dropping the two tables, we can see that they have successfully deleted from the hive metastore The managed table directory and the data files are no longer exist in the database directory located in the custom location. While the external table's directory, and its data file are not deleted and are still in this external location outside the database directory ","version":"Next","tagName":"h2"},{"title":"2.9 Working with Views (HandsOn)","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.9 Working with Views","content":"","keywords":"","version":"Next"},{"title":"How to create a temporary view​","type":1,"pageTitle":"2.9 Working with Views (HandsOn)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.9 Working with Views#how-to-create-a-temporary-view","content":"  Let us now create a temporary view The syntax is very similar but we add a TEMPORARY keyword or simply a TEMP keyword The logical query of this view is simply retrieving the unique list of the brands in our smartphones table info CREATE TEMP VIEW temp_view_phones_brands AS SELECT DISTINCT brand FROM smartphones; SELECT * FROM temp_view_phones_brands Here is the list of brands retrieved by the temporary view Let us run the SHOW TABLE command again info SHOW TABLES; Here, we see that the temporary view has been added to the list The isTemporary column shows that this view is indeed a temporary object. And since it is temporary, it is not persisted to any database   ","version":"Next","tagName":"h2"},{"title":"How to create a global table​","type":1,"pageTitle":"2.9 Working with Views (HandsOn)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.9 Working with Views#how-to-create-a-global-table","content":"  We create a global temporary view We simply add the GLOBAL keyword to the command Our global temporary view is called global_temp_view_latest_phones, since the logical query here is retrieving all the smartphones from our table that has a releasing year newer than 2020 and we order the result in descending order, to show first the most recent phones info CREATE GLOBAL TEMP VIEW global_temp_view_latest_phones AS SELECT * FROM smartphones WHERE year &gt; 2020 ORDER BY year DESC; To query a global temporary view in a select statement, we need to use the global database qualifier, which is in fact a temporary database in the cluster info SELECT * FROM global_temp.global_temp_view_latest_phones; Let us review one last time the database table and views info SHOW TABLES; Our global temporary view is not listed here as it is tied to the global_temp database To show tables and views in the global_temp database. We use the command SHOW TABLES IN and we specify the database, in our case, it's global_temp info SHOW TABLES IN global_temp; Here, we can see the global_temp_view_latest_phones, which is tied to the global_temp database info Since our temp_view_phones_brands is not tied to any database, it is usually shown with every SHOW TABLES command  ","version":"Next","tagName":"h2"},{"title":"How tables and views are persisted across multiple sessions​","type":1,"pageTitle":"2.9 Working with Views (HandsOn)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.9 Working with Views#how-tables-and-views-are-persisted-across-multiple-sessions","content":"  We are going to create a new notebook, which means a new SparkSession, and explore how our created views will behave there In this new SparkSession. let us first run the SHOW TABLES command info SHOW TABLES; The result here confirms that the table smartphones, of course, still exist As expected, the stored view of Apple phones still exist in this new session. However, the temporary view of the brand phones does not exist Let us go back to the previous notebook and rerun the SHOW TABLES command info SHOW TABLES; All these three tables and views from this old session are still here Let us go back to the new notebook Temporary views are not accessible, for example, from another notebook Or after detaching and re-attaching a notebook to a cluster or after installing a Python package, which in turn, restarts the Python interpreter, or simply after restarting the cluster What about the global temporary view? Let us run the command SHOW TABLES IN global_temp info SHOW TABLES IN global_temp; Our global temporary view still exists info In fact, as long as the cluster in running, the database global_temp persist, and any notebook attached to the cluster can access its global temporary views We can see this in action by querying this global temp view in this session info SELECT * FROM global_temp.temp_view_latest_phone; If you were to restart the cluster, this statement would fail because the view will no longer exist  ","version":"Next","tagName":"h2"},{"title":"4.1 Structured Streaming","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#introduction","content":"  This lecture focuses on Spark Structured Streaming in Databricks You will learn what is a Data Stream and how to process streaming data using Spark Structured Streaming You will also understand how to use DataStreamReader to perform a stream read from a source and how to use and configure DataStreamWriter to perform a streaming write to sink  ","version":"Next","tagName":"h2"},{"title":"Data Stream​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#data-stream","content":"  A Data Stream is any data source that grows over time info New data in a Data Stream might correspond to: A new JSON log file landing into a cloud storage Updates to a database captured in a CDC or Change Data Capture feed Events queued in a pub/sub messaging feed like Kafka  ","version":"Next","tagName":"h2"},{"title":"Approaches to data stream processing​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#approaches-to-data-stream-processing","content":"  There are 2 approaches to processing a data stream:   Reprocess the entire dataset each time you receive a new update to your data Only capture files or records that have been added since the last time an update was run  ","version":"Next","tagName":"h3"},{"title":"Spark Structured Streaming​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#spark-structured-streaming","content":"  Spark Structured Streaming is a scalable streaming processing engine Spark Structured Streaming allows you to query an infinite data source and automatically detect new data and process the result incrementally into a data sink A sink is just a durable file system, such as files or tables  ","version":"Next","tagName":"h2"},{"title":"How to interact and query an infinite data source?​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#how-to-interact-and-query-an-infinite-data-source","content":"  Spark Structured Streaming allows the user to interact with a streaming source by treating the source as if it were a static table of records info New data in the input data stream is simply treated as a new rows appended to a table Such a table representing a infinite data source is seen as an &quot;unbounded&quot; table  ","version":"Next","tagName":"h3"},{"title":"Input Streaming Table​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#input-streaming-table","content":"  An input data stream could be: A directory of files; A messaging system like Kafka; A Delta table. Delta Lake is well-integrated with Spark Structured Streaming We can simply use spark.readStream to query the delta table as a streaming source, which allows to process: All of the data present in the table; Any new data that arrive later info streamDF = spark.readStream.table(&quot;Input_Table&quot;) This creates a streaming data frame on which we can apply any transformation as if it were just a static data frame  ","version":"Next","tagName":"h2"},{"title":"How to persist the result of a streaming query​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#how-to-persist-the-result-of-a-streaming-query","content":"  To persist the result of a streaming query, we need to write them out to durable storage using dataframe.writeStream method With the writeStream method, we can configure our output info We can trigger the streaming processing every 2 minutes to check if there are new arriving records, and we choose to append them to the target table streamDF = spark.readStream.table(&quot;Input_Table&quot;) streamDF.writeStream.trigger( processingTime=&quot;2 minutes&quot; ).outputMode( &quot;append&quot; ).option( &quot;checkpointLocation&quot;, &quot;/path&quot; ).table(&quot;Output_Table&quot;) All this happened thanks to checkpoints created by Spark to track the progress of your streaming processing  ","version":"Next","tagName":"h3"},{"title":"Trigger methods​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#trigger-methods","content":"  When defining a streaming write, the trigger method specify when the system should store the next set of data info This is called the trigger interval By default, if you don't provide any trigger interval, the data will be processed every half second  ","version":"Next","tagName":"h2"},{"title":"Trigger based on fixed intervals with processingTime option​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#trigger-based-on-fixed-intervals-with-processingtime-option","content":"  You can specify a fixed interval. The data will be processed in micro batches at your specified interval, for example every 5 minutes  ","version":"Next","tagName":"h3"},{"title":"Trigger based on available data with Once or availableNow option​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#trigger-based-on-available-data-with-once-or-availablenow-option","content":"  You can run your stream in a batch mode to process all available data at once using either trigger Once option or availableNow option With the Once and availableNow option, the trigger will stop on its own once all available data is processed The difference between Once and availableNow is: With Once, all data will be processed in a single batch; With availableNow, all data is processed in multiple micro batches  ","version":"Next","tagName":"h3"},{"title":"Output Modes​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#output-modes","content":"  In append mode, only new rows are incrementally appended to the target table with each batch In complete mode, the result table is recalculated each time a write is triggered, so the target table is overwritten with each batch  ","version":"Next","tagName":"h2"},{"title":"Checkpointing​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#checkpointing","content":"  Databricks creates checkpoints by storing the current state of your streaming job to cloud storage info An important note here is that checkpoints cannot be shared between several streams A separate checkpoint location is required for every streaming write to ensure processing guarantees  ","version":"Next","tagName":"h2"},{"title":"Guarantees​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#guarantees","content":"  Structured streaming provide two guarantees:  ","version":"Next","tagName":"h2"},{"title":"Fault Tolerance​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#fault-tolerance","content":"  The streaming agent can resume from where it left off if there is a failure checkpointing and the mechanism called write-ahead logs allows the user to track the streaming progress by recording the offset range of data being processed during each trigger interval  ","version":"Next","tagName":"h3"},{"title":"Exactly-once guarantee​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#exactly-once-guarantee","content":"  Structured streaming also ensures exactly once data processing because the streaming sink are designed to be idempotent info Multiple writes of the same data identified by the offset, do not result in duplicates being written to the sink The two guarantees here only work if the streaming source is repeatable, like cloud-based object storage or pub/sub messaging service Repeatable data sources and idempotent sinks allows Spark Structured Streaming to ensure end-to-end exactly once semantics under any failure condition  ","version":"Next","tagName":"h3"},{"title":"Unsupported Operations​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#unsupported-operations","content":"  We need to understand that some operations are not supported by streaming data frames Operations such as sorting and deduplication, are either too complex or logically not possible to do when working with streaming data There are advanced streaming methods like windowing and watermarking that can help to do such operations ","version":"Next","tagName":"h2"},{"title":"3.5 Higher Order Functions and SQL UDFs","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.5 Higher Order Functions and SQL UDFs","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"3.5 Higher Order Functions and SQL UDFs","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.5 Higher Order Functions and SQL UDFs#introduction","content":"  In this notebook, we will explore higher order functions and user-defined functions also known as UDF We will continue using our bookstore dataset, which contains the 3 tables of customers, orders and books  ","version":"Next","tagName":"h2"},{"title":"Dataset setup​","type":1,"pageTitle":"3.5 Higher Order Functions and SQL UDFs","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.5 Higher Order Functions and SQL UDFs#dataset-setup","content":"  Let us start by copying this dataset info %run ../Includes/Copy-Datasets Let us take a look again on our orders table info SELECT * FROM orders As you can see here, the books column is of complex data type info In fact, it's an array of a struct type To work directly with such a complex datatype We need to use higher order functions  ","version":"Next","tagName":"h2"},{"title":"What are Higher Order Functions?​","type":1,"pageTitle":"3.5 Higher Order Functions and SQL UDFs","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.5 Higher Order Functions and SQL UDFs#what-are-higher-order-functions","content":"  Higher order functions allow you to work directly with hierarchical data like arrays and map type objects  ","version":"Next","tagName":"h2"},{"title":"How to use higher order functions?​","type":1,"pageTitle":"3.5 Higher Order Functions and SQL UDFs","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.5 Higher Order Functions and SQL UDFs#how-to-use-higher-order-functions","content":" ","version":"Next","tagName":"h2"},{"title":"Use the filter function to filter an array based on a condition​","type":1,"pageTitle":"3.5 Higher Order Functions and SQL UDFs","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.5 Higher Order Functions and SQL UDFs#use-the-filter-function-to-filter-an-array-based-on-a-condition","content":"  One of the most common higher order functions is the filter function which filters an array using a given lambda function In this example, we are creating a new column called multiple_copies, where we filter the books column to extract only those books that have a quantity greater or equal to 2 info It means they have been bought in multiple copies, 2 or more We are creating a new column called multiple_copies, where we have an array that contains only the filtered data info SELECT order_id, books, FILTER(books, i -&gt; i.quantity &gt;= 2) AS multiple_copies FROM orders As you can see, we are creating a lot of empty array in this new column In this case, it is useful to use a WHERE clause to show only non-empty array values in the return column We can accomplish that by using a subquery, which is a query within another query in order to apply the WHERE clause on the size of the returned column info SELECT order_id, books, FILTER(books, i -&gt; i.quantity &gt;= 2) AS multiple_copies FROM orders WHERE size(multiple_copies) &gt; 0;   ","version":"Next","tagName":"h3"},{"title":"Use the transform function to apply transformations to every single item in an array​","type":1,"pageTitle":"3.5 Higher Order Functions and SQL UDFs","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.5 Higher Order Functions and SQL UDFs#use-the-transform-function-to-apply-transformations-to-every-single-item-in-an-array","content":"  Our second higher order function is the transform function that is used to apply a transformation on all the items in an array and extract the transformed value In this example, for each book in the books array, we are applying a discount on the subtotal value info SELECT order_id, books, TRANSFORM ( books, b -&gt; CAST(b.subtotal * 0.8 AS INT) ) AS subtotal_after_discount FROM orders; As you can see, we created a new column containing an array of the transformed values for each element in the books array  ","version":"Next","tagName":"h3"},{"title":"How to create user-defined functions (UDFs)?​","type":1,"pageTitle":"3.5 Higher Order Functions and SQL UDFs","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.5 Higher Order Functions and SQL UDFs#how-to-create-user-defined-functions-udfs","content":"  Let us now talk about user-defined functions or UDFs, which allow you to register a custom combination of SQL logic as function in a database, making these methods reusable in any SQL query UDF functions leverage Spark SQL directly maintaining all the optimization of Spark when applying your custom logic to large datasets Here is an example of creating a user-defined function. At minimum, it requires a function name, optional parameters, the type to be returned, and some custom logic info CREATE OR REPLACE FUNCTION get_url( email STRING ) RETURNS STRING RETURN CONCAT(&quot;https://www.&quot;, split(email, &quot;@&quot;)[1]) Our function here is named get_url, that accepts an email address as an argument and return a value of type STRING. Here we are splitting the email into two parts based on the @ sign info CREATE OR REPLACE FUNCTION get_url( email STRING ) RETURNS STRING RETURN CONCAT(&quot;https://www.&quot;, split(email, &quot;@&quot;)[1])   ","version":"Next","tagName":"h2"},{"title":"How to use UDF functions​","type":1,"pageTitle":"3.5 Higher Order Functions and SQL UDFs","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.5 Higher Order Functions and SQL UDFs#how-to-use-udf-functions","content":"  Let us now start to using it. Here we are applying our UDF on the customer emails to get the URL address info SELECT email, get_url(email) AS domain FROM customers note User defined functions are permanent objects that are persisted to the database, so you can use them between different Spark Sessions and Notebooks With DESCRIBE FUNCTION command, we can see where it was registered and basic information about expected inputs and the expected return type info DESCRIBE FUNCTION get_url As you can see our function, it belongs to the default database and accept the email address as a string input, and return a string value We can get even more information by running DESCRIBE FUNCTION EXTENDED info DESCRIBE FUNCTION EXTENDED get_url For example, the Body field at the bottom shows the SQL logic used in the function itself  ","version":"Next","tagName":"h2"},{"title":"How to add more complex logic in UDF functions​","type":1,"pageTitle":"3.5 Higher Order Functions and SQL UDFs","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.5 Higher Order Functions and SQL UDFs#how-to-add-more-complex-logic-in-udf-functions","content":"  We can have more complex logic in our function. For example, here we are applying the standard SQL CASE WHEN statements in order to evaluate multiple condition statements within our function Here, for example, we are checking the email extension using the LIKE command in order to detect the category of the domain name and otherwise, we are reporting it as unknown extension info CREATE FUNCTION site_type(email STRING) RETURNS STRING RETURN CASE WHEN email LIKE &quot;%.com&quot; THEN &quot;Commerical business&quot; WHEN email LIKE &quot;%.org&quot; THEN &quot;Non-profit organizations&quot; WHEN email LIKE &quot;%.edu&quot; THEN &quot;Educational institution&quot; ELSE CONCAT( &quot;Unknown extension for domain: &quot;, SPLIT(email, &quot;@&quot;)[1] ) END; SELECT email, site_type(email) as domain_category FROM customers; As you can see UDF functions are really powerful  ","version":"Next","tagName":"h2"},{"title":"3.3 Writing to Tables (Hands On)","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.3 Writing to Tables Hands On","content":"","keywords":"","version":"Next"},{"title":"Benefits of overwriting tables instead of deleting​","type":1,"pageTitle":"3.3 Writing to Tables (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.3 Writing to Tables Hands On#benefits-of-overwriting-tables-instead-of-deleting","content":"  There are multiple benefits to overwriting tables instead of deleting and recreating tables info For example, the old version of the table still exists and can easily retrieve all data using Time Travel Overwriting a table is much faster because it does not need to list the directory recursively or delete any files Concurrent queries can still read the table while you are overwriting it info Due to the ACID transaction guarantees, if overwriting the table fails, the table will be in its previous state  ","version":"Next","tagName":"h2"},{"title":"How to overwrite existing tables​","type":1,"pageTitle":"3.3 Writing to Tables (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.3 Writing to Tables Hands On#how-to-overwrite-existing-tables","content":" ","version":"Next","tagName":"h2"},{"title":"Overwrite tables with CREATE OR REPLACE​","type":1,"pageTitle":"3.3 Writing to Tables (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.3 Writing to Tables Hands On#overwrite-tables-with-create-or-replace","content":"  The first method to accomplish complete overwrite is to use CREATE OR REPLACE TABLE info CREATE OR REPLACE TABLE orders AS SELECT * FROM parquet.`${dataset.bookstore}/orders` CREATE OR REPLACE TABLE statements fully replace the content of a table each time they execute Let us now check our table history info DESCRIBE HISTORY orders As you can see, the version 0 is a CREATE TABLE AS SELECT statement. While CREATE OR REPLACE statement has generated a new table version  ","version":"Next","tagName":"h3"},{"title":"Overwrite tables with INSERT OVERWRITE​","type":1,"pageTitle":"3.3 Writing to Tables (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.3 Writing to Tables Hands On#overwrite-tables-with-insert-overwrite","content":"  The second method to overwrite table data is to use INSERT OVERWRITE statement. It provide a nearly identical output as above info INSERT OVERWRITE orders SELECT * FROM parquet.`${dataset.bookstore}/orders` It means data in the target table will be replaced by data from the query INSERT OVERWRITE statement has some differences info For example, it can only overwrite an existing table and not creating a new one like our CREATE OR REPLACE statement It can override only the new records that match the current table schema, which means that it is a safer technique for overwriting an existing table without the risk of modifying the table schema info INSERT OVERWRITE orders SELECT * FROM parquet.`${dataset.bookstore}/orders` We can see our table history info DESCRIBE HISTORY orders As you can see here, the INSERT OVERWRITE operation has been recorded as a new version in the table as WRITE operation If you try to insert overwrite the data with different schema, for example, here we are adding a new column of the data for the current timestamp warning By running this command, we see that it generates an exception info INSERT OVERWRITE orders SELECT *, current_timestamp() FROM parquet.`${dataset.bookstore}/orders` The exception says a schema mismatch detected when writing to the Delta table tip The way how they enforce schema on-write is the primary difference between INSERT OVERWRITE and CREATE OR REPLACE TABLE statements  ","version":"Next","tagName":"h3"},{"title":"How to add records to an existing table​","type":1,"pageTitle":"3.3 Writing to Tables (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.3 Writing to Tables Hands On#how-to-add-records-to-an-existing-table","content":"  Let us now talk about appending records to tables. The easiest method is to use INSERT INTO statement Here we are inserting a new data using an input query that query the parquet files in the orders-new directory We have successfully added 700 new records to our table. And we can check the new number of orders info INSERT INTO orders SELECT * FROM parquet.`${dataset.bookstore}/orders-new`   ","version":"Next","tagName":"h2"},{"title":"How to prevent the same records from being inserted with MERGE​","type":1,"pageTitle":"3.3 Writing to Tables (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.3 Writing to Tables Hands On#how-to-prevent-the-same-records-from-being-inserted-with-merge","content":"  The INSERT INTO statement is a simple and efficient operation for inserting new data. However, it does not have any built in guarantees to prevent inserting the same records multiple times info It means re-executing the query will write the same records to the target table resulting in duplicate records To resolve this issue, we can use our second method, which is MERGE INTO statement With the MERGE statement, you can upsert data from a source table, view, or dataframe into the target data table info It means you can insert, update and delete using the MERGE INTO statements Here we will use the MERGE operation to update the customer data with updated emails and adding new customers We are creating a temporary view of the new customer data info CREATE OR REPLACE TEMP VIEW customers_updates AS SELECT * FROM json.`${dataset.bookstore}/customers-json-new`; MERGE INTO customers c USING customers_updates u ON c.customer_id = u.customer_id WHEN MATCHED AND c.email and u.email IS NOT NULL THEN UPDATE SET email = u.email, updated = u.updated WHEN NOT MATCHED THEN INSERT * We can apply the MERGE operation that says MERGE INTO customers the new changes coming from customer_updates TEMP VIEW on the customer_id key info CREATE OR REPLACE TEMP VIEW customers_updates AS SELECT * FROM json.`${dataset.bookstore}/customers-json-new`; MERGE INTO customers c USING customers_updates u ON c.customer_id = u.customer_id WHEN MATCHED AND c.email and u.email IS NOT NULL THEN UPDATE SET email = u.email, updated = u.updated WHEN NOT MATCHED THEN INSERT * We have two actions here. When MATCH, we do an UPDATE and when NOT MATCH, we do an INSERT. In addition, we add extra conditions In this case, we are checking that the current row has a NULL email while the new record does not. In such a case, we UPDATE the email and we also update the last updated timestamp If the new record does not match any existing customers based on the customer_id, in this case, we will insert this new record. info CREATE OR REPLACE TEMP VIEW customers_updates AS SELECT * FROM json.`${dataset.bookstore}/customers-json-new`; MERGE INTO customers c USING customers_updates u ON c.customer_id = u.customer_id WHEN MATCHED AND c.email and u.email IS NOT NULL THEN UPDATE SET email = u.email, updated = u.updated WHEN NOT MATCHED THEN INSERT * As we can see here, we have updated 100 records and we have inserted 201 records. And no records have been deleted  ","version":"Next","tagName":"h2"},{"title":"How to avoid duplicates when inserting records with MERGE​","type":1,"pageTitle":"3.3 Writing to Tables (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.3 Writing to Tables Hands On#how-to-avoid-duplicates-when-inserting-records-with-merge","content":"  In a MERGE operation, updates, inserts and deletes are completed in a single atomic transaction MERGE operation is a great solution for avoiding duplicates when inserting records  ","version":"Next","tagName":"h2"},{"title":"Create a temporary view from CSV files​","type":1,"pageTitle":"3.3 Writing to Tables (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.3 Writing to Tables Hands On#create-a-temporary-view-from-csv-files","content":"  Let us see another example. Here we have new books to be inserted and they are coming in CSV Format We will create this temporary view (TEMP VIEW) against this new data info CREATE OR REPLACE TEMP VIEW books_updates ( book_id STRING, title STRING, author STRING, category STRING, price DOUBLE ) USING CSV OPTIONS ( path=&quot;${dataset.bookstore}/books-csv-new&quot;, header=&quot;true&quot;, delimiter=&quot;;&quot; ); SELECT * FROM books_updates Here we have first new books and we are only interested by inserting the computer science books in our database  ","version":"Next","tagName":"h3"},{"title":"Update delta table from temporary view​","type":1,"pageTitle":"3.3 Writing to Tables (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.3 Writing to Tables Hands On#update-delta-table-from-temporary-view","content":"  Let us now use the MERGE INTO statement to update the table books with the data coming from the temporary view books_updates We can use the MERGE INTO statement where we provide only the NOT MATCH condition info It means we are only inserting new data if they are not already exist based on our key, which is the book_id and the title We are specifying the category of the new record to be inserted is only Computer Science info **`MERGE`** INTO books b USING books_updates u ON b.book_id = u.book_id AND b.title = u.title WHEN NOT MATCHED AND u.category = 'Computer Science' THEN INSERT * As expected, we are only inserting three new records, which are the 3 computer science books One of the main benefits of the MERGE operation is to avoid duplicate If we try to rerun this statement, it will not re-insert those records as they are already on the table ","version":"Next","tagName":"h3"},{"title":"4.5 Multi-hop Architecture","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.5 Multihop Architecture","content":"","keywords":"","version":"Next"},{"title":"What is a multi-hop architecture?​","type":1,"pageTitle":"4.5 Multi-hop Architecture","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.5 Multihop Architecture#what-is-a-multi-hop-architecture","content":"  A multi-hop architecture, also known as medallion architecture, is a design pattern used to logically organize data in a multi-layered approach Its goal is to incrementally improve the structure and the quality of the data as it flows through each layer of the architecture   ","version":"Next","tagName":"h2"},{"title":"Layers of a multi-hop architecture​","type":1,"pageTitle":"4.5 Multi-hop Architecture","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.5 Multihop Architecture#layers-of-a-multi-hop-architecture","content":"  Multi-hop architecture usually consists of 3 layers  Bronze Bronze table contains raw data ingested from various sources info Like JSON files, operational databases, or Kafka Stream, for example Silver Silver tables provide a more refined view of our data info For example, data can be cleaned and filtered at this level note We can also join fields from various bronze tables to enrich our silver records Gold The gold table provide business-level aggregations, often used for reporting and dashboarding or even for machine learning    With this architecture, we incrementally improve the structure and the quality of data as it flows through each layer  ","version":"Next","tagName":"h2"},{"title":"Benefits with multi-hop architecture​","type":1,"pageTitle":"4.5 Multi-hop Architecture","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.5 Multihop Architecture#benefits-with-multi-hop-architecture","content":"  It is a simple data model that is easy to understand and implement It enables incremental ETL, that is Extract, Transform and Load data incrementally It can combine streaming and batch workloads in the same pipeline info Each stage can be configured as a batch or streaming job It can recreate your tables from raw data at any time ","version":"Next","tagName":"h2"},{"title":"3.2 Query Files (Hands On)","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.2 Query Files Hands On","content":"","keywords":"","version":"Next"},{"title":"How to list the files in a directory​","type":1,"pageTitle":"3.2 Query Files (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.2 Query Files Hands On#how-to-list-the-files-in-a-directory","content":"  Let us list the files in the customers directory info files = dbutils.fs.ls( f&quot;{dataset_bookstore}/customers-json&quot; ) display(files) We can see that there are 6 JSON files in this directory  ","version":"Next","tagName":"h2"},{"title":"How to query a JSON file​","type":1,"pageTitle":"3.2 Query Files (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.2 Query Files Hands On#how-to-query-a-json-file","content":"  To query a single JSON file, we use SELECT * FROM json, and we specify the full path for this JSON file info Notice here that we are using back ticks and not single quotes around the path SELECT * FROM json.`${dataset.bookstore}/customers-json/export_001.json` We managed to read the table, and here we can see the different columns of our table, the customer's ID, the email, the profile information, which itself is JSON string and the last updated timestamp We can see also that our preview display shows all the 300 records of our source file We can also use a wildcard character (*) to query multiple files simultaneously info SELECT * FROM json.`${dataset.bookstore}/customers-json/export_*.json` For example, here we are querying all the JSON files, starting with the name export_ By default, the preview display shows only the first 1000 records  ","version":"Next","tagName":"h2"},{"title":"How to query a complete directory​","type":1,"pageTitle":"3.2 Query Files (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.2 Query Files Hands On#how-to-query-a-complete-directory","content":"  We can query a complete directory of files, assuming all the files in the directory have the same format and schema Here we will specify simply the directory path rather than an individual file. info SELECT COUNT(*) FROM json.`{dataset.bookstore}/customers-json` When reading multiple files, it is useful to add the input_file_name() function, which is a built-in Spark SQL command that records the source data file for each record info SELECT input_file_name() as source_file FROM json.`{dataset.bookstore}/customers-json`; info This can be especially helpful if troubleshooting problems in the source data become necessary In addition to our columns, we have also the source file column Another interesting option here is to use the text format, which allows you to query and text-based files like JSON, CSV, TSV, or TEXT format info SELECT input_file_name() as source_file FROM text.`{dataset.bookstore}/customers-json` This load each line of the file as a row with one string column, Value This can be useful when data could be corrupted. And we need to use, in such cases, some custom text parsing function to extract data We can use binaryFile to extract the raw bytes and some metadata of files info SELECT input_file_name() as source_file FROM binaryFile.`{dataset.bookstore}/customers-json` As we can see here, this gives us the path of the file, the modification time, the length and the content, which is the binary representation of the file  ","version":"Next","tagName":"h2"},{"title":"How to query and parse a CSV file​","type":1,"pageTitle":"3.2 Query Files (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.2 Query Files Hands On#how-to-query-and-parse-a-csv-file","content":"  Let us now switch to reading books data which is coming in CSV format. In the same way we will use the SELECT statement, but this time with the CSV format info SELECT input_file_name() as source_file FROM csv.`{dataset.bookstore}/customers-json` warning We managed to read the data. However, it is not well parsed The header row is being extracted as a table row and all columns are being loaded in a single column It seems that this is because of the delimiter of the file, which is in our case, a semicolon instead of a comma Querying files in this way works well only with self-describing formats Formats that have well-defined schema like JSON and parquet. However, for other formats like CSV where there is no schema defined, this does not work and we need another way that allows us to provide additional configuration and schema declaration   ","version":"Next","tagName":"h2"},{"title":"How to query files without a schema defined such as CSV​","type":1,"pageTitle":"3.2 Query Files (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.2 Query Files Hands On#how-to-query-files-without-a-schema-defined-such-as-csv","content":"  One solution is to create a table with the USING keyword info This allows us to create a table against an external sources like CSV format Here we need to specify the table schema. I mean, the column names and types, the file format, which is in our case, CSV and whether if there is a header in the source files, and the delimiter used to separate fields, in our case, it's a semicolon We need to specify the location to the files directory info CREATE TABLE books_csv ( book_id STRING, title STRING, author STRING, category STRING, price DOUBLE ) USING CSV OPTIONS ( header=&quot;true&quot;, delimiter=&quot;;&quot; ) LOCATION &quot;${dataset.bookstore}/books-csv&quot; Let's now query this table info SELECT * FROM books_csv We have managed to read the books that are in the CSV files info Remember, when working with CSV files as a data source, it is important to ensure that column orders does not change if additional data files will be added to the source directory Spark will always load data and apply column names and data types in the order specified during table creation Let us now run DESCRIBE EXTENDED to see some information on our table info DESCRIBE EXTENDED books_csv Here we can see that we have created an external table. And this table is not a Delta table info It's a table that's referring directly to the CSV files. It means that no data has moved during table creation All the metadata and options passed during table creation will be persisted to the metadata, ensuring that data in the location will always be read with these options Let us now see the impact of not having a Delta Table All the guarantees and features that we have them usually when work with Delta Tables we will no longer having them with external data sources like CSV info For example, Delta Lake Tables guarantee that you always query the most recent version of your source data, while tables registered against other data sources like CSV may represent older cached versions Let us add some new CSV file to our directory and see what will happen Let us check how many CSV files we have in the directory Here we will use a Spark Dataframe API that allows us to write data in a specific format like CSV info spark.read.table( &quot;books_csv&quot; ).write.mode( &quot;append&quot; ).format( &quot;csv&quot; ).option( &quot;header&quot;, &quot;true&quot; ).option( &quot;delimiter&quot;, &quot;;&quot; ).save(f&quot;{dataset_bookstore}/books-csv&quot;) We are going to read our books table we have just created We are going to rewrite the table data in new additional CSV files in the same directory Let us now see how many CSV files in the directory info files = dbutils.fs.ls( f&quot;{dataset_bookstore}/books-csv&quot; ) display(files) There are extra CSV files that have been written to the directory by Spark If we calculate the number of books in our table, we should see 24 rows instead of 12 info SELECT COUNT(*) FROM books_csv warning Even with the new data has been successfully written to the table directory, we are still unable to see this new data This is because Spark automatically cached the underlying data in local storage to ensure that on subsequent queries, Spark will provide the optimal performance by just querying this local cache This external CSV file is not configured to tell Spark that it should refresh this data  ","version":"Next","tagName":"h2"},{"title":"How to refresh the table​","type":1,"pageTitle":"3.2 Query Files (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.2 Query Files Hands On#how-to-refresh-the-table","content":"  We can manually refresh the cache of our data by running the REFRESH table command info REFRESH TABLE books_csv Refreshing a table will invalidate its cache, meaning that we will need to scan our original data source and pull all data back into memory info For a very large dataset, this may take a significant amount of time Let us now check again the number of books in our table. And indeed, refreshing the table now we see that we have 24 books info SELECT COUNT(*) FROM books_csv To create Delta tables where we load data from external sources, we use CREATE TABLE AS SELECT statement or CTAS statements info CREATE TABLE customers As SELECT * FROM json.`${dataset.bookstore}/customers-json`; DESCRIBE EXTENDED customers; Here, we create and populate customer data table using data retrieved from this input query These will extract the data from the JSON files and load them into the table, customers From the table metadata, we can see that we are indeed creating a Delta Table, and it is also a managed table We can see that schema has been inferred automatically from the query results CTAS statements automatically infer schema information from a query results and do not support manual schema declaration. info This means that CTAS statements are useful for external data ingestion from sources with well-defined schema such as parquet files and tables warning CTAS statements do not support specifying additional file options which presents significant limitation when trying to ingest data from CSV files We have successfully created a Delta table here, however, the data is not well-parsed To correct this, we need to first to use a reference to the files that allow us to specify options This is what we are doing here by creating this temporary view that allows us to specify file options info CREATE TEMP VIEW books_tmp_vw ( book_id STRING, title STRING, author STRING, category STRING, price DOUBLE ) USING CSV OPTIONS ( path = &quot;${dataset.bookstore}/books-csv.export_*.csv&quot;, header=&quot;true&quot;, delimiter=&quot;;&quot; ); CREATE TABLE books AS SELECT * FROM books_tmp_vw; SELECT * FROM books; We will use this temporary view as the source for our CTAS statement to successfully register the Delta table info CREATE TEMP VIEW books_tmp_vw ( book_id STRING, title STRING, author STRING, category STRING, price DOUBLE ) USING CSV OPTIONS ( path = &quot;${dataset.bookstore}/books-csv.export_*.csv&quot;, header=&quot;true&quot;, delimiter=&quot;;&quot; ); CREATE TABLE books AS SELECT * FROM books_tmp_vw; SELECT * FROM books; Notice here we are retrieving only 12 records because we use the wildcard in the path location Let us finally check the metadata of our Delta Table It's a Delta table where we have extracted all the data from the CSV files and loaded them into this location info DESCRIBE EXTENDED books;  ","version":"Next","tagName":"h2"},{"title":"4.3 Incremental Data Ingestion","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.3 Incremental Data Ingestion","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"4.3 Incremental Data Ingestion","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.3 Incremental Data Ingestion#introduction","content":"  We will talk about incremental data ingestion from files in Databricks We will talk about two methods: COPY INTO command; Auto Loader  ","version":"Next","tagName":"h2"},{"title":"What is incremental data ingestion?​","type":1,"pageTitle":"4.3 Incremental Data Ingestion","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.3 Incremental Data Ingestion#what-is-incremental-data-ingestion","content":"  Incremental data ingestion is the ability to load data from new files that have been encountered since the last ingestion Each time we run our data pipeline, we don't need to reprocess the files we have processed before. We need to process only the new arriving data files Databricks provides 2 mechanisms for incrementally and efficiently processing new data files as they arrive in a storage location: COPY INTO SQL command Auto Loader  ","version":"Next","tagName":"h2"},{"title":"COPY INTO​","type":1,"pageTitle":"4.3 Incremental Data Ingestion","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.3 Incremental Data Ingestion#copy-into","content":"  COPY INTO is a SQL command that allows a user to load data from a file location into a Delta table The COPY INTO command loads data idempotently and incrementally info Idempotently means each time you run the command, it will load only the new files from the source location while the files that have been loaded before are simply skipped The command is pretty simple. COPY INTO a target table from a specific source location We specify the format of the source file to load, for example, CSV or Parquet and any related format options In adittion to any option to control the operation of the COPY INTO command Here we are loading from a CSV file, having a header and a specific delimiter info COPY INTO my_table FROM '/path/to/files' FILEFORMAT=CSV FORMAT_OPTIONS ( 'delimiter'='|', 'header'='true' ) COPY_OPTIONS ( 'mergeSchema'='true' ) In the COPY_OPTIONS, we are specifying that the schema can be evolved according to the incoming data info COPY INTO my_table FROM '/path/to/files' FILEFORMAT=CSV FORMAT_OPTIONS ( 'delimiter'='|', 'header'='true' ) COPY_OPTIONS ( 'mergeSchema'='true' )   ","version":"Next","tagName":"h2"},{"title":"Auto Loader​","type":1,"pageTitle":"4.3 Incremental Data Ingestion","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.3 Incremental Data Ingestion#auto-loader","content":"  The second method to load data incrementally from files is Auto Loader, which uses structured streaming in Spark to efficiently process new data files as they arrive in a storage location You can use Auto Loader to load billions of files into a table Auto Loader can scale to support real time ingestion of millions of files per hour  ","version":"Next","tagName":"h2"},{"title":"Auto Loader Checkpointing​","type":1,"pageTitle":"4.3 Incremental Data Ingestion","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.3 Incremental Data Ingestion#auto-loader-checkpointing","content":"  Auto Loader uses checkpointing to track the ingestion process and to store metadata of the discovered files Auto Loader ensures that data files are processed exactly once Auto Loader can resume from where it left off if a failure occurs With Auto Loader, we use the readStream and writeStream methods Auto Loader has a specific format of StreamReader called cloudFiles. And in order to specify the format of the source files, we use simply cloudFiles.format option info spark.readStream.format( &quot;cloudFiles&quot; ).option( &quot;cloudFiles.format&quot;, &lt;source_format&gt; ).load( &quot;/path/to/files&quot; ).writeStream.option( &quot;checkpointLocation&quot;, &lt;checkpoint_directory&gt; ).table( &lt;table_name&gt; ) The location of the source files is specified with the load function info Auto Loader will detect new files as they arrive in this location and queue them for ingestion spark.readStream.format( &quot;cloudFiles&quot; ).option( &quot;cloudFiles.format&quot;, &lt;source_format&gt; ).load( &quot;/path/to/files&quot; ).writeStream.option( &quot;checkpointLocation&quot;, &lt;checkpoint_directory&gt; ).table( &lt;table_name&gt; ) We write the data into a target table using the StreamWriter, where you provide the location to store the checkpointing information info spark.readStream.format( &quot;cloudFiles&quot; ).option( &quot;cloudFiles.format&quot;, &lt;source_format&gt; ).load( &quot;/path/to/files&quot; ).writeStream.option( &quot;checkpointLocation&quot;, &lt;checkpoint_directory&gt; ).table( &lt;table_name&gt; ) Auto Loader can automatically configure the schema of your data. It can detect any update to the fields of the source dataset info The inferred schema can be stored to a location to be used later Use the option cloudFiles.schemaLocation to provide the location where Auto Loader can store the schema info This location could be simply the same as the checkpoint location spark.readStream.format( &quot;cloudFiles&quot; ).option( &quot;cloudFiles.format&quot;, &lt;source_format&gt; ).option( &quot;cloudFiles.schemaLocation&quot;, &lt;schema_directory&gt; ).load( &quot;/path/to/files&quot; ).writeStream.option( &quot;checkpointLocation&quot;, &lt;checkpoint_directory&gt; ).option( &quot;mergeSchema&quot;, &quot;true&quot; ).table( &lt;table_name&gt; )   ","version":"Next","tagName":"h3"},{"title":"When to use Auto Loader vs COPY INTO command?​","type":1,"pageTitle":"4.3 Incremental Data Ingestion","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.3 Incremental Data Ingestion#when-to-use-auto-loader-vs-copy-into-command","content":"  Use the COPY INTO command to ingest thousands of files Use Auto Loader to ingest millions of files or more over time Auto Loader can split the processing into multiple batches so it is more efficient at scale info Databricks recommends to use Auto Loader as general best practice when ingesting data from a cloud-object storage ","version":"Next","tagName":"h2"},{"title":"3.4 Advanced Transformations (HandsOn)","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.4 Advanced Transformations Hands On","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"3.4 Advanced Transformations (HandsOn)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.4 Advanced Transformations Hands On#introduction","content":"  In this notebook, we are going to explore advanced transformations present in Spark SQL We will continue using our bookstore dataset, with its 3 tables:  Customers;Orders;Books  ","version":"Next","tagName":"h2"},{"title":"Dataset setup​","type":1,"pageTitle":"3.4 Advanced Transformations (HandsOn)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.4 Advanced Transformations Hands On#dataset-setup","content":"  Let us start by running the copy dataset notebook info %run ../Includes/Copy-Datasets   ","version":"Next","tagName":"h2"},{"title":"Explore nested data structures​","type":1,"pageTitle":"3.4 Advanced Transformations (HandsOn)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.4 Advanced Transformations Hands On#explore-nested-data-structures","content":"  If we query our customer data, we see that we have several columns One of them is the profile information of the customer. It has a complex data structure, which is a nested JSON string As you can see the address itself is a JSON string that contains: Street;CityCountry Let us run DESCRIBE command on our table info DESCRIBE customers As you can see, the profile here is nothing but a string. It's a JSON string  ","version":"Next","tagName":"h2"},{"title":"How to interact with nested JSON structures in tables​","type":1,"pageTitle":"3.4 Advanced Transformations (HandsOn)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.4 Advanced Transformations Hands On#how-to-interact-with-nested-json-structures-in-tables","content":"  Spark SQL has built-in functionality to directly interact with JSON data stored as strings  ","version":"Next","tagName":"h2"},{"title":"Use the : syntax to interact with nested data structures​","type":1,"pageTitle":"3.4 Advanced Transformations (HandsOn)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.4 Advanced Transformations Hands On#use-the--syntax-to-interact-with-nested-data-structures","content":"  We can simply use the colon (:) syntax to traverse nested data structures Here we are accessing the first name of the profile. In the same way we can access the nested value of the country from the address of the profile info SELECT customer_id, profile:first_name, profile:address:country FROM customers   ","version":"Next","tagName":"h3"},{"title":"Use the from_json function to interact with nested data structures​","type":1,"pageTitle":"3.4 Advanced Transformations (HandsOn)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.4 Advanced Transformations Hands On#use-the-from_json-function-to-interact-with-nested-data-structures","content":"  Spark has also the ability to parse JSON object into struct types info struct is a native spark type with nested attributes. This can be done with the from_json function. SELECT from_json(profile) AS profile_struct FROM customers; warning This function requires the schema of the JSON object  How to derive the schema from existing data​   We can derive the schema from our current data info For this we need a sample data of our JSON value with non-null fields SELECT profile FROM customers LIMIT 1   How to use existing data as a schema​   We can copy this sample data and provide it to the schema_of_json function. And we will store this parsed record in a temporary view info CREATE OR REPLACE TEMP VIEW parsed_customers AS SELECT customer_id, from_json( profile, schema_of_json( '{ &quot;first_name&quot;: &quot;Thomas&quot;, &quot;last_name&quot;: &quot;Lane&quot;, &quot;gender&quot;: &quot;Male&quot;, &quot;address&quot;: { &quot;street&quot;: &quot;06 Boulevard Victor Hugo&quot;, &quot;city&quot;: &quot;Paris&quot;, &quot;country&quot;: &quot;France&quot; } }' ) ) as profile_struct FROM customers; SELECT * FROM parsed_customers;   How to interact with nested object using a struct type​   The first thing to notice when you work with a struct type is the ability to interact with the nested object Let us see some more details. As you can see, this new column profile_struct has a struct datatype where the address field also is of a struct type info DESCRIBE parsed_customers   How to interact with subfields using a struct type​   With struct type, we can interact with the subfields using standard period or dot (.) syntax instead of the colon (:) syntax we use for the JSON string info SELECT customer_id, profile_struct.first_name, profile_struct.address.country FROM parsed_customers   ","version":"Next","tagName":"h3"},{"title":"How to flatten fields into columns from a nested data structure with * operation​","type":1,"pageTitle":"3.4 Advanced Transformations (HandsOn)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.4 Advanced Transformations Hands On#how-to-flatten-fields-into-columns-from-a-nested-data-structure-with--operation","content":"  Once a JSON string is converted to a struct type, we can use the * operation to flatten fields into columns info CREATE OR REPLACE TEMP VIEW customers_final AS SELECT customer_id, profile_struct.* FROM parsed_customers; SELECT * FROM customers_final As you can see here, the first_name, last_name, gender and the address itself are all now separate columns  ","version":"Next","tagName":"h2"},{"title":"How to put each element of an array on its own row with explode function​","type":1,"pageTitle":"3.4 Advanced Transformations (HandsOn)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.4 Advanced Transformations Hands On#how-to-put-each-element-of-an-array-on-its-own-row-with-explode-function","content":"  Let us now switch to our orders tables to see another interesting feature, which is the explode function Let us first explore again some fields of our table info SELECT orders_id, customers_id, books FROM orders Here, the books column is an array of struct type Spark SQL has a number of functions specifically to deal with arrays The most important one is the explode function that allows us to put each element of an array on its own row Each element of the book's array has its own row and we are repeating the other information like the customer_id and the order_id info SELECT order_id, customer_id, explode(books) AS book FROM orders   ","version":"Next","tagName":"h2"},{"title":"How to collect unique values from a field (including fields within an array) with collect_set function​","type":1,"pageTitle":"3.4 Advanced Transformations (HandsOn)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.4 Advanced Transformations Hands On#how-to-collect-unique-values-from-a-field-including-fields-within-an-array-with-collect_set-function","content":"  Another interesting function is the collect_set aggregation function that allows us to collect unique values for a field, including fields within arrays info SELECT customer_id, collect_set(order_id) AS orders_set, collect_set(books.book_id) AS book_set FROM orders GROUP BY customer_id As you can see here, the books_set column is actually an array of array The question, can we flatten this array? Yes, we can  ","version":"Next","tagName":"h2"},{"title":"How to keep only the distinct values from an array​","type":1,"pageTitle":"3.4 Advanced Transformations (HandsOn)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.4 Advanced Transformations Hands On#how-to-keep-only-the-distinct-values-from-an-array","content":"  In addition, we can also keep only the distinct values as, for example, for the B08 that is exist in 2 elements of the array info Instead of having the B08 twice after flatten the array, we will get only one unique value Here we are applying flatten function and then we apply the array_distinct function to keep only the distinct values. And here we can see the final results before and after info SELECT collect_set(books.book_id) AS before_flatten, array_distinct( flatten( collect_set(books.book_id) ) ) FROM orders GROUP BY customer_id Here we can see the final results before and after  ","version":"Next","tagName":"h2"},{"title":"JOIN operations in Spark SQL​","type":1,"pageTitle":"3.4 Advanced Transformations (HandsOn)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.4 Advanced Transformations Hands On#join-operations-in-spark-sql","content":"  Let us now switch to join operations Spark SQL supports standard join operations: INNER;OUTER;LEFT;RIGHTCross joins Here we are joining the result of the explode operation to the books lookup table in order to retrieve books information like books, title, and author's name. We simply specify the type of the operation to apply. In our case, it's an inner join based on the book_id key We are storing the results in a view called orders_enriched info CREATE OR REPLACE VIEW orders_enriched AS SELECT * FROM ( SELECT *, explode(books) AS book FROM orders ) o INNER JOIN books b ON o.book.book_id = b.book_id; SELECT * FROM orders_enriched As you can see, we are grabbing for each book the title and the author name and the category for this book  ","version":"Next","tagName":"h2"},{"title":"How to use set operations in Spark SQL​","type":1,"pageTitle":"3.4 Advanced Transformations (HandsOn)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.4 Advanced Transformations Hands On#how-to-use-set-operations-in-spark-sql","content":" ","version":"Next","tagName":"h2"},{"title":"How to use UNION in Spark SQL​","type":1,"pageTitle":"3.4 Advanced Transformations (HandsOn)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.4 Advanced Transformations Hands On#how-to-use-union-in-spark-sql","content":"  Spark SQL also supports set operations like UNION info For example, we can UNION the old and the new data of the orders table CREATE OR REPLACE TEMP VIEW orders_updates AS SELECT * FROM parquet.`${dataset.bookstore}/orders-new`; SELECT * FROM orders UNION SELECT * FROM orders_updates   ","version":"Next","tagName":"h3"},{"title":"How to use INTERSECT in Spark SQL​","type":1,"pageTitle":"3.4 Advanced Transformations (HandsOn)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.4 Advanced Transformations Hands On#how-to-use-intersect-in-spark-sql","content":"  In the same way we can do INTERSECT operation info The INTERSECT command returns all rows found in both relations SELECT * FROM orders INTERSECT SELECT * FROM orders_updates   ","version":"Next","tagName":"h3"},{"title":"How to use MINUS in Spark SQL​","type":1,"pageTitle":"3.4 Advanced Transformations (HandsOn)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.4 Advanced Transformations Hands On#how-to-use-minus-in-spark-sql","content":"  If you are orders MINUS orders_updates, you will get only the orders data without the 700 new records info SELECT * FROM orders MINUS SELECT * FROM orders_updates   ","version":"Next","tagName":"h3"},{"title":"How to use PIVOT in Spark SQL​","type":1,"pageTitle":"3.4 Advanced Transformations (HandsOn)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.4 Advanced Transformations Hands On#how-to-use-pivot-in-spark-sql","content":"  Spark SQL also support PIVOT clause, which is used to change data perspective We can get the aggregated values based on a specific column values, which will be turned to multiple columns used in SELECT clause info We here we have SELECT * FROM and we specify between two parentheses the SELECT statement that we will be the input for this table In the PIVOT clause, the first argument is an aggregation function (SUM, AVG, MAX, etc.), and the column to be aggregated. info CREATE OR REPLACE TABLE transactions AS SELECT * FROM ( SELECT customer_id, book.book_id AS book_id, book.quantity AS quantity FROM orders_enriched ) PIVOT ( SUM(quantity) FOR book_id IN ( 'B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B09', 'B10', 'B11', 'B12', ) ); SELECT * FROM transactions Then we specify the PIVOT column in the FOR subclause. The IN operator contains the pivot column values info CREATE OR REPLACE TABLE transactions AS SELECT * FROM ( SELECT customer_id, book.book_id AS book_id, book.quantity AS quantity FROM orders_enriched ) PIVOT ( SUM(quantity) FOR book_id IN ( 'B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B09', 'B10', 'B11', 'B12', ) ); SELECT * FROM transactions Here we use the PIVOT command to create a new transaction table that flatten out the information contained in the orders table for each customer info Such a flatten data format can be useful for dashboarding, but also useful for applying machine learning algorithms for inference and predictions ","version":"Next","tagName":"h3"},{"title":"4.2 Structured Streaming (Hands On)","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.2 Structured Streaming Hands On","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"4.2 Structured Streaming (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.2 Structured Streaming Hands On#introduction","content":"  We will explore the basics of working with Spark Structured Streaming to allow incremental processing of data We will continue using our bookstore dataset, with its 3 tables:  Customers;Orders;Books  ","version":"Next","tagName":"h2"},{"title":"Dataset setup​","type":1,"pageTitle":"4.2 Structured Streaming (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.2 Structured Streaming Hands On#dataset-setup","content":"  Let us first copy our dataset info %run ../Includes/Copy-Datasets   ","version":"Next","tagName":"h2"},{"title":"How to work with data streaming in Spark SQL​","type":1,"pageTitle":"4.2 Structured Streaming (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.2 Structured Streaming Hands On#how-to-work-with-data-streaming-in-spark-sql","content":"  To work with data streaming in SQL, you must first use spark.readStream method PySpark API spark.readStream method allows to query a Delta table as a stream source info spark.readStream.table( &quot;books&quot; ) We can register a temporary view against this stream source info spark.readStream.table( &quot;books&quot; ).createOrReplaceTempView( &quot;books_streaming_tmp_vw&quot; ) The temporary view created here is a &quot;streaming&quot; temporary view that allows to apply most transformations in SQL the same way as we would with the static data   ","version":"Next","tagName":"h2"},{"title":"How to query a streaming temporary view​","type":1,"pageTitle":"4.2 Structured Streaming (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.2 Structured Streaming Hands On#how-to-query-a-streaming-temporary-view","content":"  Let us first query this streaming temporary view info SELECT * FROM books_streaming_tmp_vw As you can see, the query is still running, waiting for any new data to be displayed here We don't display a streaming result unless a human is actively monitoring the output of a query during development or live dashboarding Let us now apply some aggregations on this streaming temporary view info SELECT author, COUNT(book_id) AS total_books FROM books_streaming_tmp_vw GROUP BY author We are querying a streaming temporary view, this becomes a streaming query that executes infinitely, rather than completing after retrieving a single set of results. And here we are just displaying an aggregation of input as seen by the stream For streaming queries like this, we can always explore an interactive dashboard to monitor the streaming performance warning When working with streaming data, some operations are not supported like sorting info SELECT * FROM books_streaming_tmp_vw ORDER BY author You can use advanced methods like windowing and watermarking to achieve such operations  ","version":"Next","tagName":"h2"},{"title":"How to persist incremental results​","type":1,"pageTitle":"4.2 Structured Streaming (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.2 Structured Streaming Hands On#how-to-persist-incremental-results","content":"  In order to persist incremental results, we need first to pass our logic back to PySpark DataFrame API We are creating another temporary view Since we are creating this temporary view from the result of a query against a streaming temporary view info CREATE OR REPLACE TEMP VIEW author_counts_tmp_vw AS ( SELECT author, COUNT(book_id) AS total_books FROM books_streaming_tmp_vw GROUP BY author ) In PySpark DataFrame API, we can use the spark.table() to load data from a streaming temporary view back to a DataFrame note Spark always load streaming views as a streaming DataFrames, and static views as a static DataFrames, meaning that incremental processing must be defined from the very beginning with Read logic to support later an incremental writing We are using DataFrame writeStream method to persist the result of a streaming query to a durable storage This allows us to configure the output with the three settings   The trigger intervals, here every 4 seconds. info spark.table( &quot;author_counts_tmp_vw&quot; ).writeStream.trigger( processingTime='4 seconds' ).outputMode( 'complete' ).option( 'checkpointLocation', 'dbfs:/mnt/demo/author_counts_checkpoint' ).table( 'author_counts' ) The output mode, either append or complete. warning For aggregation streaming queries, we must always use complete mode to overwrite the table with the new calculation info spark.table( &quot;author_counts_tmp_vw&quot; ).writeStream.trigger( processingTime='4 seconds' ).outputMode( 'complete' ).option( 'checkpointLocation', 'dbfs:/mnt/demo/author_counts_checkpoint' ).table( 'author_counts' ) The checkpointLocation to help tracking the progress of the streaming processing info spark.table( &quot;author_counts_tmp_vw&quot; ).writeStream.trigger( processingTime='4 seconds' ).outputMode( 'complete' ).option( 'checkpointLocation', 'dbfs:/mnt/demo/author_counts_checkpoint' ).table( 'author_counts' )    You can think about such a streaming query as an always-on incremental query, and we can always explore its interactive dashboard From this dashboard, we can see that the data has been processed and we can now query our target table Our table has been written to the target table, the author_counts table, and we can see that each author has currently only 1 book info SELECT * FROM author_counts warning What you see here is not a streaming query simply because we are querying the table directly. I mean, not as a streaming source through a streaming DataFrame  ","version":"Next","tagName":"h2"},{"title":"What happens when new data arrives?​","type":1,"pageTitle":"4.2 Structured Streaming (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.2 Structured Streaming Hands On#what-happens-when-new-data-arrives","content":"  When we execute a streaming query, the streaming query will continue to update as new data arrives in the source To confirm this, let us add new data to our source table Let us run this query and see what will happen in our streaming info INSERT INTO books VALUES ( &quot;B19&quot;, &quot;Introduction to Modeling and Simulation&quot;, &quot;Mark W. Spong&quot;, &quot;Computer Science&quot;, 25 ), ( &quot;B20&quot;, &quot;Robot Modeling and Control&quot;, &quot;Mark W. Spong&quot;, &quot;Computer Science&quot;, 30 ), ( &quot;B21&quot;, &quot;Turing's Vision: The Birth of Computer Science&quot;, &quot;Chris Bernhardt&quot;, &quot;Computer Science&quot;, 35 ) We can see that there is new data arriving. Let us query our target tabel again to see the updated books counts for each author info SELECT * FROM author_counts Let us come back to our streaming query, and cancel it to see another scenario warning Always remember to cancel any active stream in your notebook, otherwise the stream will be always on and prevents the cluster from auto termination  ","version":"Next","tagName":"h2"},{"title":"How to create a triggered incremental batch with the availableNow trigger​","type":1,"pageTitle":"4.2 Structured Streaming (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.2 Structured Streaming Hands On#how-to-create-a-triggered-incremental-batch-with-the-availablenow-trigger","content":"  For our last scenario, we will add some books for new authors to our source table info INSERT INTO books VALUES ( &quot;B16&quot;, &quot;Hands-On Deep Learning Algorithms with Python&quot;, &quot;Sudharsan Ravichandiran&quot;, &quot;Computer Science&quot;, 25 ), ( &quot;B17&quot;, &quot;Neural Network Methods in Natural Language Processing&quot;, &quot;Yoav Goldberg&quot;, &quot;Computer Science&quot;, 30 ), ( &quot;B18&quot;, &quot;Understanding digital signal processing&quot;, &quot;Richard Lyons&quot;, &quot;Computer Science&quot;, 35 ) Use the availableNow option to modify the trigger method to change our query from an always-on query triggered every 4 seconds to a triggered incremental batch With the availableNow trigger option, the query will process all new available data and stop on its own after execution info spark.table( &quot;author_counts_tmp_vw&quot; ).writeStream.trigger( availableNow=True ).outputMode( &quot;complete&quot; ).option( &quot;checkpointLocation&quot;, &quot;dbfs:/mnt/demo/author_counts_checkpoint&quot; ).table( &quot;author_counts&quot; ).awaitTermination() Use the awaitTermination method to block the execution of any cell in this notebook until the incremental batch's write has succeeded info spark.table( &quot;author_counts_tmp_vw&quot; ).writeStream.trigger( availableNow=True ).outputMode( &quot;complete&quot; ).option( &quot;checkpointLocation&quot;, &quot;dbfs:/mnt/demo/author_counts_checkpoint&quot; ).table( &quot;author_counts&quot; ).awaitTermination() With the availableNow trigger option, the query run in a batch mode It is executed to process all the available data and then stop on its own Let us finally query the target table again to see the updated data info SELECT * FROM author_counts  ","version":"Next","tagName":"h2"},{"title":"1.1 Course Overview","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.1 Course Overview","content":"1.1 Course Overview This course covers 5 broad categories related to the exam topics: Databricks Lakehouse Platform ETL with Spark SQL and Python Incremental Data Processing Production Pipelines Data Governance","keywords":"","version":"Next"},{"title":"1.3 Get started with Community Edition","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.3 Get started with Community Edition","content":"1.3 Get started with Community Edition Go to https://www.databricks.com/try-databricks Fill the registration form Click Get started for free tip Databricks offers a 14-day free trial in your own cloud Sign up for the community edition by clicking on the link. Get started with Community Edition Solve the puzzle in order to prove that you are not a robot Go to your email and click on the received link to validate your email address Set the password of your new Databricks account. Confirm your new password and click Reset password Bookmark the page to access your workspace later tip You can always get to the page by going to the website https://community.cloud.databricks.com","keywords":"","version":"Next"},{"title":"4.4 Auto Loader (Hands On)","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.4 Auto Loader Hands On","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"4.4 Auto Loader (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.4 Auto Loader Hands On#introduction","content":"  In this notebook, we will explore incremental data ingestion from files using Auto Loader We will continue using our bookstore dataset with its 3 tables: Customers; Orders; Books  ","version":"Next","tagName":"h2"},{"title":"Dataset Setup​","type":1,"pageTitle":"4.4 Auto Loader (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.4 Auto Loader Hands On#dataset-setup","content":"  Let us start by running the copy dataset script info %run ../Includes/Copy-Datasets We will ingest the new data from orders received in .parquet Files  ","version":"Next","tagName":"h2"},{"title":"Explore the data source directory​","type":1,"pageTitle":"4.4 Auto Loader (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.4 Auto Loader Hands On#explore-the-data-source-directory","content":"  Let us explore our data source directory info files = dbutils.fs.ls( f&quot;{dataset_bookstore}/orders-raw&quot; ) display(files) Currently we have only one parquet file in this directory  ","version":"Next","tagName":"h2"},{"title":"How to work with the Auto Loader to read the current file​","type":1,"pageTitle":"4.4 Auto Loader (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.4 Auto Loader Hands On#how-to-work-with-the-auto-loader-to-read-the-current-file","content":"  Use Auto Loader to read the current file in this directory and detect new files as they arrive to ingest them into a target table Use the readStream and writeStream methods from Spark Structured Streaming API to work with Auto Loader info spark.readStream.format( &quot;cloudFiles&quot; ).option( &quot;cloudFiles.format&quot;, &quot;parquet&quot; ).option( &quot;cloudFiles.schemaLocation&quot;, &quot;dbfs:/mnt/demo/orders_checkpoint&quot; ).load( f&quot;{dataset_bookstore}/orders_raw&quot; ).writeStream.option( &quot;checkpointLocation&quot;, &quot;dbfs:/mnt/demo/orders_checkpoint&quot; ).table( &quot;order_updates&quot; ) The format here is cloudFiles indicating that this is an Auto Loader stream info spark.readStream.format( &quot;cloudFiles&quot; ).option( &quot;cloudFiles.format&quot;, &quot;parquet&quot; ).option( &quot;cloudFiles.schemaLocation&quot;, &quot;dbfs:/mnt/demo/orders_checkpoint&quot; ).load( f&quot;{dataset_bookstore}/orders_raw&quot; ).writeStream.option( &quot;checkpointLocation&quot;, &quot;dbfs:/mnt/demo/orders_checkpoint&quot; ).table( &quot;order_updates&quot; ) We provide two options cloudFile.format and we describe that we are reading data files of type parquet info spark.readStream.format( &quot;cloudFiles&quot; ).option( &quot;cloudFiles.format&quot;, &quot;parquet&quot; ).option( &quot;cloudFiles.schemaLocation&quot;, &quot;dbfs:/mnt/demo/orders_checkpoint&quot; ).load( f&quot;{dataset_bookstore}/orders_raw&quot; ).writeStream.option( &quot;checkpointLocation&quot;, &quot;dbfs:/mnt/demo/orders_checkpoint&quot; ).table( &quot;order_updates&quot; ) The schemaLocation, a directory in which Auto Loader can store the information of the inferred schema. info spark.readStream.format( &quot;cloudFiles&quot; ).option( &quot;cloudFiles.format&quot;, &quot;parquet&quot; ).option( &quot;cloudFiles.schemaLocation&quot;, &quot;dbfs:/mnt/demo/orders_checkpoint&quot; ).load( f&quot;{dataset_bookstore}/orders_raw&quot; ).writeStream.option( &quot;checkpointLocation&quot;, &quot;dbfs:/mnt/demo/orders_checkpoint&quot; ).table( &quot;order_updates&quot; ) We provide the location of our data source files with the load method info spark.readStream.format( &quot;cloudFiles&quot; ).option( &quot;cloudFiles.format&quot;, &quot;parquet&quot; ).option( &quot;cloudFiles.schemaLocation&quot;, &quot;dbfs:/mnt/demo/orders_checkpoint&quot; ).load( f&quot;{dataset_bookstore}/orders_raw&quot; ).writeStream.option( &quot;checkpointLocation&quot;, &quot;dbfs:/mnt/demo/orders_checkpoint&quot; ).table( &quot;order_updates&quot; ) We are chaining immediately the writeStream to write the data into the target table order_updates` info spark.readStream.format( &quot;cloudFiles&quot; ).option( &quot;cloudFiles.format&quot;, &quot;parquet&quot; ).option( &quot;cloudFiles.schemaLocation&quot;, &quot;dbfs:/mnt/demo/orders_checkpoint&quot; ).load( f&quot;{dataset_bookstore}/orders_raw&quot; ).writeStream.option( &quot;checkpointLocation&quot;, &quot;dbfs:/mnt/demo/orders_checkpoint&quot; ).table( &quot;order_updates&quot; ) We provide the location for storing the checkpoint information which allows Auto Loader to track the ingestion process info spark.readStream.format( &quot;cloudFiles&quot; ).option( &quot;cloudFiles.format&quot;, &quot;parquet&quot; ).option( &quot;cloudFiles.schemaLocation&quot;, &quot;dbfs:/mnt/demo/orders_checkpoint&quot; ).load( f&quot;{dataset_bookstore}/orders_raw&quot; ).writeStream.option( &quot;checkpointLocation&quot;, &quot;dbfs:/mnt/demo/orders_checkpoint&quot; ).table( &quot;order_updates&quot; ) Let us run this command to begin our Auto Loader stream tip Before running the command, notice that we are using the same directory for storing both the schema and the checkpoints The Auto Loader is a streaming query since it uses Spark Structured Streaming to load data incrementally info This query will be continuously active. New data will be processed and loaded into the target table as soon as the new data arrives in the data source. Once the data has been ingested to Delta Lake by Auto Loader, we can interact with it the same way we would with any table The data has been loaded well from our source directory. Let us check how many records we have in our table info SELECT COUNT(*) FROM orders_updates We are going to land the new data files in our source directory. We use this helper function coming with our bookstore dataset to copy new files in our source directory info This allows us to simulate somehow an external system writing data in this directory, and each time we execute the celll, a new file will be landed in our source directory Run the cell twice to add new data files. Be aware that each file has 1000 records Let us list the contents of our source directory again We have two additional files added to the directory. Our Auto Loader stream is still active and can process these new data files The Auto Loader has detected automatically that there are new data files in our source directory and it started processing them Let us run this query again to confirm that the data has been ingested info SELECT COUNT(*) FROM orders_updates Let us finally explore our table history info DESCRIBE HISTORY orders_updates A new table version is indicated for each streaming update. These update events is related to the new batches of data arriving at the source Let us end up by dropping our table and removing the checkpoint location info DROP TABLE orders_updates dbutils.fs.rm( &quot;dbfs:/mnt/demo/orders_checkpoint&quot;, True )  ","version":"Next","tagName":"h2"},{"title":"1.2 What is Databricks?","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.2 What is Databricks","content":"","keywords":"","version":"Next"},{"title":"What is Databricks?​","type":1,"pageTitle":"1.2 What is Databricks?","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.2 What is Databricks#what-is-databricks","content":"  Databricks is a multi-cloud lakehouse platform based on Apache Spark  ","version":"Next","tagName":"h2"},{"title":"What is a lakehouse?​","type":1,"pageTitle":"1.2 What is Databricks?","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.2 What is Databricks#what-is-a-lakehouse","content":"  A lake house is a unified analytics platform that combines the best elements of data lakes and data warehouses. Deliver the openness, flexibility and machine learning support of data lakes With the reliability, strong governance and performance of data warehouses In a lakehouse architecture, you work on engineering, analytics and AI all in one platform.  ","version":"Next","tagName":"h2"},{"title":"Architecture of Databricks Lakehouse​","type":1,"pageTitle":"1.2 What is Databricks?","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.2 What is Databricks#architecture-of-databricks-lakehouse","content":"  The Databricks Lakehouse architecture is divided into 3 layers:  The cloud service The runtime The workspace  ","version":"Next","tagName":"h2"},{"title":"Cloud service​","type":1,"pageTitle":"1.2 What is Databricks?","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.2 What is Databricks#cloud-service","content":"  Databricks is available on the following cloud providers: Microsoft AzureAmazon Web ServicesGoogle Cloud  ","version":"Next","tagName":"h3"},{"title":"Runtime​","type":1,"pageTitle":"1.2 What is Databricks?","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.2 What is Databricks#runtime","content":"  The Databricks runtime is a set of core components like: Apache SparkDelta LakeOther system libraries The infrastructure of the cloud provider is used to provision virtual machines or nodes of a cluster which are pre-installed with the Databricks runtime  ","version":"Next","tagName":"h3"},{"title":"Workspace​","type":1,"pageTitle":"1.2 What is Databricks?","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.2 What is Databricks#workspace","content":"  The Databricks workspace is on top of the cloud service and runtime. The workspace allows you to interactively implement and run your data engineering, analytics and AI workloads.  ","version":"Next","tagName":"h3"},{"title":"How Databricks resources are deployed within your cloud provider​","type":1,"pageTitle":"1.2 What is Databricks?","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.2 What is Databricks#how-databricks-resources-are-deployed-within-your-cloud-provider","content":"  Databricks consists of 2 high-level components:   Control plane Data plane  ","version":"Next","tagName":"h2"},{"title":"Control plane​","type":1,"pageTitle":"1.2 What is Databricks?","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.2 What is Databricks#control-plane","content":"  Control plane components are deployed when you create a Databricks workspace.   The control plane components are services in Databricks such as:  The Databricks UI The Cluster Manager The Workflow Service The Notebooks  ","version":"Next","tagName":"h3"},{"title":"Data plane​","type":1,"pageTitle":"1.2 What is Databricks?","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.2 What is Databricks#data-plane","content":"  If you choose Azure as a cloud provider, a storage account will be deployed in your Azure subscription as part of the Data plane. The storage account will be used as the Databricks File System or DBFS When you set up a Spark cluster, additional virtual machines will be deployed as part of the Data plane tip The compute and storage components will be in your own cloud account. Databricks will provide you with the tools you need to use and control your infrastructure from the Databricks UI  ","version":"Next","tagName":"h3"},{"title":"Supported features of Databricks​","type":1,"pageTitle":"1.2 What is Databricks?","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.2 What is Databricks#supported-features-of-databricks","content":"  Data is distributed and processed in-memory by multiple nodes in a cluster, because Databricks is based on Apache Spark. Databricks supports all the languages that are supported by Spark, which are: Scala Python SQL R Java  Databricks also suppports batch- and stream processing in Spark   Databricks processes all types of data:   StructuredSemi-structuredUnstructured  ","version":"Next","tagName":"h2"},{"title":"Databricks File System (DBFS)​","type":1,"pageTitle":"1.2 What is Databricks?","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.2 What is Databricks#databricks-file-system-dbfs","content":"  Databricks offers native-support of a distributed file system called Databricks File System (DBFS) info File systems are used to persist data and files DBFS comes pre-installed on a cluster in Databricks note DBFS is an abstraction layer that uses the underlying cloud storage to persist data. info If you create a file in your cluster and store it in DBFS. This file is persisted in the underlying cloud storage. Azure storage account Amazon S3 buckets Even after the cluster is terminated, all the data is saved in your cloud storage ","version":"Next","tagName":"h2"},{"title":"1.4 Free trial on Azure","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.4 Free trial on Azure","content":"","keywords":"","version":"Next"},{"title":"How to sign up for a free trial with Databricks on Microsoft Azure​","type":1,"pageTitle":"1.4 Free trial on Azure","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.4 Free trial on Azure#how-to-sign-up-for-a-free-trial-with-databricks-on-microsoft-azure","content":" ","version":"Next","tagName":"h2"},{"title":"How to sign up for Microsoft Azure Free Tier​","type":1,"pageTitle":"1.4 Free trial on Azure","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.4 Free trial on Azure#how-to-sign-up-for-microsoft-azure-free-tier","content":"  If you have a cloud account, you can use it to register for a 14-day Databricks full trial info A full trial in your cloud is recommended since its included production-grade functionalities that are not available in the limited community edition [] Microsoft offers a free tier for 12 months. This free tier allows you to explore and try out Azure services free of charge and this up to specified limits for each service Go to azure.microsoft.com/free  ","version":"Next","tagName":"h3"},{"title":"How to deploy Azure Databricks​","type":1,"pageTitle":"1.4 Free trial on Azure","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.4 Free trial on Azure#how-to-deploy-azure-databricks","content":"  You can create a Databricks workspace directly through Azure Portal Search for Databricks and choose from services Azure Databricks Click on the Create button Choose your subscription Create a resource group with the name DatabricksDemoRG since all resources needs to be within a resource group Add a friendly name for the workspace. Let us keep it as DemoWorkspace Select a location. Choose West Europe For the pricing tier, there are two main options Premium tier includes all features of standard tier as well as role-based access control Free trial Choose the 14-day free Premium option Click Review + Create Click Create  ","version":"Next","tagName":"h3"},{"title":"Review deployed resources in Azure Portal​","type":1,"pageTitle":"1.4 Free trial on Azure","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.4 Free trial on Azure#review-deployed-resources-in-azure-portal","content":"  Click on Go to Resource Before launching the workspace, take a look at the managed resource group. This resource group has been created along with the workspace in our subscription Navigate to the resource group to see the following 3 resources that are part of the data plane: A virtual network A network security group for managing the inbound and outbound trafficA storage account which is the underlying storage for DBFS Go back to our resource and click Launch workspace note Notice here that it is using as your Active Directory single sign on to login to Databricks platform Our current data project is building data pipelines, ETL and streaming Click Finish  ","version":"Next","tagName":"h3"},{"title":"1.5 Exploring Workspace","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.5 Exploring Workspace","content":"1.5 Exploring Workspace Go to your workspace UI If you hover over the left sidebar, it will be expanded. You can click menu options in order to keep it expanded Tab name\tDescriptionRepos\tgit integration Data\tTo manage the databases and tables Compute\tTo create and manage the clusters Workflows\tTo deploy and orchestrate the jobs The New button is a shortcut that allows you to quickly create different resources like: Notebooks;Clusters;Jobs On the top bar, you can see the cloud provider on which this workspace is deployed note In our case, it's Microsoft Azure You have the search bar allowing you to search for notebooks, files, dashboards and more Go to the Workspace tab. From here you can see options for creating a notebook, library folder, or MLflow experiment You can also import some code files. And you can even export all the files in the workspace. From the workspace, go to Create and choose Folder Give the folder a name In a folder, you can also create other folders or any other asset Databricks provides different working environments for machine learning engineers and for data analysts You can access these personas simply by clicking on the drop menu here warning Notice that the community edition does not have the SQL persona. This is why it is recommended to use the full trial in your cloud instead of the community edition","keywords":"","version":"Next"},{"title":"1.7 Creating a Cluster","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.7 Creating Cluster","content":"","keywords":"","version":"Next"},{"title":"Walkthrough - How to setup a single-node cluster​","type":1,"pageTitle":"1.7 Creating a Cluster","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.7 Creating Cluster#walkthrough---how-to-setup-a-single-node-cluster","content":"  The next step in setting up your environment is to create a cluster.   ","version":"Next","tagName":"h2"},{"title":"Navigate to the Compute menu​","type":1,"pageTitle":"1.7 Creating a Cluster","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.7 Creating Cluster#navigate-to-the-compute-menu","content":"  Navigate to Compute in the left side bar From here, we can create and manage our clusters info Remember, a cluster is a set of nodes or computers working together like a single entity. It consists of a master node called the Driver and some other worker nodes. The Driver node is responsible for coordinating the workers and their parallel execution of tasks Under all-purpose compute, click Create compute Click on the default name in order to change it. Name your cluster as Demo Cluster, for example Leave the policy Unrestricted to create fully configurable cluster Your cluster could be multi-node, that is having multiple workers, or simply a single node info A single node has no workers and run Spark jobs on the driver node For this course, a single node is enough. However, let us continue to see how to configure a multi-node cluster  ","version":"Next","tagName":"h3"},{"title":"Overview of multi-node cluster configuration options​","type":1,"pageTitle":"1.7 Creating a Cluster","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.7 Creating Cluster#overview-of-multi-node-cluster-configuration-options","content":"  For the access mode, you can allow your cluster to be shared by multiple users info However, only SQL and Python workloads will be supported Choose Single user if you are the only one to use this cluster  ","version":"Next","tagName":"h2"},{"title":"Select the Databricks Runtime version (VM image)​","type":1,"pageTitle":"1.7 Creating a Cluster","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.7 Creating Cluster#select-the-databricks-runtime-version-vm-image","content":"  You need to select the Databricks runtime version info Databricks runtime is the virtual machine's image that comes with pre-installed libraries, which has a specific version of Spark, Scala and other libraries Choose 11.3 LTS, which is the latest version at the time of recording this course note You can choose to activate Photon, which is a vectorized query engine developed in C++ to enhance Spark performance  ","version":"Next","tagName":"h3"},{"title":"Specify the configuration of the worker nodes​","type":1,"pageTitle":"1.7 Creating a Cluster","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.7 Creating Cluster#specify-the-configuration-of-the-worker-nodes","content":" Specify the requirements for the worker nodes​   You can go ahead and select the configuration of your worker nodes info These are different virtual machines sizes provided by your cloud provider Depending on your requirements of memory cores and hard disk, you can select the configuration Let's keep the default one   Specify the number of worker nodes​   You can select the number of workers you need for your cluster   ","version":"Next","tagName":"h3"},{"title":"Enable auto scaling​","type":1,"pageTitle":"1.7 Creating a Cluster","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.7 Creating Cluster#enable-auto-scaling","content":"  If you choose to enable auto scaling, then provide a range for the number of workers info These allow Databricks to resize your cluster automatically within this range Otherwise disable auto scaling and use a fixed number of workers Let me select 3 here   ","version":"Next","tagName":"h3"},{"title":"Specify the configuration for the driver node​","type":1,"pageTitle":"1.7 Creating a Cluster","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.7 Creating Cluster#specify-the-configuration-for-the-driver-node","content":"  You can now select the configuration for the Driver node or simply keep it the same as the worker   ","version":"Next","tagName":"h3"},{"title":"Enable auto-termination​","type":1,"pageTitle":"1.7 Creating a Cluster","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.7 Creating Cluster#enable-auto-termination","content":"  You can enable auto-termination of the cluster by providing the number of minutes Let's say 30 minutes. That is, if there is no activity for 30 minutes, the cluster will auto-terminate   ","version":"Next","tagName":"h3"},{"title":"Check the cluster configuration summary​","type":1,"pageTitle":"1.7 Creating a Cluster","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.7 Creating Cluster#check-the-cluster-configuration-summary","content":"  On the right, you can see a summary of your cluster configuration. Here you can see the number of DBUs info DBU stand for Databricks Unit and it is a unit of a processing capability per hour note Each configuration tells you how much DBUs would be consumed if a virtual machine runs for an hour and then you pay for each DBU consumed info For example, if we have less number of workers or even a single node cluster, we will have less DBUs  ","version":"Next","tagName":"h3"},{"title":"Walkthrough - How to setup a single-node cluster [Continued]​","type":1,"pageTitle":"1.7 Creating a Cluster","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.7 Creating Cluster#walkthrough---how-to-setup-a-single-node-cluster-continued","content":"  Select Single Node cluster Click Confirm info With a single node cluster, we are going to consume less DBUs Hit the Create button to finish creating our cluster info Azure will now go ahead and provision the required virtual machine with a specific configuration and libraries as specified by Databricks runtime Our cluster is now up and running To access your cluster at any time, you can simply navigate to Compute in the left side bar You can see your clusters are listed here with its current status Running or Terminated. You can even edit the cluster by clicking on its name. Then click Edit info Remember, changing the cluster configuration may require a restart of the cluster Let's cancel this In the cluster page, you can notice two things  The event log shows all the events that have happened with the cluster info For example, when the cluster was created or terminated. This helps to track the activity on a cluster In the driver log, you will get the log generated within the cluster notebooks and libraries    Let's now terminate our cluster ","version":"Next","tagName":"h2"},{"title":"1.6 Course Materials","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.6 Course Materials","content":"","keywords":"","version":"Next"},{"title":"Intro​","type":1,"pageTitle":"1.6 Course Materials","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.6 Course Materials#intro","content":"  We are going to see how to import the notebooks of this course into a Databricks workspace   ","version":"Next","tagName":"h2"},{"title":"Source code location​","type":1,"pageTitle":"1.6 Course Materials","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.6 Course Materials#source-code-location","content":"  The source code is available here on GitHub info https://github.com/derar-alhussein/Databricks-Certified-Data-Engineer-Associate  ","version":"Next","tagName":"h2"},{"title":"How to use the source code in Databricks​","type":1,"pageTitle":"1.6 Course Materials","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.6 Course Materials#how-to-use-the-source-code-in-databricks","content":"  We can easily clone this repository in a Databricks workspace using Databricks Repo Let us copy the repository URL, and switch to our Databricks workspace In our Databricks workspace, we go to the Repos tab in the left side bar info Databricks Repos provides source control for your data projects by integrating with Git providers Click on Add Repo Paste in the URL of the Git repository note The git provider will be filled automatically and also the repository name Click Submit info With this source code, you can follow along with me during this course and recreate the same solution in your Databricks workspace ","version":"Next","tagName":"h2"},{"title":"1.8 Notebook Fundamentals","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.8 Notebook Fundamentals","content":"","keywords":"","version":"Next"},{"title":"Access the filesystem with dbutils​","type":1,"pageTitle":"1.8 Notebook Fundamentals","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.8 Notebook Fundamentals#access-the-filesystem-with-dbutils","content":"  Another way to deal with filesystem operations to use Databricks utilities, also known as dbutils info dbutils provides a number of utility commands for configuring and interacting with the environment note To get some help for each utility, you can use dbutils.help() function As you can see with the dbutils, you can interact with different services and tools like: credentials, fs for file system, secrets and widgets info dbutils.fs allows you to interact with the Databricks file system or DBFS To get some help on this. info dbutils.fs.help() From here we can see that the fs has a number of available commands, like copying and removing files In addition, we also have the ls command that allows us to list the content of a directory. Let us try this note Databricks also supports auto-completion using the Tab key Let us hit Tab in this case and select ls, click enter. And specify the folder path, in our case /databricks-datasets info dbutils.fs.ls('/databricks-datasets') The question for the file system operation, should we use dbutils or the %fs magic command we saw previously? In fact, dbutils is more useful than the %fs magic command since you can use dbutils as part of python code info You can get the list of files for example in a variable and do some logic with it in Python. &quot;Example: List files in directory&quot; files = dbutils.fs.ls('/databricks-datasets') As you can see, the output is very messy and hard to read. Instead, you can display it in a much better way using the display function info files = dbutils.fs.ls('/databricks-datasets') display(files) Here we see the output is rendered in a tabular format with some fields like: path;filename;size;modification time With the display function, you can also download data as CSV file and render the result as a graph info However, the display function is limited to preview only 1000 records note When running SQL queries from cells, results will be always displayed in a tabular format like here, so there is no need to use the display function with SQL. At the end of the day, you may want to download your notebook. You can just click on the file menu, then Export and you choose IPython Notebook We can also export a folder that contains multiple notebooks from the Workspace tab. If we click next to the folder, we have the option to export it as DBC archive note The DBC or the Databricks Cloud file is a zip file that contains a collection of directories and notebooks This file can be uploaded into any Databricks workspace to move or share notebooks To import a notebook or a collection of notebooks in DBC file, simply click here on the down arrow and choose Import In Databricks, you can access the revision history of all the changes being made on a notebook Click on the Last Edit link. From here, you can choose any of the auto-saved versions Click Restore this revision  ","version":"Next","tagName":"h2"},{"title":"4.6 Multihop Architecture (Hands On)","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.6 Multihop Architecture Hands On","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"4.6 Multihop Architecture (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.6 Multihop Architecture Hands On#introduction","content":"  In this notebook, we will create a Delta Lake multi-hop pipeline We will continue using our bookstore dataset, with its 3 tables: Customers; Orders; Books  ","version":"Next","tagName":"h2"},{"title":"Dataset setup​","type":1,"pageTitle":"4.6 Multihop Architecture (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.6 Multihop Architecture Hands On#dataset-setup","content":"  Let us start by running the Copy-Datasets script info %run ../Includes/Copy-Datasts   ","version":"Next","tagName":"h2"},{"title":"Explore source directory​","type":1,"pageTitle":"4.6 Multihop Architecture (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.6 Multihop Architecture Hands On#explore-source-directory","content":"  Before starting, let us first check our source directory info files = dbutils.fs.ls( f&quot;{dataset_bookstore}/orders-raw&quot; ) display(files) Currently we have 3 .parquet files in our source directory   ","version":"Next","tagName":"h2"},{"title":"How to implement the bronze layer​","type":1,"pageTitle":"4.6 Multihop Architecture (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.6 Multihop Architecture Hands On#how-to-implement-the-bronze-layer","content":"  We start usually by creating an Auto Loader against our source directory Here, we are configuring a readStream on our parquet source using Auto Loader with schema inference info spark.readStream.format( &quot;cloudFiles&quot; ).option( &quot;cloudFiles.format&quot;, &quot;parquet&quot; ).option( &quot;cloudFiles.schemaLocation&quot;, &quot;dbfs:/mnt/demo/checkpoints/orders_raw&quot; ).load( f&quot;{dataset_bookstore}/orders-raw&quot; ).createOrReplaceTempView( &quot;orders_raw_temp&quot; ) We register a streaming temporary view to do data transformation in Spark SQL Our temporary view is named orders_raw_temp info spark.readStream.format( &quot;cloudFiles&quot; ).option( &quot;cloudFiles.format&quot;, &quot;parquet&quot; ).option( &quot;cloudFiles.schemaLocation&quot;, &quot;dbfs:/mnt/demo/checkpoints/orders_raw&quot; ).load( f&quot;{dataset_bookstore}/orders-raw&quot; ).createOrReplaceTempView( &quot;orders_raw_temp&quot; ) Our stream has been created, but notice that it is not active yet until we do a display or writeStream operation We will enrich our raw data with additional metadata describing the source file and the time it was ingested info Such information is useful for troubleshooting errors if corrupted data is encountered. CREATE OR REPLACE TEMPORARY VIEW orders_tmp AS ( SELECT *, current_timestamp(), input_file_name() AS source_file FROM orders_raw_temp ) Let us take a look on our enriched raw data info SELECT * FROM orders_tmp Our stream is active now, and we can see that we have successfully inserted the data with the metadata of the arrival time and the source file Let us cancel this stream for now We are going to pass this enriched data back to PySpark API to process an incremental write to a Delta Lake table called orders_bronze info spark.table( &quot;orders_tmp&quot; ).writeStream.format( &quot;delta&quot; ).option( &quot;checkpointLocation&quot;, &quot;dbfs:/mnt/demo/checkpoints/orders_bronze&quot; ).outputMode( &quot;append&quot; ).table(&quot;orders_bronze&quot;) Our writeStream is active now, and we can see that it has started processing our data Let us check how many records were written into the bronze table info SELECT COUNT(*) FROM orders_bronze 3000 records have been written corresponding to our three files, where each file contain 1000 records We can trigger another file arrival using the following function info load_new_data() Let us come back to our active stream and see what is happening. We see that the new data is immediately detected by the streaming query We can confirm this by querying the number of records again in our table  ","version":"Next","tagName":"h2"},{"title":"How to implement the silver layer​","type":1,"pageTitle":"4.6 Multihop Architecture (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.6 Multihop Architecture Hands On#how-to-implement-the-silver-layer","content":"  We can move on to work on our second hop, the silver layer  ","version":"Next","tagName":"h2"},{"title":"Create a new temporary view from JSON files​","type":1,"pageTitle":"4.6 Multihop Architecture (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.6 Multihop Architecture Hands On#create-a-new-temporary-view-from-json-files","content":"  Here we are creating a customers static temporary view from JSON files We are doing it with PySpark API, but we can also do it with Spark SQL info spark.read.format( &quot;json&quot; ).load( f&quot;{dataset_bookstore}/customers-json&quot; ).createOrReplaceTempView( &quot;customers_lookup&quot; ) Let us take a look at our customers_lookup temporary view info SELECT * FROM customers_lookup Here we have 3 columns: The customer id that we will use it for joining the data; The customer email; The profile information as a complex JSON object   ","version":"Next","tagName":"h3"},{"title":"Create a streaming temporary view from an existing table​","type":1,"pageTitle":"4.6 Multihop Architecture (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.6 Multihop Architecture Hands On#create-a-streaming-temporary-view-from-an-existing-table","content":"  To work on our bronze data in the silver layer, we will start by creating a streaming temporary view against our bronze table info spark.readStream.table( &quot;orders_bronze&quot; ).createOrReplaceTempView( &quot;orders_bronze_tmp&quot; )   ","version":"Next","tagName":"h3"},{"title":"Combine orders with customers data​","type":1,"pageTitle":"4.6 Multihop Architecture (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.6 Multihop Architecture Hands On#combine-orders-with-customers-data","content":"  In this silver level, we are doing several enrichments and checks We join the order data with the customers information to add customers names info CREATE OR REPLACE TEMPORARY VIEW orders_enriched_tmp AS ( SELECT order_id, quantity, o.customer_id, c.profile:first_name AS f_name, c.profile:last_name AS l_name, CAST( from_unixtime( order_timestamp, 'yyyy-MM-dd HH:mm:ss' ) AS timestamp ) AS order_timestamp, books FROM orders_bronze_tmp AS o INNER JOIN customers_lookup c ON o.customer_id = c.customer_id WHERE quantity &gt; 0 )   ","version":"Next","tagName":"h3"},{"title":"Parse Unix timestamp into human-readable format​","type":1,"pageTitle":"4.6 Multihop Architecture (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.6 Multihop Architecture Hands On#parse-unix-timestamp-into-human-readable-format","content":"  We parse the order_timestamp from Unix timestamp into human readable format info CREATE OR REPLACE TEMPORARY VIEW orders_enriched_tmp AS ( SELECT order_id, quantity, o.customer_id, c.profile:first_name AS f_name, c.profile:last_name AS l_name, CAST( from_unixtime( order_timestamp, 'yyyy-MM-dd HH:mm:ss' ) AS timestamp ) AS order_timestamp, books FROM orders_bronze_tmp AS o INNER JOIN customers_lookup c ON o.customer_id = c.customer_id WHERE quantity &gt; 0 )   ","version":"Next","tagName":"h3"},{"title":"Exclude orders without items​","type":1,"pageTitle":"4.6 Multihop Architecture (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.6 Multihop Architecture Hands On#exclude-orders-without-items","content":"  We exclude any order with no items info CREATE OR REPLACE TEMPORARY VIEW orders_enriched_tmp AS ( SELECT order_id, quantity, o.customer_id, c.profile:first_name AS f_name, c.profile:last_name AS l_name, CAST( from_unixtime( order_timestamp, 'yyyy-MM-dd HH:mm:ss' ) AS timestamp ) AS order_timestamp, books FROM orders_bronze_tmp AS o INNER JOIN customers_lookup c ON o.customer_id = c.customer_id WHERE quantity &gt; 0 )   ","version":"Next","tagName":"h3"},{"title":"Create a writeStream to persist data into a silver table​","type":1,"pageTitle":"4.6 Multihop Architecture (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.6 Multihop Architecture Hands On#create-a-writestream-to-persist-data-into-a-silver-table","content":"  Let us do a writeStream for this orders_enriched_tmp data into a silver table info spark.table( &quot;orders_enriched_tmp&quot; ).writeStream.format( &quot;delta&quot; ).option( &quot;checkpointLocation&quot;, &quot;dbfs:/mnt/demo/checkpoints/orders_silver&quot; ).outputMode( &quot;append&quot; ).table( &quot;orders_silver&quot; ) The data has been processed with the stream. Let's query our silver table info SELECT * FROM orders_silver Let us now check how many records do we have? info SELECT COUNT(*) FROM orders_silver Let us trigger another new file and wait for it to propagate through the previous two streams, from bronze to silver layer info load_new_data() Let us see our stream dashboard. The new data has been received. Let us check the counts in the silver table info SELECT COUNT(*) FROM orders_silver   ","version":"Next","tagName":"h3"},{"title":"How to implement the gold layer​","type":1,"pageTitle":"4.6 Multihop Architecture (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.6 Multihop Architecture Hands On#how-to-implement-the-gold-layer","content":"  Let us now work on the gold layer We need a stream of data from the silver table into a streaming temporary view info spark.readStream.table( &quot;orders_silver&quot; ).createOrReplaceTempView( &quot;orders_silver_tmp&quot; )   ","version":"Next","tagName":"h2"},{"title":"How to aggregate data from a streaming data source​","type":1,"pageTitle":"4.6 Multihop Architecture (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.6 Multihop Architecture Hands On#how-to-aggregate-data-from-a-streaming-data-source","content":"  We write another stream to create an aggregate gold table for the daily number of books for each customer info CREATE OR REPLACE TEMP VIEW daily_customer_books_tmp AS ( SELECT customer_id, f_name, l_name, date_trunc( &quot;DD&quot;, order_timestamp ) AS order_date, SUM(quantity) AS books_counts FROM orders_silver_tmp GROUP BY customer_id, f_name, l_name, date_trunc( &quot;DD&quot;, order_timestamp ) )   ","version":"Next","tagName":"h3"},{"title":"How to persist aggregated data into the gold table​","type":1,"pageTitle":"4.6 Multihop Architecture (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.6 Multihop Architecture Hands On#how-to-persist-aggregated-data-into-the-gold-table","content":"  Let us write this aggregated data into a gold table called daily_customer_books info spark.table( &quot;daily_customer_books_tmp&quot; ).writeStream.format( &quot;delta&quot; ).outputMode( &quot;complete&quot; ).option( &quot;checkpointLocation&quot;, &quot;dbfs:/mnt/demo/checkpoints/daily_customer_books&quot; ).trigger( availableNow=True ).table( &quot;daily_customer_books&quot; ) note The stream stopped on its own after processing all the available data in micro batches. This is because we are using trigger availableNow option With this way, we can combine streaming and batch workloads in the same pipeline We are also using the complete output mode to rewrite the updated aggregation each time our logic runs info Keep in mind that Structured Streaming assumes data is only being appended in the upstream tables. Once a table is updated or overwritten, it is no longer valid for streaming note In our case here, we cannot read a stream from this gold table To change this behavior, you can set options like ignoreChanges, but they have other limitations Let us now query the gold table info SELECT * FROM daily_customer_books Here we can see our aggregated data. Customers currently have books counts between 5 and 10 Let us finally land all the remaining data files in our source directory info load_new_data(all=True) The data will be propagated from our source directory into the bronze, silver layer, until the gold layer For the gold layer, we need to rerun our final query to update the gold table. Since the query is configured as a batch job using the trigger availableNow syntax Let us rerun this stream query info spark.table( &quot;daily_customer_books_tmp&quot; ).writeStream.format( &quot;delta&quot; ).outputMode( &quot;complete&quot; ).option( &quot;checkpointLocation&quot;, &quot;dbfs:/mnt/demo/checkpoints/daily_customer_books&quot; ).trigger( availableNow=True ).table( &quot;daily_customer_books&quot; ) All the available data has been processed. Let us confirm this by re-query our gold table info SELECT * FROM daily_customer_books Now we can see customers having more books counts after processing the new changes  ","version":"Next","tagName":"h3"},{"title":"Cleanup​","type":1,"pageTitle":"4.6 Multihop Architecture (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.6 Multihop Architecture Hands On#cleanup","content":"  Let us end up by stopping all the active streams by running this for loop info for stream in spark.streams.active: print(&quot;Stopping stream: &quot; + stream.id) stream.stop() stream.awaitTermination()  ","version":"Next","tagName":"h2"},{"title":"1.9 Databricks Repos","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.9 Databricks Repos","content":"","keywords":"","version":"Next"},{"title":"How to locate settings in Databricks to setup git integration?​","type":1,"pageTitle":"1.9 Databricks Repos","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.9 Databricks Repos#how-to-locate-settings-in-databricks-to-setup-git-integration","content":"  Under your username in the top bar, choose User settings from the menu Go to the git integration tab note As you can see from the git provider dropdown menu, Databricks has native integration with several git providers like GitHub and Azure DevOps To connect Databricks to one of the git providers, you need your git service username and a personal access token info In this demo we will use GitHub, so let us switch there to get this information  ","version":"Next","tagName":"h2"},{"title":"How to locate the personal access token in GitHub?​","type":1,"pageTitle":"1.9 Databricks Repos","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.9 Databricks Repos#how-to-locate-the-personal-access-token-in-github","content":"  In GitHub, click on your avatar in the top bar info Here it displays the username for the account For the token, let us select Settings Scroll to Developer settings. From the left choose Personal Access Token Choose Tokens (classic) Click generate a new token, classic token In the new personal access token page, specify a note describing the purpose of the token and an expiration date Choose Repo scope and scroll to click Generate token Here is our token. Let us copy this string now  ","version":"Next","tagName":"h2"},{"title":"How to connect Databricks to GitHub with the username and personal access token?​","type":1,"pageTitle":"1.9 Databricks Repos","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.9 Databricks Repos#how-to-connect-databricks-to-github-with-the-username-and-personal-access-token","content":"  Let us switch back to our Databricks workspace With GitHub selected as GitHub provider. Let us fill in our GitHub username and personal token Click Save  ","version":"Next","tagName":"h2"},{"title":"How to create a new repository in GitHub?​","type":1,"pageTitle":"1.9 Databricks Repos","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.9 Databricks Repos#how-to-create-a-new-repository-in-github","content":"  Each Databricks repo maps to a Git repository In order to set up a repo, let's first create a repository in GitHub Go to the main GitHub landing page and click New to create a repository Specify a name for the repository. Choose Private. And enable Add a README file Click Create repository Copy the HTTPS URL of this newly created repository Switch back to our Databricks workspace From the left side bar, go to the Repo tab. Click Add Repo Paste here the URL of the Git repository info Notice that the git provider and the repository name has been filled automatically Click Submit Here is our new repo, which contains a local copy of the remote git repository  ","version":"Next","tagName":"h2"},{"title":"How to create a new branch in Databricks?​","type":1,"pageTitle":"1.9 Databricks Repos","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.9 Databricks Repos#how-to-create-a-new-branch-in-databricks","content":"  Let us create a new branch named development Click on the branch name to open the Repo dialog Click Create branch Specify the branch name Click Create note Once created it automatically becomes the current branch With the development branch created and selected, we can now begin making changes as needed Let us create a folder my folder, for example Add a notebook You can also import a notebook. From file or URL  ","version":"Next","tagName":"h2"},{"title":"How to clone an existing notebook from the Workspace?​","type":1,"pageTitle":"1.9 Databricks Repos","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.9 Databricks Repos#how-to-clone-an-existing-notebook-from-the-workspace","content":"  Or clone an existing notebook from the workspace into the repo Go to the Workspace tab Click Next to our Notebook Basics created in the last lecture and select Clone Choose my folder and click Clone If we go back to Databricks Repo, we see that our new folder containing two notebooks Let us now push these changes to our remote repository Click on the branch name info Here we see all our changes Let us first write a commit message Click Commit &amp; push If we switch back now our main branch, as expected, we don't see here the new folder and notebooks we created in the development branch  ","version":"Next","tagName":"h2"},{"title":"How to create a pull request in GitHub?​","type":1,"pageTitle":"1.9 Databricks Repos","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.9 Databricks Repos#how-to-create-a-pull-request-in-github","content":"  To be able to pull these changes into the main branch, let us create a pull request in GitHub From here we go to the development branch Click Contribute, Open pull request Click Create pull request. Then Merge pull request. Confirm merge  ","version":"Next","tagName":"h2"},{"title":"How to pull a branch from GitHub to Databricks?​","type":1,"pageTitle":"1.9 Databricks Repos","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.9 Databricks Repos#how-to-pull-a-branch-from-github-to-databricks","content":"  Let us switch back to our Databricks Workspace to see how to pull this in Databricks repos Click on the branch name to open the repos dialog With the main branch selected, click the Pull button on the right info Remember pulling regularly is important to avoid conflicts especially when multiple developers are developing on the same branch ","version":"Next","tagName":"h2"},{"title":"5.2 Change Data Capture","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.2 Change Data Capture","content":"","keywords":"","version":"Next"},{"title":"Change Data Capture​","type":1,"pageTitle":"5.2 Change Data Capture","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.2 Change Data Capture#change-data-capture","content":"  Change Data Capture or CDC refers to the process of identifying and capturing changes made to data in the data source, and then delivering those changes to the target  ","version":"Next","tagName":"h2"},{"title":"Row-level changes​","type":1,"pageTitle":"5.2 Change Data Capture","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.2 Change Data Capture#row-level-changes","content":"  Those changes could be obviously new records to be inserted from the source to the target Updated records in the source that need to be reflected in the target Deleted records in the source that must be deleted in the target  ","version":"Next","tagName":"h3"},{"title":"CDC Feed​","type":1,"pageTitle":"5.2 Change Data Capture","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.2 Change Data Capture#cdc-feed","content":"  Changes are logged at the source as events that contain both the data of the records along with metadata information info This metadata indicates whether the specified record was inserted, updated or deleted in addition to a version number or timestamp indicating the order in which changes happened Here's an example of CDC events need to be applied on our target table info France, for example has two records, so we need to apply the most recent change. Canada need to be deleted, so we don't need to send all the data of the record. Lastly, USA and India are new records need to be inserted Country ID\tCountry\tVaccination Rate\tsequence\toperationFR\tFrance\t0.74\t2022-11-01 07:00\tUPDATE FR\tFrance\t0.75\t2022-11-01 08:00\tUPDATE CA 2022-11-01 00:00\tDELETE US\tUSA\t0.5\t2022-11-01 00:00\tINSERT IN\tIndia\t0.66\t2022-11-01 00:00\tINSERT Here we see the changes applied on our target table We don't see the record of Canada as it has been deleted info Such a CDC feed could be received from the source as a data stream or simply in JSON files, for example Country ID\tCountry\tVaccination RateFR\tFrance\t0.75 US\tUSA\t0.5 IN\tIndia\t0.66 Delta Live Tables supports CDC feed processing using the APPLY CHANGES INTO command info APPLY CHANGES INTO LIVE.target_table FROM STREAM( LIVE.cdc_feed_table ) KEYS ( key_field ) APPLY AS DELETE WHEN operation_field = &quot;DELETE&quot; SEQUENCE BY sequence_field COLUMNS * It is the target table into which the changes need to be applied. FROM a CDC feed table specified as a streaming source. KEYS is where you identify the primary key fields. If the key exists in the target table, the record will be updated info APPLY CHANGES INTO LIVE.target_table FROM STREAM( LIVE.cdc_feed_table ) KEYS ( key_field ) APPLY AS DELETE WHEN operation_field = &quot;DELETE&quot; SEQUENCE BY sequence_field COLUMNS * With the APPLY AS DELETE WHEN condition, you specify that records where the operation field is DELETE should be deleted info APPLY CHANGES INTO LIVE.target_table FROM STREAM( LIVE.cdc_feed_table ) KEYS ( key_field ) APPLY AS DELETE WHEN operation_field = &quot;DELETE&quot; SEQUENCE BY sequence_field COLUMNS * SEQUENCE BY, where do you specify the sequence field for ordering how operation should be applied info APPLY CHANGES INTO LIVE.target_table FROM STREAM( LIVE.cdc_feed_table ) KEYS ( key_field ) APPLY AS DELETE WHEN operation_field = &quot;DELETE&quot; SEQUENCE BY sequence_field COLUMNS * You indicate the list of fields that should be added to the target table info APPLY CHANGES INTO LIVE.target_table FROM STREAM( LIVE.cdc_feed_table ) KEYS ( key_field ) APPLY AS DELETE WHEN operation_field = &quot;DELETE&quot; SEQUENCE BY sequence_field COLUMNS * note Note here the target Delta Live Table need to be already created before executing the APPLY CHANGES INTO command  ","version":"Next","tagName":"h2"},{"title":"Features of the APPLY CHANGES INTO command​","type":1,"pageTitle":"5.2 Change Data Capture","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.2 Change Data Capture#features-of-the-apply-changes-into-command","content":"  It automatically orders late arriving records using the user-provided sequencing key info This pattern ensures that if any records arrive out of order, down stream result can be properly re-computed to reflect the updates It also ensures that when records are deleted from a source table, these values are no longer reflected in tables later in the pipeline The default behavior for insert and update operation is to upsert the CDC events into the target table info That means it updates any rows in the target table that match the specified key or insert new records when a matching record does not exist in the target table Optional handling for delete events can be specified with the APPLY AS DELETE WHEN condition You can specify one or many fields as the primary key for a table The EXCEPT keyword can be added to specify columns to ignore You can choose whether to store records as slowly changing dimension, type 1 or type 2 APPLY CHANGES INTO defaults to creating a type 1 slowly changing dimension table, meaning that each unique key will have at most one record, and that the updates would overwrite the original information  ","version":"Next","tagName":"h2"},{"title":"Disadvantage of APPLY CHANGES INTO​","type":1,"pageTitle":"5.2 Change Data Capture","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.2 Change Data Capture#disadvantage-of-apply-changes-into","content":"  Since data is being updated and delete in the target table, this breaks the append-only requirements for streaming table sources info That means we will no longer be able to use this updated table as a streaming source later in the next layer ","version":"Next","tagName":"h2"},{"title":"5.4 Jobs (Hands On)","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.4 Jobs Hands On","content":"","keywords":"","version":"Next"},{"title":"How to orchestrate jobs with Databricks​","type":1,"pageTitle":"5.4 Jobs (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.4 Jobs Hands On#how-to-orchestrate-jobs-with-databricks","content":"  Databricks allows you to schedule one or multiple tasks as part of a job We are going to create a multi-task job consisting of 3 tasks   Executing a notebook that lands a new batch of data in our source directory Running our Delta Live Tables pipeline created last session to process this data through a series of tables Executing the notebook we created in the last session to show the pipeline results    To create such a multi-task job, navigate to the workflow tabs on the sidebar In the Jobs tab, click the Create Job button Set a name for our job info For example, Bookstore Demo Job Then, we can start configuring our first task in this job Fill in a task name say land_new_data For type, select notebook info The notebook could be located in your Databricks workspace or in a repository info In our case, the notebook is in the workspace For path, we are going to select the land_new_data notebook in our workspace Click here to open the notebook info The notebook has nothing but just a call to the load_new_data function From the Cluster dropdown, under existing All-Purpose Clusters, let us select our Demo cluster info For production jobs, we must use job clusters instead of cost saving Click on Create button Now we have a job of a single task Let us add another task for our DLT pipeline to be executed after the success of this first task To do so, click this blue circle with the + sign to add a new task Enter DLT for the task name For type, select Delta Live Tables Pipeline For pipeline, select our demo pipeline we created during our last session The Depends on field defaults to your previously defined task, land_new_data in our case, so leave this value as it is Click now on the Create Task button Now, we see the two tasks and the dependencies between them Let us add a third task for executing a notebook to show the pipeline results Enter Pipeline Results for the task name For type, select Notebook For path, select the notebook pipeline result created in the last session info This notebook just shows the content of the pipeline storage location and query our gold table From the cluster dropdown, select the Demo cluster The Depends On field defaults again to your previously defined task, which is DLT task in our case Click now on the Create Task button We configured now our 3 tasks for this job On the right, we see the schedule section that allows us to schedule our job Click on the Edit Schedule button to explore the scheduling option We can change here the trigger type to: Scheduled. And configure the schedule for this job info You can edit the CRON syntax as well note For this demo, we will not set any schedule, so let us cancel this window From here, you can also set email notification. So, you can be alerted on the job's start, success and failure In the permissions section, you can control who can run, manage or review the jobs, either a user or a group of users info This also allows to change the owner of the job to another user but of course not to a group of users Let us click the Run Now button to start our job You can see your runs of your job on the job &quot;Runs&quot; tab The running jobs will be under the Active Runs section and the finished jobs will be under the Completed Runs section Let us click on the Start Time link to open the current run of this job The visualization for tasks will update in real time to reflect which tasks are actively running Let us click on the first task to show its results info Here we can see that we landed the parquet file number 7. If we come back to the run output and click on the DLT task DLT pipelines scheduled as tasks do not directly render the results in the runs UI Instead, you can click on this link to be directed back to the DLT pipeline UI to see the results Here's the result of running our DLT pipeline on the new data If we click on the Pipeline Results task, we can see the results of all the cells in our notebook Let us see another scenario where we have some bad codes in this notebook that cause our job to fail Let us query a table that does not exist Let us run our job again If we click on the Pipeline Results task, we see the Table Not Found error Let us correct this error and see how we can fix our job If we come back to our job, and click on the failed run. We can see that we have the Repair Run button info This is a great option that will allow us to rerun only the failed tasks Let us click this button Here, it shows the task to be rerun info In our case, it's only the pipeline results task Let us click now Repair Run  ","version":"Next","tagName":"h2"},{"title":"5.3 Processing CDC Feed with DLT (Hands On)","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.3 Processing CDC Feed with DLT Hands On","content":"","keywords":"","version":"Next"},{"title":"Creating the bronze table layer​","type":1,"pageTitle":"5.3 Processing CDC Feed with DLT (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.3 Processing CDC Feed with DLT Hands On#creating-the-bronze-table-layer","content":"  In this notebook, we start by creating a bronze table to ingest books CDC feed We are using auto loader to load the JSON files incrementally info CREATE OR REFRESH STREAMING LIVE TABLE books_bronze COMMENT &quot;The raw books data, ingested from CDC feed&quot; AS SELECT * FROM cloud_files( &quot;${datasets.path}/books-cdc&quot;, &quot;json&quot; )   ","version":"Next","tagName":"h2"},{"title":"Creating the silver table layer​","type":1,"pageTitle":"5.3 Processing CDC Feed with DLT (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.3 Processing CDC Feed with DLT Hands On#creating-the-silver-table-layer","content":"  We are creating the silver table info This is our target table into which the changes from the CDC feed will be applied We start by declaring the table. Since APPLY CHANGES INTO requires the target table to be declared in a separate statement With a target table created, we can write the APPLY CHANGES INTO command In this command, we specify the table books_silver as the target table The table books_bronze as the streaming source of our CDC feed info CREATE OR REFRESH STREAMING LIVE TABLE books_silver; APPLY CHANGES INTO LIVE.books_silver FROM STREAM(LIVE.books_bronze) KEYS (book_id) APPLY AS DELETE WHEN row_status = &quot;DELETE&quot; SEQUENCE BY row_time COLUMNS * EXCEPT ( row_status, row_time ) Then we identify the book_id as the primary key If the key exists in the target table, the record will be updated. If not it will be inserted info CREATE OR REFRESH STREAMING LIVE TABLE books_silver; APPLY CHANGES INTO LIVE.books_silver FROM STREAM(LIVE.books_bronze) KEYS (book_id) APPLY AS DELETE WHEN row_status = &quot;DELETE&quot; SEQUENCE BY row_time COLUMNS * EXCEPT ( row_status, row_time ) We specify that records where the row status is DELETE should be deleted from the target table info CREATE OR REFRESH STREAMING LIVE TABLE books_silver; APPLY CHANGES INTO LIVE.books_silver FROM STREAM(LIVE.books_bronze) KEYS (book_id) APPLY AS DELETE WHEN row_status = &quot;DELETE&quot; SEQUENCE BY row_time COLUMNS * EXCEPT ( row_status, row_time ) We specify the row_time field for ordering the operations info CREATE OR REFRESH STREAMING LIVE TABLE books_silver; APPLY CHANGES INTO LIVE.books_silver FROM STREAM(LIVE.books_bronze) KEYS (book_id) APPLY AS DELETE WHEN row_status = &quot;DELETE&quot; SEQUENCE BY row_time COLUMNS * EXCEPT ( row_status, row_time ) We indicate that all books fields should be added to the target table except the operational columns: row_status and row_time info CREATE OR REFRESH STREAMING LIVE TABLE books_silver; APPLY CHANGES INTO LIVE.books_silver FROM STREAM(LIVE.books_bronze) KEYS (book_id) APPLY AS DELETE WHEN row_status = &quot;DELETE&quot; SEQUENCE BY row_time COLUMNS * EXCEPT ( row_status, row_time )   ","version":"Next","tagName":"h2"},{"title":"Creating the gold table layer​","type":1,"pageTitle":"5.3 Processing CDC Feed with DLT (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.3 Processing CDC Feed with DLT Hands On#creating-the-gold-table-layer","content":"  In the gold layer, we define a simple aggregate query to create a live table from the data in our books_silver table note Notice here that this is not a streaming table Since data is being updated and deleted from our books_silver table, it is no more valid to be a streaming source for this new table info Remember, streaming sources must be append-only tables CREATE LIVE TABLE author_counts_state COMMENT &quot;Number of books per author&quot; AS SELECT author, COUNT(*) AS books_count, CURRENT_TIMESTAMP() AS updated_time FROM LIVE.books_silver GROUP BY author In the CDC pipeline, we can also define views To define a view simply replace table with the VIEW keyword info These views are temporary views scoped to the pipeline they are a part of, so they are not persisted to the metastore Views can still be used to enforce data quality and metrics for views will be collected and reported as they would be for tables  ","version":"Next","tagName":"h2"},{"title":"How to join and reference tables across notebooks​","type":1,"pageTitle":"5.3 Processing CDC Feed with DLT (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.3 Processing CDC Feed with DLT Hands On#how-to-join-and-reference-tables-across-notebooks","content":"  Here we see how we can join and reference tables across notebooks We are joining our books_silver table to the orders_cleaned table, which we created in another notebook in the last lecture Since the API supports scheduling multiple notebooks as part of a single pipeline, configuration code in any notebook can reference tables and views created in any other notebook info You can think of the scope of the schema referenced by the LIVE keyword to be at the DLT pipeline level, rather than the individual notebook CREATE LIVE VIEW books_sales AS SELECT b.title, o.quantity FROM ( SELECT *, EXPLODE(books) AS books FROM LIVE.orders_cleaned ) AS o INNER JOIN LIVE.books_silver AS b ON o.book.book_id = b.book_id; Let us now go to the pipeline we created in the last lecture to add this new notebook Here is our demo bookstore pipeline we created in the last lecture Click on the Settings button From here, click Add Notebook library Click Browse Go to Repo and choose the Books Pipeline notebook Click Save As you can see in the pipeline details, this pipeline is now referencing 2 notebooks instead of one Let us now click Start to run our updated pipeline info If you have any issue running this pipeline, try to do a full refresh in order to run your pipeline from scratch It means the attempts to clear all data from each table and then load all data from the streaming sources For now, let us click Start In addition to tables you created in the last lecture, we see our new books tables. And the view books_sales that join the two pipelines tables together ","version":"Next","tagName":"h2"},{"title":"5.5 Databricks SQL","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.5 Databricks SQL","content":"5.5 Databricks SQL In this video, we will explore Databricks SQL During all previous videos, we were working in the Data Science &amp; Engineering workspace In order to work with Databricks SQL, we need to switch to the SQL persona on top of the sidebar Databricks SQL, also known as DBSQL is a data warehouse that allows you to run all your SQL and BI applications at scale and with a unified governance model As you can see in the sidebar, you have new options like SQL editor, Queries, Dashboards, Alerts In addition to Data Explorer and SQL warehouses Let us start by clicking on the SQL warehouse tab From here, we can create our first SQL warehouse info A SQL warehouse is the compute power of Databricks SQL note It is a SQL engine or endpoint based on the Spark cluster Click on Create SQL Warehouse to configure a new SQL engine Give it a name, say demo warehouse Set the cluster size to 2x small For this demo, leave all other options as default and click Create If it shows you a menu to manage the permissions for the SQL warehouse, leave all other options as default and click Close Our SQL warehouse is running and ready to be used Click on Dashboards in the sidebar Click on Visit gallery From here, click Import, next to the New York Taxi Trip Analysis option info Like this, we have created our first dashboard If we click again on Dashboards in the sidebar We can see our sample dashboard we have just created, and I have my name under the Created By field note Note that from this page, you can also create a new dashboard from scratch Let us click on our dashboard's name to reopen it We can any time click Refresh to rerun the queries behind each graph and refresh the data To view the query behind any graph, simply click on the three vertical dots on the graph, and select View Query from the menu note Note here the three tiers namespaces that is used to identify the source table, which are: catalog, database and table name info This is a preview of the new functionality to be supported by Unity Catalog Let us click Run to preview the result of this query In the Table tab, we can see the query results If we click on the second tab, we switch to the preview of our graph Click Edit visualization to modify this graph From here, you can change the setting of your visualization Let us click Cancel for now To add a new visualization using this query, click the + sign Then, Visualization Let us create a pie graph We set the X column as day of week Add a Y column for fare_amount Let us leave all other settings as defaults and click Save Click on the tab to give this visualization a name If we click on the three vertical dots of this tab. We have the option to add this graph to a dashboard We select our sample dashboard We click Add Let us navigate back to the dashboard to view this change If you scroll down, you can see your new graph has been successfully added to the dashboard We can adjust the organization of the graphs in a dashboard From the three vertical button, click Edit We can drag our visualization. Resize it Let us click on Done Editing You can share your dashboard with other users Click the Share button Select, for example, all users from the top field Choose Can Run permission Click Add From Credentials, you can choose to run using the viewer credentials if the user have access to the underlying data info If not, choose the Run As Owner to run the dashboard using the owner credential Let us now navigate to the editor to write some custom queries Click on the + button Choose create new query info Make sure you are connected to a SQL warehouse Click on the schema browser to select the Sample Catalog Select a database, click on Select info Choose NYC Taxi In this database, we have only one table called trips Click on the table name to get a preview of its schema Let us write our first query To add the table name to the query text, we can simply hover over the table name and click this double arrows button info SELECT * FROM nyctaxi.trips Let us now click on Run to preview the results Let us update this query by adding a GROUP BY clause on the pickup_zip code info SELECT pickup_zip, SUM(fare_amount) AS total_fare FROM nyctaxi.trips GROUP BY pickup_zip We will add this column again to the SELECT clause. And apply some aggregation function on the fair_amount We give this column an alias called total_fare info SELECT pickup_zip, SUM(fare_amount) AS total_fare FROM nyctaxi.trips GROUP BY pickup_zip Click Run to preview the results We can save our query by clicking on the Save button Give it a name We can also add the query result to a dashboard Click on the three vertical dots on the Table tab, then Add to Dashboard and we select our sample dashboard If we navigate back to our dashboard, and we scroll down, we can see that our query result has been successfully added to the dashboard You can also set a schedule for the query to refresh the result automatically Click on the Schedule button Use the dropdown menu to change to refresh every one week at 7 a.m. Click Okay If we navigate to Queries in the sidebar, we can find our saved query. In Databricks SQL, we can use the saved query to set up an alert From the left side bar, navigate to Alerts Alerts in Databricks SQL allow you to receive notification when a field of your query meets a certain threshold Click Create Alert in the top right Select your saved query Let us first give this alert a name, by clicking on the field at the top left of the screen For the Trigger When option, set the value column to total_fare, with a condition greater than 10,000 threshold For Refresh, we can select Never Click on Create Alert Click on the Refresh button to evaluate the alert info If the top row value has been updated and exceeded this thresholds, the alert will be triggered and I will receive a notification on this email You can set other destination to receive notifications If you click on the Add button, you can choose either an existing alert destination, or create a new destination in the Alert Destinations page As you can see, you have plenty of alerting options like Slack and Microsoft Teams ","keywords":"","version":"Next"},{"title":"5.1 Delta Live Tables (Hands On)","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.1 Delta Live Tables Hands On","content":"","keywords":"","version":"Next"},{"title":"What are Delta Live Tables?​","type":1,"pageTitle":"5.1 Delta Live Tables (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.1 Delta Live Tables Hands On#what-are-delta-live-tables","content":"  Delta Live Tables or DLT is a framework for building reliable and maintainable data processing pipelines DLT simplify the hard work of building large scale ETL while maintaining table dependencies and data quality Our DLT multi-hop pipeline is well-visualized and we can see our two bronze tables, customers and orders_raw. They are joined together into the silver table orders_cleaned. From which we calculate our gold table daily_customer_books DLT pipelines are implemented using Databricks notebooks On the pipeline details on the right, we can see the path to the notebook containing the DLT table definitions We can simply click here to navigate to the source code   ","version":"Next","tagName":"h2"},{"title":"Explore the source code​","type":1,"pageTitle":"5.1 Delta Live Tables (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.1 Delta Live Tables Hands On#explore-the-source-code","content":"  Let us explore the content of this notebook to better understand the syntax used by Delta Live Tables In this SQL notebook, we declare our Delta Live Tables that together implement a simple multi-hop architecture DLT tables will always be preceded by the LIVE keyword We start by declaring two tables implementing the bronze layer   ","version":"Next","tagName":"h2"},{"title":"Second bronze table - orders_raw​","type":1,"pageTitle":"5.1 Delta Live Tables (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.1 Delta Live Tables Hands On#second-bronze-table---orders_raw","content":"  The table orders_raw ingest Parquet data incrementally by Auto Loader from our dataset directory Incremental processing via Auto Loader required the addition of the STREAMING keyword in that declaration The cloud_files method enable auto loader to be used natively with SQL This method takes three parameters  The data file source location info CREATE OR REFRESH STREAMING LIVE TABLE orders_raw COMMENT &quot;The raw books orders, ingested from orders-raw&quot; AS SELECT * FROM cloud_files( &quot;${datasets_path}/orders-raw&quot;, &quot;parquet&quot;, map( &quot;schema&quot;, &quot;order_id STRING, order_timestamp LONG, customer_id STRING, quantity LONG&quot; ) ) The source data format, which is parquet in this case info CREATE OR REFRESH STREAMING LIVE TABLE orders_raw COMMENT &quot;The raw books orders, ingested from orders-raw&quot; AS SELECT * FROM cloud_files( &quot;${datasets_path}/orders-raw&quot;, &quot;parquet&quot;, map( &quot;schema&quot;, &quot;order_id STRING, order_timestamp LONG, customer_id STRING, quantity LONG&quot; ) ) An array of reader options info We declare the schema of our data CREATE OR REFRESH STREAMING LIVE TABLE orders_raw COMMENT &quot;The raw books orders, ingested from orders-raw&quot; AS SELECT * FROM cloud_files( &quot;${datasets_path}/orders-raw&quot;, &quot;parquet&quot;, map( &quot;schema&quot;, &quot;order_id STRING, order_timestamp LONG, customer_id STRING, quantity LONG&quot; ) ) note Notice that we add a comment here that would be visible to anyone exploring the data catalog   Let us run this query and see what will happen info CREATE OR REFRESH STREAMING LIVE TABLE orders_raw COMMENT &quot;The raw books orders, ingested from orders-raw&quot; AS SELECT * FROM cloud_files( &quot;${datasets_path}/orders-raw&quot;, &quot;parquet&quot;, map( &quot;schema&quot;, &quot;order_id STRING, order_timestamp LONG, customer_id STRING, quantity LONG&quot; ) ) Running at a DLT query from here only validate that it is syntactically valid To define and populate this table, you must create a DLT pipeline We will see later how to configure and run a new pipeline from this notebook  ","version":"Next","tagName":"h2"},{"title":"Second bronze table - customer​","type":1,"pageTitle":"5.1 Delta Live Tables (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.1 Delta Live Tables Hands On#second-bronze-table---customer","content":"  The second bronze table is customer that presents JSON customer data info This table is used below in a join operation to look up customer information CREATE OR REFRESH LIVE TABLE customers COMMENT &quot;The customers lookup table, ingested from customers-json&quot; AS SELECT * FROM json.`${datasets_path}/customers-json` We declare tables implementing the silver layer info This layer represents a refined copy of data from the bronze layer note At this level, we apply operations like data cleansing and enrichment Here we declare our silver table orders_cleaned, which enriches the order's data with customer information   ","version":"Next","tagName":"h2"},{"title":"How to implement quality control with Delta Live Tables​","type":1,"pageTitle":"5.1 Delta Live Tables (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.1 Delta Live Tables Hands On#how-to-implement-quality-control-with-delta-live-tables","content":"  In addition, we implement quality control using CONSTRAINT keywords. Here we reject records with no order_id The CONSTRAINT keyword enables DLT to collect metrics on constraint violations It provides an optional ON VIOLATION clause specifying an action to take on records that violate the constraints The three modes currently supported by Delta are included in this table   DROP ROW where we discard records that violate constraints FAIL UPDATE where the pipeline fails when the constraint is violated When omitted records violating CONSTRAINT will be included, but violation will be reported in the metrics note Notice also that we need to use the LIVE prefix in order to refer to other DLT tables   For streaming DLT tables we need to use the STREAM method info CREATE OR REFRESH STREAMING LIVE TABLE orders_cleaned ( CONSTRAINT valid_order_number EXPECT ( order_id IS NOT NULL ) ON VIOLATION DROP ROW ) COMMENT &quot;The cleaned books orders with valid order_id&quot; AS SELECT order_id, quantity, o.customer_id, c.profile:first_name AS f_name, c.profile:last_name as l_name, CAST( from_unixtime( order_timestamp, 'yyyy-MM-dd HH:mm:ss' ) AS timestamp ) AS order_timestamp, c.profile:address:country AS country FROM STREAM( LIVE.orders_raw ) AS o LEFT JOIN LIVE.customers c ON o.customer_id = c.customer_id We declare the gold table, in this case the daily number of books per customer in a specific region info Here it is China CREATE OR REFRESH LIVE TABLE cn_daily_customer_books COMMENT &quot;Daily number of books per customer in China&quot; AS SELECT customer_id, f_name, l_name, DATE_TRUNC( &quot;DD&quot;, order_timestamp ) AS order_date, SUM(quantity) AS book_counts FROM LIVE.orders_cleaned WHERE country = 'China' GROUP BY customer_id, f_name, l_name, DATE_TRUNC( &quot;DD&quot;, order_timestamp ) Let us see now how to use this notebook to create a new DLT pipeline Start by navigating to the Workflows tab on the sidebar Select the Delta Live Table tab Click Create Pipeline Fill in a pipeline name info For example, demo bookstore For notebook libraries use the navigator to locate and select the notebook with the delta tables definition, the one we have just explored Under configuration, add a new configuration parameter. Set the key to dataset_path, and the value to the location of the bookstore dataset info This parameter is used in the notebook in order to specify the path to our source data files datasets_path=dbfs:/mnt/demo-datasets/bookstore In the storage location field enter a path where the pipeline logs and data files will be stored info dbfs:/mnt/demo/dlt/demo_bookstore In the target field, enter a target database name info demo_bookstore_dlt_db The pipeline mode specifies how the pipeline will be run info Triggered pipelines run once and then shut down until the next manual or scheduled updates Continuous pipeline will continuously ingesting new data as it arrives For this demo, let us keep it triggered A new cluster would be created for our DLT pipeline Choose the cluster mode info For example, fixed size We set the number of workers to zero to create a single node cluster note Notice below the DBUs estimate provided similar to that provided when configuring interactive clusters Click Create Select Development to run the pipeline in development mode info This mode allows for interactive development by reusing the cluster, compared to creating a new cluster for each run in the prediction mode The development mode also disable retries so that we can quickly identify and fix errors Let us now click Start info The initial run will take several minutes while the cluster is provisioned Below we see all the events of our running pipeline, either information, warning, or errors On the right hand side, we see all the pipeline details and also the information related to the cluster In the middle, we see the execution flow visualized as a Directed Acyclic Graph or DAG info This representing the entities involved in the pipeline and the relationships between them Click on each entity to view a summary which includes the run status and other metadata summary, including the comment we set during the table definition in the notebook We can also see the schema of the table If you select the orders_cleaned table, you can notice the results reported in the data quality section info Because this flow has data expectation declared, those metrics are extracted here As you can see, we have no records violating our constraint. Let us now come back to our notebook for adding another table and see how this change is reflected here Let us open the notebook of this pipeline by clicking on the link here Let us scroll to the end of this notebook Add a new cell We will add a new table similar to the previous gold table declaration. But this time, instead of China, we will filter for France. But let us do something different to see what happens if we remove, for example, the LIVE prefix info CREATE OR REFRESH LIVE TABLE cn_daily_customer_books COMMENT &quot;Daily number of books per customer in China&quot; AS SELECT customer_id, f_name, l_name, DATE_TRUNC( &quot;DD&quot;, order_timestamp ) AS order_date, SUM(quantity) AS book_counts FROM orders_cleaned WHERE country = 'France' GROUP BY customer_id, f_name, l_name, DATE_TRUNC( &quot;DD&quot;, order_timestamp ) Let us see what will happen in our pipeline Let us now click start again to rerun our pipeline and examine the updated results As you can see, this generates an error. Table or view not found, because we missed the LIVE namespace Add again the LIVE keyword info CREATE OR REFRESH LIVE TABLE cn_daily_customer_books COMMENT &quot;Daily number of books per customer in China&quot; AS SELECT customer_id, f_name, l_name, DATE_TRUNC( &quot;DD&quot;, order_timestamp ) AS order_date, SUM(quantity) AS book_counts FROM LIVE.orders_cleaned WHERE country = 'France' GROUP BY customer_id, f_name, l_name, DATE_TRUNC( &quot;DD&quot;, order_timestamp ) Let us rerun our pipeline by clicking Start Our pipeline is successfully completed and we can see now our two gold tables The events and information we see in this UI are stored in the storage configuration we provided during configuring our pipeline Let us explore this directory. For this, let us create a Python notebook and copy the storage location Let us see the content of our pipeline storage location info files = dbutils.fs.ls( &quot;dbfs:/mnt/demo/dlt/demo_bookstore&quot; ) display(files) There are four directories: auto loader, checkpoints, system and tables The system directory captures all the events associated with the pipeline Let us explore the events file in the system directory info files = dbutils.fs.ls( &quot;dbfs:/mnt/demo/dlt/demo_bookstore/system/events&quot; ) display(files) These event logs are stored as a delta table Let us query this table info SELECT * FROM delta.`dbfs:/mnt/demo/dlt/demo_bookstore/system/events` All events we see in the UI are stored in this data table Let us also see what we have in the tables directory in the storage location info files = dbutils.fs.ls( &quot;dbfs:/mnt/demo/dlt/demo_bookstore/tables&quot; ) display(files) Here we see our five DLT tables Let us come back to our pipeline to grab the database name to query these tables If you click on any table, you can see the metastore information Let us write a select query on our table using the metadata store information info SELECT * FROM demo_bookstore_dlt_db.cn_daily_customer_books Let us end up by turning off our job cluster. To do so navigate to the compute tab in the left side bar Click on the job clusters tab Terminate this pipeline cluster ","version":"Next","tagName":"h2"}],"options":{"id":"default"}}