{"searchDocs":[{"title":"6.3 Unity Catalog","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.3 Unity Catalog","content":"","keywords":"","version":"Next"},{"title":"What is Unity Catalog?​","type":1,"pageTitle":"6.3 Unity Catalog","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.3 Unity Catalog#what-is-unity-catalog","content":"  Unity Catalog is a centralized governance solution across all your workspaces on any cloud It unifies governance for all data and AI assets in your Lakehouse including: Files;Tables;Machine learning models;Dashboards info These can be simply achieved using SQL language With Unity Catalog, you define your data access rules once across multiple workspaces and clouds  ","version":"Next","tagName":"h2"},{"title":"Architecture​","type":1,"pageTitle":"6.3 Unity Catalog","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.3 Unity Catalog#architecture","content":"  Before Unity Catalog, users and groups were defined per workspace Access control was managed via the Hive metastore within the workspace Unity Catalog sits out of the workspace and accessed via a user interface called the Account Console Users and the groups for Unity Catalog are managed through this account console and assign it to one or more workspaces Metastores are likewise separated out of the workspaces and managed through the account console where they can be assigned to the workspaces A Unity Catalog metastore can be assigned to more than one workspace, enabling multiple workspaces to share the same DBFS storage and the same access control lists  ","version":"Next","tagName":"h2"},{"title":"Unity Catalog 3-level namespace​","type":1,"pageTitle":"6.3 Unity Catalog","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.3 Unity Catalog#unity-catalog-3-level-namespace","content":"  We saw previously the traditional two-level namespaces used to address tables within the schemas Unity Catalog introduces a third level, which is catalogs Let us understand better the hierarchy of Unity Catalog The metastore is the top level logical container in Unity Catalog It represents metadata, that is, information about the objects being managed by the metadata, as well as the access control list that govern access to those objects In a metastore, you have catalog, which is the top level container for data objects in Unity Catalog, and forms the first part of the three level namespace we just saw info Don't confuse Unity Catalog metastore with the Hive metastore The Hive store is the default metastore linked to each Databricks workspace info While it may seem functionality similar to a Unity Catalog metastore, Unity Catalog metastores offers improved security and advanced features Unity Catalog metastore can have as many catalogs as desired  ","version":"Next","tagName":"h2"},{"title":"Unity Catalog Hierarchy​","type":1,"pageTitle":"6.3 Unity Catalog","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.3 Unity Catalog#unity-catalog-hierarchy","content":"  Catalogs contain schemas A schema, also known as database, is the second part of the three-level namespace Schemas usually contain data assets like tables, views and functions, forming the third part of the three-level namespace You need to catalog also support authentication to the underlying cloud storage through Storage Credentials Storage Credentials apply to an entire storage container External Locations represent the storage directories within a cloud storage container In addition, Unity Catalog adds Shares and Recipients which are related to Delta Sharing Shares are collections of tables shared with one or more recipient info Data sharing is out of scope for this course  ","version":"Next","tagName":"h2"},{"title":"Unity Catalog Identities​","type":1,"pageTitle":"6.3 Unity Catalog","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.3 Unity Catalog#unity-catalog-identities","content":"  In Unity Catalog, we have three types of identities   Users are individual physical users which are uniquely identified by their email addresses A user can have an admin role to perform several administrative tasks important to Unity Catalog, such as managing and assigning metastores to workspaces and managing other users A service principle is an individual identity for use with automated tools and applications Service principles are uniquely identified by Application ID info Like users, service principles can have an admin role which allow them to programmatically carry out administrative tasks We have Groups that collect users and service principles into a single entity Groups can be nested with other groups info For example, a parent group called Employees can contain two inner groups: HR and Finance groups  ","version":"Next","tagName":"h2"},{"title":"Identity Federation​","type":1,"pageTitle":"6.3 Unity Catalog","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.3 Unity Catalog#identity-federation","content":"  Databricks identities exist at two levels: at account-level and at workspace-level Unity Catalog supports a feature called Identity Federation, where identities are simply created once in the accounts console They can be assigned to one or more workspaces as needed Identity Federation eliminates the need to manually create and maintain copies of identities at workspace-levels  ","version":"Next","tagName":"h2"},{"title":"Privileges​","type":1,"pageTitle":"6.3 Unity Catalog","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.3 Unity Catalog#privileges","content":"  Unity Catalog has CREATE, USAGE, SELECT and MODIFY privileges In addition, we have also privileges related to the underlying storage which are READ FILES and WRITE FILES which replace the ANY FILE privilege we saw previously with Hive metastore We have EXECUTE privilege to allow executing user defined functions  ","version":"Next","tagName":"h2"},{"title":"Security Model​","type":1,"pageTitle":"6.3 Unity Catalog","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.3 Unity Catalog#security-model","content":"  Unity Catalog uses a different security model than Hive metastores for granting privileges There are different privileges types, and extra securable objects and principles We continue to use GRANT statement in order to give a privilege on a secure object to a principle info GRANT Privilege ON Securable_Object TO Principal   ","version":"Next","tagName":"h2"},{"title":"Accessing legacy Hive metastore​","type":1,"pageTitle":"6.3 Unity Catalog","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.3 Unity Catalog#accessing-legacy-hive-metastore","content":"  Unity Catalog is additive, this means that your legacy Hive metastore is still accessible once Unity Catalog is enabled Regardless of the Unity Catalog metastore assigned to the workspace, the catalog named hive_metastore always provide access to the Hive metastore local to that workspace  ","version":"Next","tagName":"h2"},{"title":"Features​","type":1,"pageTitle":"6.3 Unity Catalog","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.3 Unity Catalog#features","content":"  Unity Catalog also has a built-in data search and discovery Unity Catalog also provides automated lineage where you can identify the origin of your data and where it is used across all data types like tables, notebooks, workflows and dashboards Unity Catalog unifies existing legacy catalogs info There is no hard migration needed when enabling Unity Catalog In order to access the account console, you can log in as an account administrator via this link info https://accounts.cloud.databricks.com  ","version":"Next","tagName":"h2"},{"title":"7.1 Certification Overview","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Certification Overview/7.1 Certification Overview","content":"","keywords":"","version":"Next"},{"title":"Exam Questions​","type":1,"pageTitle":"7.1 Certification Overview","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Certification Overview/7.1 Certification Overview#exam-questions","content":"  The question expected during the exam will be distributed in the following way 11 questions out of 45 on the use and the benefits of using the Databricks Lakehouse platform 13 questions on building ETL pipelines with Apache Spark SQL and Python 10 questions on processing data incrementally 7 questions on building production pipelines in addition to Databricks SQL queries and Dashboards 4 questions on data governance and security practices  ","version":"Next","tagName":"h2"},{"title":"Out-of-scope​","type":1,"pageTitle":"7.1 Certification Overview","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Certification Overview/7.1 Certification Overview#out-of-scope","content":"  The following are not expected on the associate level data engineer exam All topics related to Spark interns, Databricks CLI and API Change Data Capture Data Modeling Concepts Data Protection Regulations Monitoring and logging production jobs, dependency management and testing During the exam, Data manipulation code will be always provided in SQL when possible In all other cases, code will be in Python Databricks certification are taken through webassessor platform via this link info https://www.webassessor.com/databricks You just need to sign up in order to schedule and take your exam During this exam, you will be monitored via wecam by a webassessor proctor You will be asked to provide a valid photo-based identification The proctor will be monitoring you during the exam and he can provide you with technical support if needed There will be no test aids during the exam The certification exams are automatically graded and you will receive your pass or fail grade immediately The badge and certificate will be received within 24 hours of passing the exam You will receive them via credentials.databricks.com The exam questions are multiple choice questions note This means there is only one correct answer for each question The exam has two types of questions either conceptual or code-based questions Databricks offers a practice for the Associate-Level Data Engineer exam ","version":"Next","tagName":"h2"},{"title":"6.1 Data Objects Privileges","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.1 Data Objects Privileges","content":"","keywords":"","version":"Next"},{"title":"Data objects​","type":1,"pageTitle":"6.1 Data Objects Privileges","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.1 Data Objects Privileges#data-objects","content":"  Let us see what our other object types we have in Databricks Databricks allows you to configure permissions for the following object types  Object\tScopeCATALOG\tControls access to the entire data catalog SCHEMA\tControls access to a database TABLE\tControls access to a managed or external table VIEW\tControls access to SQL views FUNCTION\tControls access to a named function ANY FILE\tControls access to the underlying filesystem  ","version":"Next","tagName":"h2"},{"title":"Privileges​","type":1,"pageTitle":"6.1 Data Objects Privileges","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.1 Data Objects Privileges#privileges","content":"  The following privileges can be configured on the data objects  Privilege\tAbilitySELECT\tRead access to an object MODIFY\tAdd, delete and modify data to or from an object CREATE\tCreate an object READ_METADATA\tView an object and its metadata USAGE\tNo effect required to perform any action on a database object ALL PRIVILEGES\tGives all privileges  ","version":"Next","tagName":"h2"},{"title":"Granting Privileges by Role​","type":1,"pageTitle":"6.1 Data Objects Privileges","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.1 Data Objects Privileges#granting-privileges-by-role","content":" Role\tCan grant access privileges forDatabricks administrator\tAll objects in the catalog and the underlying filesystem Catalog owner\tAll objects in the catalog Database owner\tAll objects in the database Table owner\tOnly the table ...\t...  ","version":"Next","tagName":"h2"},{"title":"More operations​","type":1,"pageTitle":"6.1 Data Objects Privileges","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Data Governance/6.1 Data Objects Privileges#more-operations","content":"  In addition to GRANT operation, you have also other useful operations to manage object privileges info You can deny and revoke privileges ","version":"Next","tagName":"h2"},{"title":"2.3 Advanced Delta Lake Features","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.3 Advanced Delta Lake Features","content":"","keywords":"","version":"Next"},{"title":"Time travel feature in Delta Lake​","type":1,"pageTitle":"2.3 Advanced Delta Lake Features","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.3 Advanced Delta Lake Features#time-travel-feature-in-delta-lake","content":"  With Delta, every operation on the table is automatically versioned, which provides the full audit trail of all the changes that have happened on the table You can look at the history of the table in SQL using DESCRIBE HISTORY command In addition, we can query older versions of the table. This can be done in two different ways  Use a timestamp, So, in a SELECT statement we use the keyword TIMESTAMP AS OF and we provide the time or date string info SELECT * FROM my_table TIMESTAMP AS OF &quot;2019-01-01&quot; Use a version number Since every operation on the table has a version number, you can use this version number to travel back in time as well. Here using the keyword VERSION AS OF, or simply @v, which is the short syntax info SELECT * FROM my_table VERSION AS OF 36 SELECT * FROM my_table @v36    Time travel also makes it easy to do rollbacks in case of bad writes info For example, if your pipeline job had a bug that accidentally deleted user information, you can easily fix this using the RESTORE TABLE command. Either to restore the table to a specific timestamp or to a specific version number RESTORE TABLE my_table TO TIMESTAMP AS OF &quot;2019-01-01&quot; RESTORE TABLE my_table TO VERSION AS OF 36   ","version":"Next","tagName":"h2"},{"title":"Compacting small files feature in Delta Lake​","type":1,"pageTitle":"2.3 Advanced Delta Lake Features","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.3 Advanced Delta Lake Features#compacting-small-files-feature-in-delta-lake","content":"  Delta Lake can improve the speed of read queries from a table One way to improve this speed is by compacting small files into larger ones You trigger compaction simply by running the OPTIMIZE command info OPTIMIZE my_table For example, if you have many files by running the OPTIMIZE command, they will be compacted in one or more larger files which improves the table performance  ","version":"Next","tagName":"h2"},{"title":"Z-order indexing in Delta Lake​","type":1,"pageTitle":"2.3 Advanced Delta Lake Features","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.3 Advanced Delta Lake Features#z-order-indexing-in-delta-lake","content":"  With OPTIMIZE, we can also do z-order indexing. Z-order indexing in Delta Lake is about co-locating and reorganizing column information in the same set of files This can be done by adding the ZORDER BY keyword to the OPTIMIZE command, followed by one or more column name info OPTIMIZE my_table ZORDER BY colum_name For example, if you have a numerical column in the data files, ID for example, by applying the ZORDER on this column, the first compacted file will contain values from 1 to 50, while the other one will contain values from 51 to 100 Z-Order indexing is used by data skipping algorithm to extremely reduce the amount of data that need to be read info In our example, if you query an ID, say 30, Delta is sure now that ID 30 is in file number 1, so it can easily skip the scanning the file number 2, which will sav a huge amount of time  ","version":"Next","tagName":"h2"},{"title":"Vacuum a Delta Table​","type":1,"pageTitle":"2.3 Advanced Delta Lake Features","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.3 Advanced Delta Lake Features#vacuum-a-delta-table","content":"  You might be wondering what about unused data files like uncommitted files and files that are no longer in the latest state of the transaction log for the table? Delta Lake allows you to do garbage collection by using VACUUM command With VACUUM command, you just need to specify the threshold of retention period for the files, so you delete all the files older than this threshold info VACUUM table_name [retention period] By default, the threshold is 7 days. This means that VACUUM operation will prevent you from deleting files less than 7 days old. Just to be sure that no longer running operations are still referencing any of the files to be deleted note Once you run a VACUUM on a Delta table, you lose the ability to time travel back to a version older than the specified retention period, simply because the data files are no longer exist ","version":"Next","tagName":"h2"},{"title":"2.4 Apply Advanced Delta Features (Hands","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.4 Apply Advanced Delta Features (Hands","content":"","keywords":"","version":"Next"},{"title":"How to read the history of a table​","type":1,"pageTitle":"2.4 Apply Advanced Delta Features (Hands","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.4 Apply Advanced Delta Features (Hands#how-to-read-the-history-of-a-table","content":"  Let us review again our table history info DESCRIBE HISTORY employees Here we can see the 3 versions of our table  ","version":"Next","tagName":"h2"},{"title":"How to use the Time Travel feature with VERSION AS OF​","type":1,"pageTitle":"2.4 Apply Advanced Delta Features (Hands","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.4 Apply Advanced Delta Features (Hands#how-to-use-the-time-travel-feature-with-version-as-of","content":"  In Delta Lake, we can easily query previous versions of our table, and this feature of time travel is possible thanks to those extra data files that had been marked as removed in our transaction log Let's say we want to access our data before the update operation, which is version number 1 We can simply use SELECT query with VERSION AS OF keyword, and we specify the version number, in our case it is version 1 info SELECT * FROM employees VERSION AS OF 1 Here we can see our data before the update operation info Another alternative syntax is to use @v followed by the version number  ","version":"Next","tagName":"h2"},{"title":"How to restore data with the RESTORE command​","type":1,"pageTitle":"2.4 Apply Advanced Delta Features (Hands","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.4 Apply Advanced Delta Features (Hands#how-to-restore-data-with-the-restore-command","content":"  Imagine this scenario, we deleted our data and we need to restore them info DELETE FROM employees note Here, -1 means that all data has been removed Let us confirm this. So, our table data has been indeed removed We can simply roll back to a previous version before deletion using RESTORE TABLE command info RESTORE TABLE employees TO VERSION AS OF 2 Let us explore what really happened in our table info DESCRIBE HISTORY employees As you can see, the RESTORE command has been recorded as a transaction  ","version":"Next","tagName":"h2"},{"title":"How to compact small files with OPTIMIZE and z-order indexing with ZORDER BY​","type":1,"pageTitle":"2.4 Apply Advanced Delta Features (Hands","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.4 Apply Advanced Delta Features (Hands#how-to-compact-small-files-with-optimize-and-z-order-indexing-with-zorder-by","content":"  Let us now talk about the OPTIMIZE command and how to compact small files and do Z-order indexing Since Spark work in parallel, you usually end up by writing too many small files. Having many small data files negatively affect the performance of the Delta Table note In our case, we have 4 small data files To resolve this issue, we can use OPTIMIZE command that combines files toward an optimal size OPTIMIZE will replace existing data files by combining records and rewriting the results info OPTIMIZE employees ZORDER BY id Here, we can see that our 4 data files have been soft deleted, and a new file has been added that compacts those 4 files. In addition, you may notice that we added the ZORDER indexing with our OPTIMIZE command. Z-order indexing speeds up data retrieval when filtering on provided fields, by grouping data with similar values within the same data files info In our case, we do z-order by the id column. However, on such a small data set, it does not provide any benefit Let us confirm the output of the OPTIMIZE command by running DESCRIBE DETAIL on our table info DESCRIBE DETAIL employees Here, we can see that the number of files in the current version is only one. Let us see how to OPTIMIZE operation has been recorded in our table history info DESCRIBE HISTORY employees As expected, OPTIMIZE command created another version in our table, meaning that version 5 is the most recent version of our table Let us now explore the data files in our table directory info %fs ls 'dbfs:/user/hive/warehouse/employees' Here, we can see that there are 7 data files. But, we know, that our current table version referencing only one file after the OPTIMIZE operation. It means that other data files are unused files, and we can simply clean them up We can manually remove old data files using the VACUUM command. Let run this command and see what happens info VACUUM employees If we check the table directory again, we see that nothing happened. The data files are still there. This is because we need to specify a retention period, and by default this retention period is 7 days info That means that VACUMM operation will prevent us from deleting files than 7 days old, just to ensure that no long running operations are still referencing any of the files to be deleted If we try to execute VACUUM command with a retention of 0 hour for keeping only the current version, this will not work because, the default threshold is 7 days info VACUUM employees RETAIN 0 HOURS In this demo, we will do a work around for demonstration purposes only note You should not do this in production The idea is to turn off the retention duration check info SET spark.databricks.delta.retentionDurationCheck.enabled = false; We can run our VACUUM command info VACUUM employees RETAIN 0 HOURS Let us explore again the table directory info %fs ls &quot;dbfs:/user/hive/warehouse/employees&quot; Files have been successfully deleted. 6 data files have been removed info Those files were really useful for Delta Lake time travel feature We are no longer able to access old data versions We can easily confirm this by querying an old table version We got here a file not found exception, because the data files for this version no longer exist Finally, let us permanently delete the table with its data from the Lakehouse For this, we use the DROP TABLE command, like in SQL. The table has been successfully deleted  ","version":"Next","tagName":"h2"},{"title":"2.1 Delta Lake","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.1 Delta Lake","content":"","keywords":"","version":"Next"},{"title":"What is Delta Lake?​","type":1,"pageTitle":"2.1 Delta Lake","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.1 Delta Lake#what-is-delta-lake","content":"  Delta Lake is an open-source storage framework that bring reliability to data lakes Data lakes have many limitations, such as data inconsistency and performance issues Delta lake technology helps overcoming these challenges It's a storage framework or a storage layer, but it not a storage format or a storage medium It enables building lakehouse architecture info Lakehouse is a platform that unifies both data warehouse and advanced analytics Delta Lake is a component which is deployed on the cluster as part of the Databricks runtime If you are creating a Delta Lake table, it gets stored on the storage in one or more data files in parquet format Along with these files, Delta Lake stores a transaction log as well   ","version":"Next","tagName":"h2"},{"title":"What is the Delta Lake transaction log?​","type":1,"pageTitle":"2.1 Delta Lake","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.1 Delta Lake#what-is-the-delta-lake-transaction-log","content":"  The Delta Lake transaction log (also known as Delta Log) is ordered records of every transaction performed on the table since its creation. It serves as a single source of truth info Every time you query the table, Spark checks this transaction log to retrieve the most recent version of the data Each committed transaction is recorded in a JSON file info It contains the operation that has been performed, whether, for example, it's an INSERT or UPDATE and the predicates such as conditions and filters used during this operation in addition to all the files that have been affected because of this operation  ","version":"Next","tagName":"h2"},{"title":"Example of Delta Lake Transaction Logs​","type":1,"pageTitle":"2.1 Delta Lake","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.1 Delta Lake#example-of-delta-lake-transaction-logs","content":" ","version":"Next","tagName":"h2"},{"title":"Scenario 1: Inserting new files​","type":1,"pageTitle":"2.1 Delta Lake","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.1 Delta Lake#scenario-1-inserting-new-files","content":"  In this scenario we have a writer process and a reader process  Writer process​   Once the writer process starts, it stores the Delta Lake table in two data files in a parquet format As soon as the writer process finishes writing, it adds the transaction log as 000.json into the _delta_log directory  Reader process​   A reader process always starts by reading a transaction log info In this case, it reads the 000.json transaction log that contains information of the File1.parquet and File2.parquet  ","version":"Next","tagName":"h3"},{"title":"Scenario 2: Updating existing files​","type":1,"pageTitle":"2.1 Delta Lake","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.1 Delta Lake#scenario-2-updating-existing-files","content":" Writer process​   In our second scenario, the writer process wants to update a record which presents in File1.parquet, but in Delta Lake, instead of updating the record in the file itself, it will make a copy of this file and make the necessary updates in the new file called File3.parquet It then updates the log by writing a new JSON file. This new log file knows that File1.parquet is no longer needed  Reader process​   The reader process reads the transaction log that tells that only File2.parquet and File3.parquet are part of the current table version so it can start reading them  ","version":"Next","tagName":"h3"},{"title":"Scenario 3: Handling write and read simultaneously​","type":1,"pageTitle":"2.1 Delta Lake","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.1 Delta Lake#scenario-3-handling-write-and-read-simultaneously","content":"  Let us see one more scenario. Here, both processes want to work at the same time   Writer and Reader process​   The writer process starts writing the File4.parquet. On the other hand, the reader process reads the transaction log that only has information about File2.parquet and File3.parquet and not File4.parquet as it is not fully written yet It starts reading those two files, File2.parquet and File3.parquet which represent the most recent data at the moment info Delta Lake guarantees that you will always get the most recent version of the data  Reader process​   Your read operation will never have a deadlock state or conflicts with any ongoing operation on the table   Writer process​   The writer process finishes and it adds a new file to the log  ","version":"Next","tagName":"h3"},{"title":"Scenario 4: Handling errors when writing files​","type":1,"pageTitle":"2.1 Delta Lake","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.1 Delta Lake#scenario-4-handling-errors-when-writing-files","content":" Writer process​   The writer process starts writing the File5.parquet to the lake, but this time there is an error in the job, which leads to adding an incomplete file Because of this failure, Delta Lake module does not write any information to the log   Reader process​   The reader process reads the transaction log that has no information about that incomplete File5.parquet note The reader process will read only File2.parquet, File3.parquet, and File4.parquet As you can see Delta Lake guarantees that you will never read dirty data  ","version":"Next","tagName":"h3"},{"title":"Delta Lake Advantages​","type":1,"pageTitle":"2.1 Delta Lake","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.1 Delta Lake#delta-lake-advantages","content":"  The transaction log allows Delta Lake to perform ACID transactions on data lakes It allows also to handle scalable metadata This log also provides the full audit trail of all the changes that have happened on the table The underlying file format for Delta is nothing but .parquet and .json format  ","version":"Next","tagName":"h2"},{"title":"2.5 Relational entities","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.5 Relational entities","content":"","keywords":"","version":"Next"},{"title":"Databases​","type":1,"pageTitle":"2.5 Relational entities","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.5 Relational entities#databases","content":"  In Databricks, a database is actually a schema in Hive metastore In order to create a database, you could use either CREATE DATABASE syntax or instead use CREATE SCHEMA keyword, which is exactly the same  ","version":"Next","tagName":"h2"},{"title":"What is really a Hive metastore in Databricks?​","type":1,"pageTitle":"2.5 Relational entities","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.5 Relational entities#what-is-really-a-hive-metastore-in-databricks","content":"  A Hive metastore is a repository of metadata that stores information for data structure, such as databases, tables and partitions It holds metadata about your table and data, such as: The table definition The format of the data Where the data is actually stored in the underlying storage Every Databricks workspace has a central Hive metastore accessible by all clusters to persist table metadata info By default, you have a database called &quot;default&quot; To create some tables in this default database, we use the CREATE TABLE statement without specifying any database name info The table definition would be under the default database in the Hive metastore The table data will be located under the hive default directory, which is /user/hive/warehouse In addition to the default database, we can create other databases To do so we use the CREATE DATABASE or CREATE SCHEMA syntax The database will be created in the Hive metastore and the database folder will be under the hive default directory note Notice that the database folder has an extension .db to distinguish it from the tables directories We can use this database to create some tables. The table's definition will be in the Hive metastore and the data files would be under the database folder in the Hive default directory  ","version":"Next","tagName":"h3"},{"title":"How to create databases outside of the default hive directory​","type":1,"pageTitle":"2.5 Relational entities","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.5 Relational entities#how-to-create-databases-outside-of-the-default-hive-directory","content":"  It is possible to create databases outside of the default hive directory We use the CREATE SCHEMA syntax with the LOCATION keyword to specify the path in which the database will be stored info CREATE SCHEMA db_y LOCATION 'dbfs:/custom/path/db_y.db' warning The database definition would be as usual in the hive metastore. While the database folder would be in the specified custom location outside of the hive default directory As usual, we can use this database to create some tables. While the table definition will be in the hive metastore, the actual data files for these tables will be stored in the database folder in that custom location  ","version":"Next","tagName":"h3"},{"title":"Tables​","type":1,"pageTitle":"2.5 Relational entities","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.5 Relational entities#tables","content":"  In Databricks, there are two types of tables: Managed tables and External tables  ","version":"Next","tagName":"h2"},{"title":"Managed Tables​","type":1,"pageTitle":"2.5 Relational entities","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.5 Relational entities#managed-tables","content":"  A managed table is when the table is created in the storage under the database directory, which is the default case info For a managed table, Hive owns both the metadata and table data, which means that it manages the lifecycle of the table. When you drop a managed table, the underlying data files will be deleted  ","version":"Next","tagName":"h3"},{"title":"External Tables​","type":1,"pageTitle":"2.5 Relational entities","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.5 Relational entities#external-tables","content":"  An external table is when the table is created in the storage outside the database directory in a path specified by the LOCATION keyword info Hive owns only the table metadata, but not the underlying data files. So, when you drop an external table, the underlying data files will not be deleted We can simply create an external table in the default database simply by using the CREATE TABLE statement with the LOCATION keyword The definition for this external table will be in the hive metastore under the default database. While the data files will be stored in the specified external location In the same way, we can create an external table in any database We specify the database name with the keyword USE, and we create the table with the LOCATION keyword, followed by the path to where this external table needs to be stored info USE db_x; CREATE TABLE table3 LOCATION 'dbfs:/some/path_2/x_tabl3'; We could choose the same path as the one for the default database or simply use another location like in this case info The table definition will be in the hive metastore while the data files will be in the specified external location Even if the database was created in a custom location outside of the hive default directory. We can normally create external table in this database. Again, we choose the database by the USE keyword and we create the external table with the LOCATION keyword ","version":"Next","tagName":"h3"},{"title":"2.8 Views","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.8 Views","content":"","keywords":"","version":"Next"},{"title":"Types of Views​","type":1,"pageTitle":"2.8 Views","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.8 Views#types-of-views","content":"  There are three types of views in Databricks  Stored Views Like tables stored views are presented in the database To define a view, we use the CREATE VIEW statement with the AS keyword followed by your SQL query Once created, you can query your view with a standard SELECT statement, just as if it were a table info CREATE VIEW view_name AS query Temporary Views Temporary view is tied to a Spark session, so it dropped when the session ends To create a temporary view, we simply add TEMP or TEMPORARY keyword to CREATE VIEW command info CREATE TEMP VIEW view_name AS query When a SparkSession is created in Databricks There are several scenarios in which a new Spark session is created info For example, when opening a new notebook, a new session is created. Also, when detaching and reattaching a notebook to a cluster. And after installing a Python package, which lead to restarting the Python interpreter. Or simply after restarting the cluster. Global Temporary Views It behaves much like other views, but differs in one important way. It is tied to the cluster As long as the cluster is running, any notebook attached to the cluster can access its global temporary views To define a global temporary view, we simply add GLOBAL TEMP to the command Global temporary views are added to a cluster's temporary database called global_temp info CREATE GLOBAL TEMP VIEW view_nameAS query SELECT * FROM global_temp.view_name When you query this view in a SELECT statement, you need to use the global_temp database qualifier  ","version":"Next","tagName":"h2"},{"title":"Comparison between views​","type":1,"pageTitle":"2.8 Views","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.8 Views#comparison-between-views","content":" (Stored) Views\tTemp Views\tGlobal Temp ViewsPersisted in DB\tSession-scoped\tCluster-scoped Dropped only by DROP VIEW\tDropped when session ends\tDropped when cluster restarted CREATE VIEW\tCREATE TEMP VIEW\tCREATE GLOBAL TEMP VIEW ","version":"Next","tagName":"h2"},{"title":"2.7 Set Up Delta Tables","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.7 Set Up Delta Tables","content":"","keywords":"","version":"Next"},{"title":"Table Constraints​","type":1,"pageTitle":"2.7 Set Up Delta Tables","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.7 Set Up Delta Tables#table-constraints","content":"  Once you create your Delta table, either with regular CREATE TABLE or CTAS statements, you can add constraints to your table Databricks currently supports two types of table constraints, NOT NULL constraints and CHECK constraints In both cases, you must ensure that there is no data violating the constraint is already in the table prior to defining the constraint Once a constraint has been added to a table, new data violating the constraint would result in write failure In this example, we add a Check constraint to the date column of our table. info Note that check constraints look like standard WHERE clauses you might use to filter a dataset ALTER TABLE orders ADD CONSTRAINT valid_date CHECK (date &gt; '2020-01-01')   ","version":"Next","tagName":"h2"},{"title":"Cloning Delta Lake Tables​","type":1,"pageTitle":"2.7 Set Up Delta Tables","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.7 Set Up Delta Tables#cloning-delta-lake-tables","content":"  What if you want to backup or make a copy of your delta table? For this Delta Lake has two options for efficiently copying Delta Tables  DEEP CLONE info Deep clone fully copies both data and metadata from a source table to a target The command is pretty simple. CREATE TABLE and you provide the name of the new target table, followed by DEEP CLONE keyboard and you indicate the name of the source table CREATE TABLE table_clone DEEP CLONE source_table This copy can occur incrementally Executing this command again can synchronize changes from the source to the target location. And because all the data must be copied over, this can take a while for large data sets SHALLOW CLONE info With SHALLOW CLONE, you can quickly create a copy of a table since it just copies the Delta transaction logs That means there is no data moving during shallow cloning CREATE TABLE table_clone SHALLOW CLONE source_table SHALLOW CLONE is a good option, for example, to test out applying changes on a table without the risk of modifying the current table   In either cases, deep or shallow, data modification applied to the cloned version of the table will be tracked and stored separately from the source, so it will not affect the source table ","version":"Next","tagName":"h2"},{"title":"4.1 Structured Streaming","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#introduction","content":"  This lecture focuses on Spark Structured Streaming in Databricks You will learn what is a Data Stream and how to process streaming data using Spark Structured Streaming You will also understand how to use DataStreamReader to perform a stream read from a source and how to use and configure DataStreamWriter to perform a streaming write to sink  ","version":"Next","tagName":"h2"},{"title":"Data Stream​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#data-stream","content":"  A Data Stream is any data source that grows over time info New data in a Data Stream might correspond to: A new JSON log file landing into a cloud storage Updates to a database captured in a CDC or Change Data Capture feed Events queued in a pub/sub messaging feed like Kafka  ","version":"Next","tagName":"h2"},{"title":"Approaches to data stream processing​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#approaches-to-data-stream-processing","content":"  There are 2 approaches to processing a data stream:   Reprocess the entire dataset each time you receive a new update to your data Only capture files or records that have been added since the last time an update was run  ","version":"Next","tagName":"h3"},{"title":"Spark Structured Streaming​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#spark-structured-streaming","content":"  Spark Structured Streaming is a scalable streaming processing engine Spark Structured Streaming allows you to query an infinite data source and automatically detect new data and process the result incrementally into a data sink A sink is just a durable file system, such as files or tables  ","version":"Next","tagName":"h2"},{"title":"How to interact and query an infinite data source?​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#how-to-interact-and-query-an-infinite-data-source","content":"  Spark Structured Streaming allows the user to interact with a streaming source by treating the source as if it were a static table of records info New data in the input data stream is simply treated as a new rows appended to a table Such a table representing a infinite data source is seen as an &quot;unbounded&quot; table  ","version":"Next","tagName":"h3"},{"title":"Input Streaming Table​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#input-streaming-table","content":"  An input data stream could be: A directory of files; A messaging system like Kafka; A Delta table. Delta Lake is well-integrated with Spark Structured Streaming We can simply use spark.readStream to query the delta table as a streaming source, which allows to process: All of the data present in the table; Any new data that arrive later info streamDF = spark.readStream.table(&quot;Input_Table&quot;) This creates a streaming data frame on which we can apply any transformation as if it were just a static data frame  ","version":"Next","tagName":"h2"},{"title":"How to persist the result of a streaming query​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#how-to-persist-the-result-of-a-streaming-query","content":"  To persist the result of a streaming query, we need to write them out to durable storage using dataframe.writeStream method With the writeStream method, we can configure our output info We can trigger the streaming processing every 2 minutes to check if there are new arriving records, and we choose to append them to the target table streamDF = spark.readStream.table(&quot;Input_Table&quot;) streamDF.writeStream.trigger( processingTime=&quot;2 minutes&quot; ).outputMode( &quot;append&quot; ).option( &quot;checkpointLocation&quot;, &quot;/path&quot; ).table(&quot;Output_Table&quot;) All this happened thanks to checkpoints created by Spark to track the progress of your streaming processing  ","version":"Next","tagName":"h3"},{"title":"Trigger methods​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#trigger-methods","content":"  When defining a streaming write, the trigger method specify when the system should store the next set of data info This is called the trigger interval By default, if you don't provide any trigger interval, the data will be processed every half second  ","version":"Next","tagName":"h2"},{"title":"Trigger based on fixed intervals with processingTime option​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#trigger-based-on-fixed-intervals-with-processingtime-option","content":"  You can specify a fixed interval. The data will be processed in micro batches at your specified interval, for example every 5 minutes  ","version":"Next","tagName":"h3"},{"title":"Trigger based on available data with Once or availableNow option​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#trigger-based-on-available-data-with-once-or-availablenow-option","content":"  You can run your stream in a batch mode to process all available data at once using either trigger Once option or availableNow option With the Once and availableNow option, the trigger will stop on its own once all available data is processed The difference between Once and availableNow is: With Once, all data will be processed in a single batch; With availableNow, all data is processed in multiple micro batches  ","version":"Next","tagName":"h3"},{"title":"Output Modes​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#output-modes","content":"  In append mode, only new rows are incrementally appended to the target table with each batch In complete mode, the result table is recalculated each time a write is triggered, so the target table is overwritten with each batch  ","version":"Next","tagName":"h2"},{"title":"Checkpointing​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#checkpointing","content":"  Databricks creates checkpoints by storing the current state of your streaming job to cloud storage info An important note here is that checkpoints cannot be shared between several streams A separate checkpoint location is required for every streaming write to ensure processing guarantees  ","version":"Next","tagName":"h2"},{"title":"Guarantees​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#guarantees","content":"  Structured streaming provide two guarantees:  ","version":"Next","tagName":"h2"},{"title":"Fault Tolerance​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#fault-tolerance","content":"  The streaming agent can resume from where it left off if there is a failure checkpointing and the mechanism called write-ahead logs allows the user to track the streaming progress by recording the offset range of data being processed during each trigger interval  ","version":"Next","tagName":"h3"},{"title":"Exactly-once guarantee​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#exactly-once-guarantee","content":"  Structured streaming also ensures exactly once data processing because the streaming sink are designed to be idempotent info Multiple writes of the same data identified by the offset, do not result in duplicates being written to the sink The two guarantees here only work if the streaming source is repeatable, like cloud-based object storage or pub/sub messaging service Repeatable data sources and idempotent sinks allows Spark Structured Streaming to ensure end-to-end exactly once semantics under any failure condition  ","version":"Next","tagName":"h3"},{"title":"Unsupported Operations​","type":1,"pageTitle":"4.1 Structured Streaming","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.1 Structured Streaming#unsupported-operations","content":"  We need to understand that some operations are not supported by streaming data frames Operations such as sorting and deduplication, are either too complex or logically not possible to do when working with streaming data There are advanced streaming methods like windowing and watermarking that can help to do such operations ","version":"Next","tagName":"h2"},{"title":"2.6 Databases and Tables on Databricks (Hands On)","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.6 Databases and Tables on Databricks (","content":"","keywords":"","version":"Next"},{"title":"How to create an external table​","type":1,"pageTitle":"2.6 Databases and Tables on Databricks (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Databricks Lakehouse Platform/2.6 Databases and Tables on Databricks (#how-to-create-an-external-table","content":"  Let us now create an external table under the default database To create an external table, you need to simply to specify in the CREATE TABLE statement the LOCATION keyword followed by the path to where this table needs to be stored. info In our case, we store this table under /mnt/demo directory CREATE TABLE managed_default ( width INT, length INT, height INT ) LOCATION 'dbfs:/mnt/demo/external_default'; INSERT INTO managed_default VALUES ( 3 INT, 2 INT, 1 INT ) Let us take a look on the Data Explorer. Here we can see that the table has been created in the hive metastore Let us now run DESCRIBE EXTENDED on our external table info DESCRIBE EXTENDED external_default Here we can see that this table is indeed an external table, and it is created in the specified location under /mnt/demo. Let us now see what will happen if we drop the managed table The table has been successfully deleted. Let us confirm this by checking the table directory info %fs ls 'dbfs:/user/hive/warehouse/managed_default' DROP TABLE external_default In the hive metastore, we see that both tables are no longer exist info However, if we check the table directory. We see that the table directory and the data files are still there %fs ls 'dbfs:/mnt/demo/external_default' Since this table is created outside the database directory, the underlying data is not managed by Hive. So, dropping the table will not delete the underlying data files as we see here In addition to the default database, we can also create extra databases. To do so we can use either CREATE SCHEMA syntax or CREATE DATABASE syntax, which is actually the same info CREATE SCHEMA new_default Here we can see that the new database has been created Let us run the DESCRIBE DATABASE EXTENDED on our database to see some metadata information info DESCRIBE DATABASE EXTENDED new_default Here we can see that the database itself is created under the default Hive warehouse directory info Notice that the database has .db extension to differentiate it from other table folders in the same directory Let us create some tables in this new database Here we will create also a managed table and an external table info USE new_default; /* managed table */ CREATE TABLE managed_new_default ( width INT, length INT, height INT ); INSERT INTO managed_new_default VALUES ( 3 INT, 2 INT, 1 INT ); /* external table */ CREATE TABLE external_new_default ( width INT, length INT, height INT ) LOCATION 'dbfs:/mnt/demo/external_new_default'; INSERT INTO external_new_default VALUES ( 3 INT, 2 INT, 1 INT ); To create a new table in a database different then the default one, you need to specify the database tob e used through the USE keywords Let us run this command. In the Data Explorer, we see that the two tables have been created info DESCRIBE EXTENDED managed_new_default Here, we can see that this new table is indeed a managed table created in its database folder under the default hive warehouse directory The second table where we use the LOCATION keyword has been defined as external table under /mnt/demo location We can simply drop those two tables to see again that the table directory and the data files of the managed table have been all removed info DROP TABLE managed_new_default; DROP TABLE external_new_default; In the Data Explorer, we see that both tables have been dropped from the new database. However, as expected, the table directory and the data files of the external table are still there Let us finally create a database in a custom location outside of the Hive directory info CREATE SCHEMA custom LOCATION 'dbfs:/Shared/schemas/custom.db' As we can see in the Data Explorer, the database has been really created in the hive metastore. However, if we run the DESCRIBE DATABASE EXTENDED, we see that it is created in the custom location we have defined during the creation of the database, and it is different from the default hive directory info DESCRIBE DATABASE EXTENDED custom You can normally create managed and external tables in this database info USE custom; /* managed table */ CREATE TABLE managed_custom ( width INT, length INT, height INT ); INSERT INTO managed_custom VALUES ( 3 INT, 2 INT, 1 INT ); /* external table */ CREATE TABLE external_custom ( width INT, length INT, height INT ) LOCATION 'dbfs:/mnt/demo/external_custom'; INSERT INTO external_custom VALUES ( 3 INT, 2 INT, 1 INT ); In hive metastore, we can see the tables of our custom location The managed_custom table is indeed a managed table since it is created in the database folder located in a custom location The second table is an external table since it is created outside the database directory By dropping the two tables, we can see that they have successfully deleted from the hive metastore The managed table directory and the data files are no longer exist in the database directory located in the custom location. While the external table's directory, and its data file are not deleted and are still in this external location outside the database directory ","version":"Next","tagName":"h2"},{"title":"3.1 Querying Files","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.1 Querying Files","content":"","keywords":"","version":"Next"},{"title":"How to extract data directly from files​","type":1,"pageTitle":"3.1 Querying Files","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.1 Querying Files#how-to-extract-data-directly-from-files","content":"  You will learn how to extract data directly from files using Spark SQL and different methods to review raw files contents We will see how to use the Spark SQL to configure options for extracting data from external sources and how to use CTAS statements to create Delta Lake tables To query file content, we can simply use a SELECT statement. SELECT * FROM a file format, and we specify the file path info SELECT * FROM file_format.`/path/to/file` Make a special note of the use of back ticks (`) and not single quotes around the path This work well with self-describing formats that have well-defined schema like JSON and parquet It is not very useful with non-describing formats like CSV A path file could be a single file. Or we can use wildcard (*) character to read multiple simultaneously. Or simply reading the whole directory info Assuming that all of the files in the directory have the same format and schema Here is an example to query a JSON file info SELECT * FROM json.`/path/file_name.json` When working with text-based files which include JSON, CSV, TSV and TXT format, You can use the &quot;text&quot; format to extract data as raw strings info SELECT * FROM text.`/path/to/file` This can be useful when input data could be corrupted. In this case, we extract the data as raw string and we apply custom text parsing functions to extract values from text files In some cases, we need the binary representation of files content, for example, when dealing with images and unstructured data Here we can use simply binaryFile as a format info SELECT * FROM binaryFile.`/path/to/file` After extracting data from external data sources, we need to load them into the Lakehouse which ensures that all of the benefits of Databricks platform can be fully leveraged To load data from files into Delta tables. We use CTAS statements, which is CREATE TABLE AS SELECT query info CREATE TABLE table_name AS SELECT * FROM file_format.`/path/to/file` Here we are querying data from files directly. CTAS statements inferior schema information from query results and do not support manual schema declaration info This means the CTAS statements are useful for external data injection from sources with well-defined schema such as parquet files and tables warning CTAS statements also do not support specifying additional file options. And this is why this statement presents significant limitation when trying to ingest data from CSV files  ","version":"Next","tagName":"h2"},{"title":"How to use additional file options with the USING option​","type":1,"pageTitle":"3.1 Querying Files","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.1 Querying Files#how-to-use-additional-file-options-with-the-using-option","content":"  We need another solution that supports options. This solution is the regular CREATE TABLE statement, but with the USING keyword By adding the USING keyword, we specify the external data source type, for example CSV format and with any additional options info CREATE TABLE table_name ( column_name1 col_type1 ) USING data_source OPTIONS ( key1=val1, key2=val2 ) LOCATION = path You need to specify a location to where these files are stored info CREATE TABLE table_name ( column_name1 col_type1 ) USING data_source OPTIONS ( key1=val1, key2=val2 ) LOCATION = path With this command, we are always creating an external table The table here is just a reference to the files. Unlike with CTAS statements, here there is no data moving during table creation We are just pointing to files stored in an external location info These files are kept in its original format, which means we are creating here a non-Delta table  ","version":"Next","tagName":"h2"},{"title":"How to create a table from a CSV file​","type":1,"pageTitle":"3.1 Querying Files","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.1 Querying Files#how-to-create-a-table-from-a-csv-file","content":"  Here is an example of creating a table using CSV external source We are pointing to CSV files exist in an external location We are specifying the options for reading the files info Like the fact that there is a header present in the files and the delimiter is a semicolon (;) CREATE TABLE table_name ( col_name1 col_type1 ) USING CSV OPTIONS ( header=&quot;true&quot;, delimiter=&quot;;&quot; ) LOCATION = path We are providing the location to these CSV files info CREATE TABLE table_name ( col_name1 col_type1 ) USING CSV OPTIONS ( header=&quot;true&quot;, delimiter=&quot;;&quot; ) LOCATION = path   ","version":"Next","tagName":"h2"},{"title":"How to create a table using JDBC connection​","type":1,"pageTitle":"3.1 Querying Files","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.1 Querying Files#how-to-create-a-table-using-jdbc-connection","content":"  Another example is to create a table using JDBC connection to refer to data in an external SQL database We provide the necessary OPTIONS like the connection string (url), the username, and the password for this database, and of course, the database table (dbtable) containing the data info CREATE TABLE table_name ( col_name1 col_type1 ) USING JDBC OPTIONS ( url=&quot;jdbc:sqlite://hostname:port&quot;, dbtable=&quot;database.table&quot;, user=&quot;username&quot;, password=&quot;pwd&quot; )   ","version":"Next","tagName":"h2"},{"title":"Limitation of tables from external data sources​","type":1,"pageTitle":"3.1 Querying Files","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.1 Querying Files#limitation-of-tables-from-external-data-sources","content":"  A table with external data source has a limitation It is not a Delta table. It means the performance and the features of Delta Lake are no more guaranteed, like time travel feature, and the guarantee that we are always reading the most recent version of the data If you are referring to a huge database table, this also can cause performance issues info Fortunately, we have a solution for this limitation. The solution is simply to create a temporary view, referring to the external data source, and then query this temporary view to create a table using CTAS statements In this way, we are extracting the data from the external data source and load it in a Delta table As you can see, with CTAS statement, you can not only query files, but you can query any object like a temporary view in this case CREATE TEMP VIEW temp_view_name ( col_name1 col_type1 ) USING data_source OPTIONS ( key1=val1, key2=val2 ); CREATE TABLE table_name AS SELECT * FROM temp_view_name  ","version":"Next","tagName":"h2"},{"title":"4.5 Multi-hop Architecture","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.5 Multihop Architecture","content":"","keywords":"","version":"Next"},{"title":"What is a multi-hop architecture?​","type":1,"pageTitle":"4.5 Multi-hop Architecture","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.5 Multihop Architecture#what-is-a-multi-hop-architecture","content":"  A multi-hop architecture, also known as medallion architecture, is a design pattern used to logically organize data in a multi-layered approach Its goal is to incrementally improve the structure and the quality of the data as it flows through each layer of the architecture   ","version":"Next","tagName":"h2"},{"title":"Layers of a multi-hop architecture​","type":1,"pageTitle":"4.5 Multi-hop Architecture","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.5 Multihop Architecture#layers-of-a-multi-hop-architecture","content":"  Multi-hop architecture usually consists of 3 layers  Bronze Bronze table contains raw data ingested from various sources info Like JSON files, operational databases, or Kafka Stream, for example Silver Silver tables provide a more refined view of our data info For example, data can be cleaned and filtered at this level note We can also join fields from various bronze tables to enrich our silver records Gold The gold table provide business-level aggregations, often used for reporting and dashboarding or even for machine learning    With this architecture, we incrementally improve the structure and the quality of data as it flows through each layer  ","version":"Next","tagName":"h2"},{"title":"Benefits with multi-hop architecture​","type":1,"pageTitle":"4.5 Multi-hop Architecture","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.5 Multihop Architecture#benefits-with-multi-hop-architecture","content":"  It is a simple data model that is easy to understand and implement It enables incremental ETL, that is Extract, Transform and Load data incrementally It can combine streaming and batch workloads in the same pipeline info Each stage can be configured as a batch or streaming job It can recreate your tables from raw data at any time ","version":"Next","tagName":"h2"},{"title":"1.1 Course Overview","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.1 Course Overview","content":"1.1 Course Overview This course covers 5 broad categories related to the exam topics: Databricks Lakehouse Platform ETL with Spark SQL and Python Incremental Data Processing Production Pipelines Data Governance","keywords":"","version":"Next"},{"title":"1.3 Get started with Community Edition","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.3 Get started with Community Edition","content":"1.3 Get started with Community Edition Go to https://www.databricks.com/try-databricks Fill the registration form Click Get started for free tip Databricks offers a 14-day free trial in your own cloud Sign up for the community edition by clicking on the link. Get started with Community Edition Solve the puzzle in order to prove that you are not a robot Go to your email and click on the received link to validate your email address Set the password of your new Databricks account. Confirm your new password and click Reset password Bookmark the page to access your workspace later tip You can always get to the page by going to the website https://community.cloud.databricks.com","keywords":"","version":"Next"},{"title":"3.5 Higher Order Functions and SQL UDFs","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.5 Higher Order Functions and SQL UDFs","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"3.5 Higher Order Functions and SQL UDFs","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.5 Higher Order Functions and SQL UDFs#introduction","content":"  In this notebook, we will explore higher order functions and user-defined functions also known as UDF We will continue using our bookstore dataset, which contains the 3 tables of customers, orders and books  ","version":"Next","tagName":"h2"},{"title":"Dataset setup​","type":1,"pageTitle":"3.5 Higher Order Functions and SQL UDFs","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.5 Higher Order Functions and SQL UDFs#dataset-setup","content":"  Let us start by copying this dataset info %run ../Includes/Copy-Datasets Let us take a look again on our orders table info SELECT * FROM orders As you can see here, the books column is of complex data type info In fact, it's an array of a struct type To work directly with such a complex datatype We need to use higher order functions  ","version":"Next","tagName":"h2"},{"title":"What are Higher Order Functions?​","type":1,"pageTitle":"3.5 Higher Order Functions and SQL UDFs","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.5 Higher Order Functions and SQL UDFs#what-are-higher-order-functions","content":"  Higher order functions allow you to work directly with hierarchical data like arrays and map type objects  ","version":"Next","tagName":"h2"},{"title":"How to use higher order functions?​","type":1,"pageTitle":"3.5 Higher Order Functions and SQL UDFs","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.5 Higher Order Functions and SQL UDFs#how-to-use-higher-order-functions","content":" ","version":"Next","tagName":"h2"},{"title":"Use the filter function to filter an array based on a condition​","type":1,"pageTitle":"3.5 Higher Order Functions and SQL UDFs","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.5 Higher Order Functions and SQL UDFs#use-the-filter-function-to-filter-an-array-based-on-a-condition","content":"  One of the most common higher order functions is the filter function which filters an array using a given lambda function In this example, we are creating a new column called multiple_copies, where we filter the books column to extract only those books that have a quantity greater or equal to 2 info It means they have been bought in multiple copies, 2 or more We are creating a new column called multiple_copies, where we have an array that contains only the filtered data info SELECT order_id, books, FILTER(books, i -&gt; i.quantity &gt;= 2) AS multiple_copies FROM orders As you can see, we are creating a lot of empty array in this new column In this case, it is useful to use a WHERE clause to show only non-empty array values in the return column We can accomplish that by using a subquery, which is a query within another query in order to apply the WHERE clause on the size of the returned column info SELECT order_id, books, FILTER(books, i -&gt; i.quantity &gt;= 2) AS multiple_copies FROM orders WHERE size(multiple_copies) &gt; 0;   ","version":"Next","tagName":"h3"},{"title":"Use the transform function to apply transformations to every single item in an array​","type":1,"pageTitle":"3.5 Higher Order Functions and SQL UDFs","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.5 Higher Order Functions and SQL UDFs#use-the-transform-function-to-apply-transformations-to-every-single-item-in-an-array","content":"  Our second higher order function is the transform function that is used to apply a transformation on all the items in an array and extract the transformed value In this example, for each book in the books array, we are applying a discount on the subtotal value info SELECT order_id, books, TRANSFORM ( books, b -&gt; CAST(b.subtotal * 0.8 AS INT) ) AS subtotal_after_discount FROM orders; As you can see, we created a new column containing an array of the transformed values for each element in the books array  ","version":"Next","tagName":"h3"},{"title":"How to create user-defined functions (UDFs)?​","type":1,"pageTitle":"3.5 Higher Order Functions and SQL UDFs","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.5 Higher Order Functions and SQL UDFs#how-to-create-user-defined-functions-udfs","content":"  Let us now talk about user-defined functions or UDFs, which allow you to register a custom combination of SQL logic as function in a database, making these methods reusable in any SQL query UDF functions leverage Spark SQL directly maintaining all the optimization of Spark when applying your custom logic to large datasets Here is an example of creating a user-defined function. At minimum, it requires a function name, optional parameters, the type to be returned, and some custom logic info CREATE OR REPLACE FUNCTION get_url( email STRING ) RETURNS STRING RETURN CONCAT(&quot;https://www.&quot;, split(email, &quot;@&quot;)[1]) Our function here is named get_url, that accepts an email address as an argument and return a value of type STRING. Here we are splitting the email into two parts based on the @ sign info CREATE OR REPLACE FUNCTION get_url( email STRING ) RETURNS STRING RETURN CONCAT(&quot;https://www.&quot;, split(email, &quot;@&quot;)[1])   ","version":"Next","tagName":"h2"},{"title":"How to use UDF functions​","type":1,"pageTitle":"3.5 Higher Order Functions and SQL UDFs","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.5 Higher Order Functions and SQL UDFs#how-to-use-udf-functions","content":"  Let us now start to using it. Here we are applying our UDF on the customer emails to get the URL address info SELECT email, get_url(email) AS domain FROM customers note User defined functions are permanent objects that are persisted to the database, so you can use them between different Spark Sessions and Notebooks With DESCRIBE FUNCTION command, we can see where it was registered and basic information about expected inputs and the expected return type info DESCRIBE FUNCTION get_url As you can see our function, it belongs to the default database and accept the email address as a string input, and return a string value We can get even more information by running DESCRIBE FUNCTION EXTENDED info DESCRIBE FUNCTION EXTENDED get_url For example, the Body field at the bottom shows the SQL logic used in the function itself  ","version":"Next","tagName":"h2"},{"title":"How to add more complex logic in UDF functions​","type":1,"pageTitle":"3.5 Higher Order Functions and SQL UDFs","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/ELT with Spark SQL and Python/3.5 Higher Order Functions and SQL UDFs#how-to-add-more-complex-logic-in-udf-functions","content":"  We can have more complex logic in our function. For example, here we are applying the standard SQL CASE WHEN statements in order to evaluate multiple condition statements within our function Here, for example, we are checking the email extension using the LIKE command in order to detect the category of the domain name and otherwise, we are reporting it as unknown extension info CREATE FUNCTION site_type(email STRING) RETURNS STRING RETURN CASE WHEN email LIKE &quot;%.com&quot; THEN &quot;Commerical business&quot; WHEN email LIKE &quot;%.org&quot; THEN &quot;Non-profit organizations&quot; WHEN email LIKE &quot;%.edu&quot; THEN &quot;Educational institution&quot; ELSE CONCAT( &quot;Unknown extension for domain: &quot;, SPLIT(email, &quot;@&quot;)[1] ) END; SELECT email, site_type(email) as domain_category FROM customers; As you can see UDF functions are really powerful  ","version":"Next","tagName":"h2"},{"title":"1.5 Exploring Workspace","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.5 Exploring Workspace","content":"1.5 Exploring Workspace Go to your workspace UI If you hover over the left sidebar, it will be expanded. You can click menu options in order to keep it expanded Tab name\tDescriptionRepos\tgit integration Data\tTo manage the databases and tables Compute\tTo create and manage the clusters Workflows\tTo deploy and orchestrate the jobs The New button is a shortcut that allows you to quickly create different resources like: Notebooks;Clusters;Jobs On the top bar, you can see the cloud provider on which this workspace is deployed note In our case, it's Microsoft Azure You have the search bar allowing you to search for notebooks, files, dashboards and more Go to the Workspace tab. From here you can see options for creating a notebook, library folder, or MLflow experiment You can also import some code files. And you can even export all the files in the workspace. From the workspace, go to Create and choose Folder Give the folder a name In a folder, you can also create other folders or any other asset Databricks provides different working environments for machine learning engineers and for data analysts You can access these personas simply by clicking on the drop menu here warning Notice that the community edition does not have the SQL persona. This is why it is recommended to use the full trial in your cloud instead of the community edition","keywords":"","version":"Next"},{"title":"1.6 Course Materials","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.6 Course Materials","content":"","keywords":"","version":"Next"},{"title":"Intro​","type":1,"pageTitle":"1.6 Course Materials","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.6 Course Materials#intro","content":"  We are going to see how to import the notebooks of this course into a Databricks workspace   ","version":"Next","tagName":"h2"},{"title":"Source code location​","type":1,"pageTitle":"1.6 Course Materials","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.6 Course Materials#source-code-location","content":"  The source code is available here on GitHub info https://github.com/derar-alhussein/Databricks-Certified-Data-Engineer-Associate  ","version":"Next","tagName":"h2"},{"title":"How to use the source code in Databricks​","type":1,"pageTitle":"1.6 Course Materials","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.6 Course Materials#how-to-use-the-source-code-in-databricks","content":"  We can easily clone this repository in a Databricks workspace using Databricks Repo Let us copy the repository URL, and switch to our Databricks workspace In our Databricks workspace, we go to the Repos tab in the left side bar info Databricks Repos provides source control for your data projects by integrating with Git providers Click on Add Repo Paste in the URL of the Git repository note The git provider will be filled automatically and also the repository name Click Submit info With this source code, you can follow along with me during this course and recreate the same solution in your Databricks workspace ","version":"Next","tagName":"h2"},{"title":"4.3 Incremental Data Ingestion","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.3 Incremental Data Ingestion","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"4.3 Incremental Data Ingestion","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.3 Incremental Data Ingestion#introduction","content":"  We will talk about incremental data ingestion from files in Databricks We will talk about two methods: COPY INTO command; Auto Loader  ","version":"Next","tagName":"h2"},{"title":"What is incremental data ingestion?​","type":1,"pageTitle":"4.3 Incremental Data Ingestion","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.3 Incremental Data Ingestion#what-is-incremental-data-ingestion","content":"  Incremental data ingestion is the ability to load data from new files that have been encountered since the last ingestion Each time we run our data pipeline, we don't need to reprocess the files we have processed before. We need to process only the new arriving data files Databricks provides 2 mechanisms for incrementally and efficiently processing new data files as they arrive in a storage location: COPY INTO SQL command Auto Loader  ","version":"Next","tagName":"h2"},{"title":"COPY INTO​","type":1,"pageTitle":"4.3 Incremental Data Ingestion","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.3 Incremental Data Ingestion#copy-into","content":"  COPY INTO is a SQL command that allows a user to load data from a file location into a Delta table The COPY INTO command loads data idempotently and incrementally info Idempotently means each time you run the command, it will load only the new files from the source location while the files that have been loaded before are simply skipped The command is pretty simple. COPY INTO a target table from a specific source location We specify the format of the source file to load, for example, CSV or Parquet and any related format options In adittion to any option to control the operation of the COPY INTO command Here we are loading from a CSV file, having a header and a specific delimiter info COPY INTO my_table FROM '/path/to/files' FILEFORMAT=CSV FORMAT_OPTIONS ( 'delimiter'='|', 'header'='true' ) COPY_OPTIONS ( 'mergeSchema'='true' ) In the COPY_OPTIONS, we are specifying that the schema can be evolved according to the incoming data info COPY INTO my_table FROM '/path/to/files' FILEFORMAT=CSV FORMAT_OPTIONS ( 'delimiter'='|', 'header'='true' ) COPY_OPTIONS ( 'mergeSchema'='true' )   ","version":"Next","tagName":"h2"},{"title":"Auto Loader​","type":1,"pageTitle":"4.3 Incremental Data Ingestion","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.3 Incremental Data Ingestion#auto-loader","content":"  The second method to load data incrementally from files is Auto Loader, which uses structured streaming in Spark to efficiently process new data files as they arrive in a storage location You can use Auto Loader to load billions of files into a table Auto Loader can scale to support real time ingestion of millions of files per hour  ","version":"Next","tagName":"h2"},{"title":"Auto Loader Checkpointing​","type":1,"pageTitle":"4.3 Incremental Data Ingestion","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.3 Incremental Data Ingestion#auto-loader-checkpointing","content":"  Auto Loader uses checkpointing to track the ingestion process and to store metadata of the discovered files Auto Loader ensures that data files are processed exactly once Auto Loader can resume from where it left off if a failure occurs With Auto Loader, we use the readStream and writeStream methods Auto Loader has a specific format of StreamReader called cloudFiles. And in order to specify the format of the source files, we use simply cloudFiles.format option info spark.readStream.format( &quot;cloudFiles&quot; ).option( &quot;cloudFiles.format&quot;, &lt;source_format&gt; ).load( &quot;/path/to/files&quot; ).writeStream.option( &quot;checkpointLocation&quot;, &lt;checkpoint_directory&gt; ).table( &lt;table_name&gt; ) The location of the source files is specified with the load function info Auto Loader will detect new files as they arrive in this location and queue them for ingestion spark.readStream.format( &quot;cloudFiles&quot; ).option( &quot;cloudFiles.format&quot;, &lt;source_format&gt; ).load( &quot;/path/to/files&quot; ).writeStream.option( &quot;checkpointLocation&quot;, &lt;checkpoint_directory&gt; ).table( &lt;table_name&gt; ) We write the data into a target table using the StreamWriter, where you provide the location to store the checkpointing information info spark.readStream.format( &quot;cloudFiles&quot; ).option( &quot;cloudFiles.format&quot;, &lt;source_format&gt; ).load( &quot;/path/to/files&quot; ).writeStream.option( &quot;checkpointLocation&quot;, &lt;checkpoint_directory&gt; ).table( &lt;table_name&gt; ) Auto Loader can automatically configure the schema of your data. It can detect any update to the fields of the source dataset info The inferred schema can be stored to a location to be used later Use the option cloudFiles.schemaLocation to provide the location where Auto Loader can store the schema info This location could be simply the same as the checkpoint location spark.readStream.format( &quot;cloudFiles&quot; ).option( &quot;cloudFiles.format&quot;, &lt;source_format&gt; ).option( &quot;cloudFiles.schemaLocation&quot;, &lt;schema_directory&gt; ).load( &quot;/path/to/files&quot; ).writeStream.option( &quot;checkpointLocation&quot;, &lt;checkpoint_directory&gt; ).option( &quot;mergeSchema&quot;, &quot;true&quot; ).table( &lt;table_name&gt; )   ","version":"Next","tagName":"h3"},{"title":"When to use Auto Loader vs COPY INTO command?​","type":1,"pageTitle":"4.3 Incremental Data Ingestion","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Incremental Data Processing/4.3 Incremental Data Ingestion#when-to-use-auto-loader-vs-copy-into-command","content":"  Use the COPY INTO command to ingest thousands of files Use Auto Loader to ingest millions of files or more over time Auto Loader can split the processing into multiple batches so it is more efficient at scale info Databricks recommends to use Auto Loader as general best practice when ingesting data from a cloud-object storage ","version":"Next","tagName":"h2"},{"title":"1.7 Creating a Cluster","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.7 Creating Cluster","content":"","keywords":"","version":"Next"},{"title":"Walkthrough - How to setup a single-node cluster​","type":1,"pageTitle":"1.7 Creating a Cluster","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.7 Creating Cluster#walkthrough---how-to-setup-a-single-node-cluster","content":"  The next step in setting up your environment is to create a cluster.   ","version":"Next","tagName":"h2"},{"title":"Navigate to the Compute menu​","type":1,"pageTitle":"1.7 Creating a Cluster","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.7 Creating Cluster#navigate-to-the-compute-menu","content":"  Navigate to Compute in the left side bar From here, we can create and manage our clusters info Remember, a cluster is a set of nodes or computers working together like a single entity. It consists of a master node called the Driver and some other worker nodes. The Driver node is responsible for coordinating the workers and their parallel execution of tasks Under all-purpose compute, click Create compute Click on the default name in order to change it. Name your cluster as Demo Cluster, for example Leave the policy Unrestricted to create fully configurable cluster Your cluster could be multi-node, that is having multiple workers, or simply a single node info A single node has no workers and run Spark jobs on the driver node For this course, a single node is enough. However, let us continue to see how to configure a multi-node cluster  ","version":"Next","tagName":"h3"},{"title":"Overview of multi-node cluster configuration options​","type":1,"pageTitle":"1.7 Creating a Cluster","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.7 Creating Cluster#overview-of-multi-node-cluster-configuration-options","content":"  For the access mode, you can allow your cluster to be shared by multiple users info However, only SQL and Python workloads will be supported Choose Single user if you are the only one to use this cluster  ","version":"Next","tagName":"h2"},{"title":"Select the Databricks Runtime version (VM image)​","type":1,"pageTitle":"1.7 Creating a Cluster","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.7 Creating Cluster#select-the-databricks-runtime-version-vm-image","content":"  You need to select the Databricks runtime version info Databricks runtime is the virtual machine's image that comes with pre-installed libraries, which has a specific version of Spark, Scala and other libraries Choose 11.3 LTS, which is the latest version at the time of recording this course note You can choose to activate Photon, which is a vectorized query engine developed in C++ to enhance Spark performance  ","version":"Next","tagName":"h3"},{"title":"Specify the configuration of the worker nodes​","type":1,"pageTitle":"1.7 Creating a Cluster","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.7 Creating Cluster#specify-the-configuration-of-the-worker-nodes","content":" Specify the requirements for the worker nodes​   You can go ahead and select the configuration of your worker nodes info These are different virtual machines sizes provided by your cloud provider Depending on your requirements of memory cores and hard disk, you can select the configuration Let's keep the default one   Specify the number of worker nodes​   You can select the number of workers you need for your cluster   ","version":"Next","tagName":"h3"},{"title":"Enable auto scaling​","type":1,"pageTitle":"1.7 Creating a Cluster","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.7 Creating Cluster#enable-auto-scaling","content":"  If you choose to enable auto scaling, then provide a range for the number of workers info These allow Databricks to resize your cluster automatically within this range Otherwise disable auto scaling and use a fixed number of workers Let me select 3 here   ","version":"Next","tagName":"h3"},{"title":"Specify the configuration for the driver node​","type":1,"pageTitle":"1.7 Creating a Cluster","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.7 Creating Cluster#specify-the-configuration-for-the-driver-node","content":"  You can now select the configuration for the Driver node or simply keep it the same as the worker   ","version":"Next","tagName":"h3"},{"title":"Enable auto-termination​","type":1,"pageTitle":"1.7 Creating a Cluster","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.7 Creating Cluster#enable-auto-termination","content":"  You can enable auto-termination of the cluster by providing the number of minutes Let's say 30 minutes. That is, if there is no activity for 30 minutes, the cluster will auto-terminate   ","version":"Next","tagName":"h3"},{"title":"Check the cluster configuration summary​","type":1,"pageTitle":"1.7 Creating a Cluster","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.7 Creating Cluster#check-the-cluster-configuration-summary","content":"  On the right, you can see a summary of your cluster configuration. Here you can see the number of DBUs info DBU stand for Databricks Unit and it is a unit of a processing capability per hour note Each configuration tells you how much DBUs would be consumed if a virtual machine runs for an hour and then you pay for each DBU consumed info For example, if we have less number of workers or even a single node cluster, we will have less DBUs  ","version":"Next","tagName":"h3"},{"title":"Walkthrough - How to setup a single-node cluster [Continued]​","type":1,"pageTitle":"1.7 Creating a Cluster","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.7 Creating Cluster#walkthrough---how-to-setup-a-single-node-cluster-continued","content":"  Select Single Node cluster Click Confirm info With a single node cluster, we are going to consume less DBUs Hit the Create button to finish creating our cluster info Azure will now go ahead and provision the required virtual machine with a specific configuration and libraries as specified by Databricks runtime Our cluster is now up and running To access your cluster at any time, you can simply navigate to Compute in the left side bar You can see your clusters are listed here with its current status Running or Terminated. You can even edit the cluster by clicking on its name. Then click Edit info Remember, changing the cluster configuration may require a restart of the cluster Let's cancel this In the cluster page, you can notice two things  The event log shows all the events that have happened with the cluster info For example, when the cluster was created or terminated. This helps to track the activity on a cluster In the driver log, you will get the log generated within the cluster notebooks and libraries    Let's now terminate our cluster ","version":"Next","tagName":"h2"},{"title":"1.4 Free trial on Azure","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.4 Free trial on Azure","content":"","keywords":"","version":"Next"},{"title":"How to sign up for a free trial with Databricks on Microsoft Azure​","type":1,"pageTitle":"1.4 Free trial on Azure","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.4 Free trial on Azure#how-to-sign-up-for-a-free-trial-with-databricks-on-microsoft-azure","content":" ","version":"Next","tagName":"h2"},{"title":"How to sign up for Microsoft Azure Free Tier​","type":1,"pageTitle":"1.4 Free trial on Azure","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.4 Free trial on Azure#how-to-sign-up-for-microsoft-azure-free-tier","content":"  If you have a cloud account, you can use it to register for a 14-day Databricks full trial info A full trial in your cloud is recommended since its included production-grade functionalities that are not available in the limited community edition [] Microsoft offers a free tier for 12 months. This free tier allows you to explore and try out Azure services free of charge and this up to specified limits for each service Go to azure.microsoft.com/free  ","version":"Next","tagName":"h3"},{"title":"How to deploy Azure Databricks​","type":1,"pageTitle":"1.4 Free trial on Azure","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.4 Free trial on Azure#how-to-deploy-azure-databricks","content":"  You can create a Databricks workspace directly through Azure Portal Search for Databricks and choose from services Azure Databricks Click on the Create button Choose your subscription Create a resource group with the name DatabricksDemoRG since all resources needs to be within a resource group Add a friendly name for the workspace. Let us keep it as DemoWorkspace Select a location. Choose West Europe For the pricing tier, there are two main options Premium tier includes all features of standard tier as well as role-based access control Free trial Choose the 14-day free Premium option Click Review + Create Click Create  ","version":"Next","tagName":"h3"},{"title":"Review deployed resources in Azure Portal​","type":1,"pageTitle":"1.4 Free trial on Azure","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.4 Free trial on Azure#review-deployed-resources-in-azure-portal","content":"  Click on Go to Resource Before launching the workspace, take a look at the managed resource group. This resource group has been created along with the workspace in our subscription Navigate to the resource group to see the following 3 resources that are part of the data plane: A virtual network A network security group for managing the inbound and outbound trafficA storage account which is the underlying storage for DBFS Go back to our resource and click Launch workspace note Notice here that it is using as your Active Directory single sign on to login to Databricks platform Our current data project is building data pipelines, ETL and streaming Click Finish  ","version":"Next","tagName":"h3"},{"title":"1.2 What is Databricks?","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.2 What is Databricks","content":"","keywords":"","version":"Next"},{"title":"What is Databricks?​","type":1,"pageTitle":"1.2 What is Databricks?","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.2 What is Databricks#what-is-databricks","content":"  Databricks is a multi-cloud lakehouse platform based on Apache Spark  ","version":"Next","tagName":"h2"},{"title":"What is a lakehouse?​","type":1,"pageTitle":"1.2 What is Databricks?","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.2 What is Databricks#what-is-a-lakehouse","content":"  A lake house is a unified analytics platform that combines the best elements of data lakes and data warehouses. Deliver the openness, flexibility and machine learning support of data lakes With the reliability, strong governance and performance of data warehouses In a lakehouse architecture, you work on engineering, analytics and AI all in one platform.  ","version":"Next","tagName":"h2"},{"title":"Architecture of Databricks Lakehouse​","type":1,"pageTitle":"1.2 What is Databricks?","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.2 What is Databricks#architecture-of-databricks-lakehouse","content":"  The Databricks Lakehouse architecture is divided into 3 layers:  The cloud service The runtime The workspace  ","version":"Next","tagName":"h2"},{"title":"Cloud service​","type":1,"pageTitle":"1.2 What is Databricks?","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.2 What is Databricks#cloud-service","content":"  Databricks is available on the following cloud providers: Microsoft AzureAmazon Web ServicesGoogle Cloud  ","version":"Next","tagName":"h3"},{"title":"Runtime​","type":1,"pageTitle":"1.2 What is Databricks?","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.2 What is Databricks#runtime","content":"  The Databricks runtime is a set of core components like: Apache SparkDelta LakeOther system libraries The infrastructure of the cloud provider is used to provision virtual machines or nodes of a cluster which are pre-installed with the Databricks runtime  ","version":"Next","tagName":"h3"},{"title":"Workspace​","type":1,"pageTitle":"1.2 What is Databricks?","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.2 What is Databricks#workspace","content":"  The Databricks workspace is on top of the cloud service and runtime. The workspace allows you to interactively implement and run your data engineering, analytics and AI workloads.  ","version":"Next","tagName":"h3"},{"title":"How Databricks resources are deployed within your cloud provider​","type":1,"pageTitle":"1.2 What is Databricks?","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.2 What is Databricks#how-databricks-resources-are-deployed-within-your-cloud-provider","content":"  Databricks consists of 2 high-level components:   Control plane Data plane  ","version":"Next","tagName":"h2"},{"title":"Control plane​","type":1,"pageTitle":"1.2 What is Databricks?","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.2 What is Databricks#control-plane","content":"  Control plane components are deployed when you create a Databricks workspace.   The control plane components are services in Databricks such as:  The Databricks UI The Cluster Manager The Workflow Service The Notebooks  ","version":"Next","tagName":"h3"},{"title":"Data plane​","type":1,"pageTitle":"1.2 What is Databricks?","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.2 What is Databricks#data-plane","content":"  If you choose Azure as a cloud provider, a storage account will be deployed in your Azure subscription as part of the Data plane. The storage account will be used as the Databricks File System or DBFS When you set up a Spark cluster, additional virtual machines will be deployed as part of the Data plane tip The compute and storage components will be in your own cloud account. Databricks will provide you with the tools you need to use and control your infrastructure from the Databricks UI  ","version":"Next","tagName":"h3"},{"title":"Supported features of Databricks​","type":1,"pageTitle":"1.2 What is Databricks?","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.2 What is Databricks#supported-features-of-databricks","content":"  Data is distributed and processed in-memory by multiple nodes in a cluster, because Databricks is based on Apache Spark. Databricks supports all the languages that are supported by Spark, which are: Scala Python SQL R Java  Databricks also suppports batch- and stream processing in Spark   Databricks processes all types of data:   StructuredSemi-structuredUnstructured  ","version":"Next","tagName":"h2"},{"title":"Databricks File System (DBFS)​","type":1,"pageTitle":"1.2 What is Databricks?","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.2 What is Databricks#databricks-file-system-dbfs","content":"  Databricks offers native-support of a distributed file system called Databricks File System (DBFS) info File systems are used to persist data and files DBFS comes pre-installed on a cluster in Databricks note DBFS is an abstraction layer that uses the underlying cloud storage to persist data. info If you create a file in your cluster and store it in DBFS. This file is persisted in the underlying cloud storage. Azure storage account Amazon S3 buckets Even after the cluster is terminated, all the data is saved in your cloud storage ","version":"Next","tagName":"h2"},{"title":"1.9 Databricks Repos","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.9 Databricks Repos","content":"","keywords":"","version":"Next"},{"title":"How to locate settings in Databricks to setup git integration?​","type":1,"pageTitle":"1.9 Databricks Repos","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.9 Databricks Repos#how-to-locate-settings-in-databricks-to-setup-git-integration","content":"  Under your username in the top bar, choose User settings from the menu Go to the git integration tab note As you can see from the git provider dropdown menu, Databricks has native integration with several git providers like GitHub and Azure DevOps To connect Databricks to one of the git providers, you need your git service username and a personal access token info In this demo we will use GitHub, so let us switch there to get this information  ","version":"Next","tagName":"h2"},{"title":"How to locate the personal access token in GitHub?​","type":1,"pageTitle":"1.9 Databricks Repos","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.9 Databricks Repos#how-to-locate-the-personal-access-token-in-github","content":"  In GitHub, click on your avatar in the top bar info Here it displays the username for the account For the token, let us select Settings Scroll to Developer settings. From the left choose Personal Access Token Choose Tokens (classic) Click generate a new token, classic token In the new personal access token page, specify a note describing the purpose of the token and an expiration date Choose Repo scope and scroll to click Generate token Here is our token. Let us copy this string now  ","version":"Next","tagName":"h2"},{"title":"How to connect Databricks to GitHub with the username and personal access token?​","type":1,"pageTitle":"1.9 Databricks Repos","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.9 Databricks Repos#how-to-connect-databricks-to-github-with-the-username-and-personal-access-token","content":"  Let us switch back to our Databricks workspace With GitHub selected as GitHub provider. Let us fill in our GitHub username and personal token Click Save  ","version":"Next","tagName":"h2"},{"title":"How to create a new repository in GitHub?​","type":1,"pageTitle":"1.9 Databricks Repos","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.9 Databricks Repos#how-to-create-a-new-repository-in-github","content":"  Each Databricks repo maps to a Git repository In order to set up a repo, let's first create a repository in GitHub Go to the main GitHub landing page and click New to create a repository Specify a name for the repository. Choose Private. And enable Add a README file Click Create repository Copy the HTTPS URL of this newly created repository Switch back to our Databricks workspace From the left side bar, go to the Repo tab. Click Add Repo Paste here the URL of the Git repository info Notice that the git provider and the repository name has been filled automatically Click Submit Here is our new repo, which contains a local copy of the remote git repository  ","version":"Next","tagName":"h2"},{"title":"How to create a new branch in Databricks?​","type":1,"pageTitle":"1.9 Databricks Repos","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.9 Databricks Repos#how-to-create-a-new-branch-in-databricks","content":"  Let us create a new branch named development Click on the branch name to open the Repo dialog Click Create branch Specify the branch name Click Create note Once created it automatically becomes the current branch With the development branch created and selected, we can now begin making changes as needed Let us create a folder my folder, for example Add a notebook You can also import a notebook. From file or URL  ","version":"Next","tagName":"h2"},{"title":"How to clone an existing notebook from the Workspace?​","type":1,"pageTitle":"1.9 Databricks Repos","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.9 Databricks Repos#how-to-clone-an-existing-notebook-from-the-workspace","content":"  Or clone an existing notebook from the workspace into the repo Go to the Workspace tab Click Next to our Notebook Basics created in the last lecture and select Clone Choose my folder and click Clone If we go back to Databricks Repo, we see that our new folder containing two notebooks Let us now push these changes to our remote repository Click on the branch name info Here we see all our changes Let us first write a commit message Click Commit &amp; push If we switch back now our main branch, as expected, we don't see here the new folder and notebooks we created in the development branch  ","version":"Next","tagName":"h2"},{"title":"How to create a pull request in GitHub?​","type":1,"pageTitle":"1.9 Databricks Repos","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.9 Databricks Repos#how-to-create-a-pull-request-in-github","content":"  To be able to pull these changes into the main branch, let us create a pull request in GitHub From here we go to the development branch Click Contribute, Open pull request Click Create pull request. Then Merge pull request. Confirm merge  ","version":"Next","tagName":"h2"},{"title":"How to pull a branch from GitHub to Databricks?​","type":1,"pageTitle":"1.9 Databricks Repos","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.9 Databricks Repos#how-to-pull-a-branch-from-github-to-databricks","content":"  Let us switch back to our Databricks Workspace to see how to pull this in Databricks repos Click on the branch name to open the repos dialog With the main branch selected, click the Pull button on the right info Remember pulling regularly is important to avoid conflicts especially when multiple developers are developing on the same branch ","version":"Next","tagName":"h2"},{"title":"intro","type":0,"sectionRef":"#","url":"/docs/intro","content":"intro","keywords":"","version":"Next"},{"title":"1.8 Notebook Fundamentals","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.8 Notebook Fundamentals","content":"","keywords":"","version":"Next"},{"title":"Access the filesystem with dbutils​","type":1,"pageTitle":"1.8 Notebook Fundamentals","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Introduction/1.8 Notebook Fundamentals#access-the-filesystem-with-dbutils","content":"  Another way to deal with filesystem operations to use Databricks utilities, also known as dbutils info dbutils provides a number of utility commands for configuring and interacting with the environment note To get some help for each utility, you can use dbutils.help() function As you can see with the dbutils, you can interact with different services and tools like: credentials, fs for file system, secrets and widgets info dbutils.fs allows you to interact with the Databricks file system or DBFS To get some help on this. info dbutils.fs.help() From here we can see that the fs has a number of available commands, like copying and removing files In addition, we also have the ls command that allows us to list the content of a directory. Let us try this note Databricks also supports auto-completion using the Tab key Let us hit Tab in this case and select ls, click enter. And specify the folder path, in our case /databricks-datasets info dbutils.fs.ls('/databricks-datasets') The question for the file system operation, should we use dbutils or the %fs magic command we saw previously? In fact, dbutils is more useful than the %fs magic command since you can use dbutils as part of python code info You can get the list of files for example in a variable and do some logic with it in Python. &quot;Example: List files in directory&quot; files = dbutils.fs.ls('/databricks-datasets') As you can see, the output is very messy and hard to read. Instead, you can display it in a much better way using the display function info files = dbutils.fs.ls('/databricks-datasets') display(files) Here we see the output is rendered in a tabular format with some fields like: path;filename;size;modification time With the display function, you can also download data as CSV file and render the result as a graph info However, the display function is limited to preview only 1000 records note When running SQL queries from cells, results will be always displayed in a tabular format like here, so there is no need to use the display function with SQL. At the end of the day, you may want to download your notebook. You can just click on the file menu, then Export and you choose IPython Notebook We can also export a folder that contains multiple notebooks from the Workspace tab. If we click next to the folder, we have the option to export it as DBC archive note The DBC or the Databricks Cloud file is a zip file that contains a collection of directories and notebooks This file can be uploaded into any Databricks workspace to move or share notebooks To import a notebook or a collection of notebooks in DBC file, simply click here on the down arrow and choose Import In Databricks, you can access the revision history of all the changes being made on a notebook Click on the Last Edit link. From here, you can choose any of the auto-saved versions Click Restore this revision  ","version":"Next","tagName":"h2"},{"title":"5.2 Change Data Capture","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.2 Change Data Capture","content":"","keywords":"","version":"Next"},{"title":"Change Data Capture​","type":1,"pageTitle":"5.2 Change Data Capture","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.2 Change Data Capture#change-data-capture","content":"  Change Data Capture or CDC refers to the process of identifying and capturing changes made to data in the data source, and then delivering those changes to the target  ","version":"Next","tagName":"h2"},{"title":"Row-level changes​","type":1,"pageTitle":"5.2 Change Data Capture","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.2 Change Data Capture#row-level-changes","content":"  Those changes could be obviously new records to be inserted from the source to the target Updated records in the source that need to be reflected in the target Deleted records in the source that must be deleted in the target  ","version":"Next","tagName":"h3"},{"title":"CDC Feed​","type":1,"pageTitle":"5.2 Change Data Capture","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.2 Change Data Capture#cdc-feed","content":"  Changes are logged at the source as events that contain both the data of the records along with metadata information info This metadata indicates whether the specified record was inserted, updated or deleted in addition to a version number or timestamp indicating the order in which changes happened Here's an example of CDC events need to be applied on our target table info France, for example has two records, so we need to apply the most recent change. Canada need to be deleted, so we don't need to send all the data of the record. Lastly, USA and India are new records need to be inserted Country ID\tCountry\tVaccination Rate\tsequence\toperationFR\tFrance\t0.74\t2022-11-01 07:00\tUPDATE FR\tFrance\t0.75\t2022-11-01 08:00\tUPDATE CA 2022-11-01 00:00\tDELETE US\tUSA\t0.5\t2022-11-01 00:00\tINSERT IN\tIndia\t0.66\t2022-11-01 00:00\tINSERT Here we see the changes applied on our target table We don't see the record of Canada as it has been deleted info Such a CDC feed could be received from the source as a data stream or simply in JSON files, for example Country ID\tCountry\tVaccination RateFR\tFrance\t0.75 US\tUSA\t0.5 IN\tIndia\t0.66 Delta Live Tables supports CDC feed processing using the APPLY CHANGES INTO command info APPLY CHANGES INTO LIVE.target_table FROM STREAM( LIVE.cdc_feed_table ) KEYS ( key_field ) APPLY AS DELETE WHEN operation_field = &quot;DELETE&quot; SEQUENCE BY sequence_field COLUMNS * It is the target table into which the changes need to be applied. FROM a CDC feed table specified as a streaming source. KEYS is where you identify the primary key fields. If the key exists in the target table, the record will be updated info APPLY CHANGES INTO LIVE.target_table FROM STREAM( LIVE.cdc_feed_table ) KEYS ( key_field ) APPLY AS DELETE WHEN operation_field = &quot;DELETE&quot; SEQUENCE BY sequence_field COLUMNS * With the APPLY AS DELETE WHEN condition, you specify that records where the operation field is DELETE should be deleted info APPLY CHANGES INTO LIVE.target_table FROM STREAM( LIVE.cdc_feed_table ) KEYS ( key_field ) APPLY AS DELETE WHEN operation_field = &quot;DELETE&quot; SEQUENCE BY sequence_field COLUMNS * SEQUENCE BY, where do you specify the sequence field for ordering how operation should be applied info APPLY CHANGES INTO LIVE.target_table FROM STREAM( LIVE.cdc_feed_table ) KEYS ( key_field ) APPLY AS DELETE WHEN operation_field = &quot;DELETE&quot; SEQUENCE BY sequence_field COLUMNS * You indicate the list of fields that should be added to the target table info APPLY CHANGES INTO LIVE.target_table FROM STREAM( LIVE.cdc_feed_table ) KEYS ( key_field ) APPLY AS DELETE WHEN operation_field = &quot;DELETE&quot; SEQUENCE BY sequence_field COLUMNS * note Note here the target Delta Live Table need to be already created before executing the APPLY CHANGES INTO command  ","version":"Next","tagName":"h2"},{"title":"Features of the APPLY CHANGES INTO command​","type":1,"pageTitle":"5.2 Change Data Capture","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.2 Change Data Capture#features-of-the-apply-changes-into-command","content":"  It automatically orders late arriving records using the user-provided sequencing key info This pattern ensures that if any records arrive out of order, down stream result can be properly re-computed to reflect the updates It also ensures that when records are deleted from a source table, these values are no longer reflected in tables later in the pipeline The default behavior for insert and update operation is to upsert the CDC events into the target table info That means it updates any rows in the target table that match the specified key or insert new records when a matching record does not exist in the target table Optional handling for delete events can be specified with the APPLY AS DELETE WHEN condition You can specify one or many fields as the primary key for a table The EXCEPT keyword can be added to specify columns to ignore You can choose whether to store records as slowly changing dimension, type 1 or type 2 APPLY CHANGES INTO defaults to creating a type 1 slowly changing dimension table, meaning that each unique key will have at most one record, and that the updates would overwrite the original information  ","version":"Next","tagName":"h2"},{"title":"Disadvantage of APPLY CHANGES INTO​","type":1,"pageTitle":"5.2 Change Data Capture","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.2 Change Data Capture#disadvantage-of-apply-changes-into","content":"  Since data is being updated and delete in the target table, this breaks the append-only requirements for streaming table sources info That means we will no longer be able to use this updated table as a streaming source later in the next layer ","version":"Next","tagName":"h2"},{"title":"5.3 Processing CDC Feed with DLT (Hands On)","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.3 Processing CDC Feed with DLT (Hands","content":"","keywords":"","version":"Next"},{"title":"Creating the bronze table layer​","type":1,"pageTitle":"5.3 Processing CDC Feed with DLT (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.3 Processing CDC Feed with DLT (Hands#creating-the-bronze-table-layer","content":"  In this notebook, we start by creating a bronze table to ingest books CDC feed We are using auto loader to load the JSON files incrementally info CREATE OR REFRESH STREAMING LIVE TABLE books_bronze COMMENT &quot;The raw books data, ingested from CDC feed&quot; AS SELECT * FROM cloud_files( &quot;${datasets.path}/books-cdc&quot;, &quot;json&quot; )   ","version":"Next","tagName":"h2"},{"title":"Creating the silver table layer​","type":1,"pageTitle":"5.3 Processing CDC Feed with DLT (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.3 Processing CDC Feed with DLT (Hands#creating-the-silver-table-layer","content":"  We are creating the silver table info This is our target table into which the changes from the CDC feed will be applied We start by declaring the table. Since APPLY CHANGES INTO requires the target table to be declared in a separate statement With a target table created, we can write the APPLY CHANGES INTO command In this command, we specify the table books_silver as the target table The table books_bronze as the streaming source of our CDC feed info CREATE OR REFRESH STREAMING LIVE TABLE books_silver; APPLY CHANGES INTO LIVE.books_silver FROM STREAM(LIVE.books_bronze) KEYS (book_id) APPLY AS DELETE WHEN row_status = &quot;DELETE&quot; SEQUENCE BY row_time COLUMNS * EXCEPT ( row_status, row_time ) Then we identify the book_id as the primary key If the key exists in the target table, the record will be updated. If not it will be inserted info CREATE OR REFRESH STREAMING LIVE TABLE books_silver; APPLY CHANGES INTO LIVE.books_silver FROM STREAM(LIVE.books_bronze) KEYS (book_id) APPLY AS DELETE WHEN row_status = &quot;DELETE&quot; SEQUENCE BY row_time COLUMNS * EXCEPT ( row_status, row_time ) We specify that records where the row status is DELETE should be deleted from the target table info CREATE OR REFRESH STREAMING LIVE TABLE books_silver; APPLY CHANGES INTO LIVE.books_silver FROM STREAM(LIVE.books_bronze) KEYS (book_id) APPLY AS DELETE WHEN row_status = &quot;DELETE&quot; SEQUENCE BY row_time COLUMNS * EXCEPT ( row_status, row_time ) We specify the row_time field for ordering the operations info CREATE OR REFRESH STREAMING LIVE TABLE books_silver; APPLY CHANGES INTO LIVE.books_silver FROM STREAM(LIVE.books_bronze) KEYS (book_id) APPLY AS DELETE WHEN row_status = &quot;DELETE&quot; SEQUENCE BY row_time COLUMNS * EXCEPT ( row_status, row_time ) We indicate that all books fields should be added to the target table except the operational columns: row_status and row_time info CREATE OR REFRESH STREAMING LIVE TABLE books_silver; APPLY CHANGES INTO LIVE.books_silver FROM STREAM(LIVE.books_bronze) KEYS (book_id) APPLY AS DELETE WHEN row_status = &quot;DELETE&quot; SEQUENCE BY row_time COLUMNS * EXCEPT ( row_status, row_time )   ","version":"Next","tagName":"h2"},{"title":"Creating the gold table layer​","type":1,"pageTitle":"5.3 Processing CDC Feed with DLT (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.3 Processing CDC Feed with DLT (Hands#creating-the-gold-table-layer","content":"  In the gold layer, we define a simple aggregate query to create a live table from the data in our books_silver table note Notice here that this is not a streaming table Since data is being updated and deleted from our books_silver table, it is no more valid to be a streaming source for this new table info Remember, streaming sources must be append-only tables CREATE LIVE TABLE author_counts_state COMMENT &quot;Number of books per author&quot; AS SELECT author, COUNT(*) AS books_count, CURRENT_TIMESTAMP() AS updated_time FROM LIVE.books_silver GROUP BY author In the CDC pipeline, we can also define views To define a view simply replace table with the VIEW keyword info These views are temporary views scoped to the pipeline they are a part of, so they are not persisted to the metastore Views can still be used to enforce data quality and metrics for views will be collected and reported as they would be for tables  ","version":"Next","tagName":"h2"},{"title":"How to join and reference tables across notebooks​","type":1,"pageTitle":"5.3 Processing CDC Feed with DLT (Hands On)","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.3 Processing CDC Feed with DLT (Hands#how-to-join-and-reference-tables-across-notebooks","content":"  Here we see how we can join and reference tables across notebooks We are joining our books_silver table to the orders_cleaned table, which we created in another notebook in the last lecture Since the API supports scheduling multiple notebooks as part of a single pipeline, configuration code in any notebook can reference tables and views created in any other notebook info You can think of the scope of the schema referenced by the LIVE keyword to be at the DLT pipeline level, rather than the individual notebook CREATE LIVE VIEW books_sales AS SELECT b.title, o.quantity FROM ( SELECT *, EXPLODE(books) AS books FROM LIVE.orders_cleaned ) AS o INNER JOIN LIVE.books_silver AS b ON o.book.book_id = b.book_id; Let us now go to the pipeline we created in the last lecture to add this new notebook Here is our demo bookstore pipeline we created in the last lecture Click on the Settings button From here, click Add Notebook library Click Browse Go to Repo and choose the Books Pipeline notebook Click Save As you can see in the pipeline details, this pipeline is now referencing 2 notebooks instead of one Let us now click Start to run our updated pipeline info If you have any issue running this pipeline, try to do a full refresh in order to run your pipeline from scratch It means the attempts to clear all data from each table and then load all data from the streaming sources For now, let us click Start In addition to tables you created in the last lecture, we see our new books tables. And the view books_sales that join the two pipelines tables together ","version":"Next","tagName":"h2"},{"title":"5.5 Databricks SQL","type":0,"sectionRef":"#","url":"/docs/Databricks Certified Data Engineer Associate - Preparation/Production Pipelines/5.5 Databricks SQL","content":"5.5 Databricks SQL In this video, we will explore Databricks SQL During all previous videos, we were working in the Data Science &amp; Engineering workspace In order to work with Databricks SQL, we need to switch to the SQL persona on top of the sidebar Databricks SQL, also known as DBSQL is a data warehouse that allows you to run all your SQL and BI applications at scale and with a unified governance model As you can see in the sidebar, you have new options like SQL editor, Queries, Dashboards, Alerts In addition to Data Explorer and SQL warehouses Let us start by clicking on the SQL warehouse tab From here, we can create our first SQL warehouse info A SQL warehouse is the compute power of Databricks SQL note It is a SQL engine or endpoint based on the Spark cluster Click on Create SQL Warehouse to configure a new SQL engine Give it a name, say demo warehouse Set the cluster size to 2x small For this demo, leave all other options as default and click Create If it shows you a menu to manage the permissions for the SQL warehouse, leave all other options as default and click Close Our SQL warehouse is running and ready to be used Click on Dashboards in the sidebar Click on Visit gallery From here, click Import, next to the New York Taxi Trip Analysis option info Like this, we have created our first dashboard If we click again on Dashboards in the sidebar We can see our sample dashboard we have just created, and I have my name under the Created By field note Note that from this page, you can also create a new dashboard from scratch Let us click on our dashboard's name to reopen it We can any time click Refresh to rerun the queries behind each graph and refresh the data To view the query behind any graph, simply click on the three vertical dots on the graph, and select View Query from the menu note Note here the three tiers namespaces that is used to identify the source table, which are: catalog, database and table name info This is a preview of the new functionality to be supported by Unity Catalog Let us click Run to preview the result of this query In the Table tab, we can see the query results If we click on the second tab, we switch to the preview of our graph Click Edit visualization to modify this graph From here, you can change the setting of your visualization Let us click Cancel for now To add a new visualization using this query, click the + sign Then, Visualization Let us create a pie graph We set the X column as day of week Add a Y column for fare_amount Let us leave all other settings as defaults and click Save Click on the tab to give this visualization a name If we click on the three vertical dots of this tab. We have the option to add this graph to a dashboard We select our sample dashboard We click Add Let us navigate back to the dashboard to view this change If you scroll down, you can see your new graph has been successfully added to the dashboard We can adjust the organization of the graphs in a dashboard From the three vertical button, click Edit We can drag our visualization. Resize it Let us click on Done Editing You can share your dashboard with other users Click the Share button Select, for example, all users from the top field Choose Can Run permission Click Add From Credentials, you can choose to run using the viewer credentials if the user have access to the underlying data info If not, choose the Run As Owner to run the dashboard using the owner credential Let us now navigate to the editor to write some custom queries Click on the + button Choose create new query info Make sure you are connected to a SQL warehouse Click on the schema browser to select the Sample Catalog Select a database, click on Select info Choose NYC Taxi In this database, we have only one table called trips Click on the table name to get a preview of its schema Let us write our first query To add the table name to the query text, we can simply hover over the table name and click this double arrows button info SELECT * FROM nyctaxi.trips Let us now click on Run to preview the results Let us update this query by adding a GROUP BY clause on the pickup_zip code info SELECT pickup_zip, SUM(fare_amount) AS total_fare FROM nyctaxi.trips GROUP BY pickup_zip We will add this column again to the SELECT clause. And apply some aggregation function on the fair_amount We give this column an alias called total_fare info SELECT pickup_zip, SUM(fare_amount) AS total_fare FROM nyctaxi.trips GROUP BY pickup_zip Click Run to preview the results We can save our query by clicking on the Save button Give it a name We can also add the query result to a dashboard Click on the three vertical dots on the Table tab, then Add to Dashboard and we select our sample dashboard If we navigate back to our dashboard, and we scroll down, we can see that our query result has been successfully added to the dashboard You can also set a schedule for the query to refresh the result automatically Click on the Schedule button Use the dropdown menu to change to refresh every one week at 7 a.m. Click Okay If we navigate to Queries in the sidebar, we can find our saved query. In Databricks SQL, we can use the saved query to set up an alert From the left side bar, navigate to Alerts Alerts in Databricks SQL allow you to receive notification when a field of your query meets a certain threshold Click Create Alert in the top right Select your saved query Let us first give this alert a name, by clicking on the field at the top left of the screen For the Trigger When option, set the value column to total_fare, with a condition greater than 10,000 threshold For Refresh, we can select Never Click on Create Alert Click on the Refresh button to evaluate the alert info If the top row value has been updated and exceeded this thresholds, the alert will be triggered and I will receive a notification on this email You can set other destination to receive notifications If you click on the Add button, you can choose either an existing alert destination, or create a new destination in the Alert Destinations page As you can see, you have plenty of alerting options like Slack and Microsoft Teams ","keywords":"","version":"Next"}],"options":{"id":"default"}}